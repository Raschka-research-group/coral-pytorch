{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc08065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "torch        : 1.12.0\n",
      "torchtext    : 0.13.0\n",
      "spacy        : 3.4.0\n",
      "lightning    : not installed\n",
      "torchmetrics : 0.9.1\n",
      "matplotlib   : 3.3.4\n",
      "coral_pytorch: 1.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p torch,torchtext,spacy,pytorch_lightning,torchmetrics,matplotlib,coral_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae82a3d",
   "metadata": {},
   "source": [
    "# A Recurrent Neural Net for Ordinal Regression using CORN -- TripAdvisor Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615a602",
   "metadata": {},
   "source": [
    "In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint:\n",
    "\n",
    "- Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint;  [https://arxiv.org/abs/2111.08851](https://arxiv.org/abs/2111.08851)\n",
    "\n",
    "\n",
    "We will be using a balanced version of the [TripAdvisor Hotel Review](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews) dataset that [we used](https://github.com/Raschka-research-group/corn-ordinal-neuralnet/blob/main/datasets/tripadvisor/tripadvisor_balanced.csv) in the CORN manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeccbee",
   "metadata": {},
   "source": [
    "## General settings and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85105d46",
   "metadata": {},
   "source": [
    "- Here, we specify some general hyperparameter values and general settings\n",
    "- Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting `NUM_WORKERS = 0` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86632a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 40\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_WORKERS = 4\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "# Architecture:\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Dataset specific:\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "VOCABULARY_SIZE = 20000\n",
    "DATA_BASEPATH = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91389f86-55db-4a6e-b87b-3a04a5aa71e6",
   "metadata": {},
   "source": [
    "This tutorial also requires the spacy English vocabulary, which can be downloaded as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b167e-0063-4bc1-a7aa-d8dbc2236011",
   "metadata": {},
   "source": [
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c01e59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Converting a regular classifier into a CORN ordinal regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f177ac8",
   "metadata": {},
   "source": [
    "Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes:\n",
    "\n",
    "**1)**\n",
    "\n",
    "Consider the following output layer used by a neural network classifier:\n",
    "\n",
    "```python\n",
    "output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n",
    "```\n",
    "\n",
    "In CORN we reduce the number of classes by 1:\n",
    "\n",
    "```python\n",
    "output_layer = torch.nn.Linear(hidden_units[-1], num_classes-1)\n",
    "```\n",
    "\n",
    "**2)** \n",
    "\n",
    "We swap the cross entropy loss from PyTorch,\n",
    "\n",
    "```python\n",
    "torch.nn.functional.cross_entropy(logits, true_labels)\n",
    "```\n",
    "\n",
    "with the CORN loss (also provided via `coral_pytorch`):\n",
    "\n",
    "```python\n",
    "loss = corn_loss(logits, true_labels,\n",
    "                 num_classes=num_classes)\n",
    "```\n",
    "\n",
    "Note that we pass `num_classes` instead of `num_classes-1` \n",
    "to the `corn_loss` as it takes care of the rest internally.\n",
    "\n",
    "\n",
    "**3)**\n",
    "\n",
    "In a regular classifier, we usually obtain the predicted class labels as follows:\n",
    "\n",
    "```python\n",
    "predicted_labels = torch.argmax(logits, dim=1)\n",
    "```\n",
    "\n",
    "In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels:\n",
    "\n",
    "```python\n",
    "predicted_labels = corn_label_from_logits(logits)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3ee59",
   "metadata": {},
   "source": [
    "## Implementing an `RNN` using PyTorch Lightning's `LightningModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42524",
   "metadata": {},
   "source": [
    "- In this section, we set up the main model architecture using the `LightningModule` from PyTorch Lightning.\n",
    "- We start with defining our recurrent neural network (`RNN`) model in pure PyTorch, and then we use it in the `LightningModule` to get all the extra benefits that PyTorch Lightning provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8fe68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Regular PyTorch Module\n",
    "class PyTorchRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim,\n",
    "                 hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                         hidden_dim,\n",
    "        #                         nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "\n",
    "        # CORN output layer ------------------------------------------\n",
    "        # Regular classifier would use num_classes instead of \n",
    "        # num_classes-1 below\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, num_classes-1)\n",
    "        # ------------------------------------------------------------\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        # text dim: [sentence len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence len, batch size, embed dim]\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_length.to('cpu'))\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        # output dim: [sentence len, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "        logits = output.view(-1, (self.num_classes-1))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2fd34",
   "metadata": {},
   "source": [
    "- In our `LightningModule` we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later.\n",
    "- Given an RNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes:\n",
    "    1. Instead of using `num_classes` in the output layer, use `num_classes-1` as shown above\n",
    "    2. Change the loss from   \n",
    "    `loss = torch.nn.functional.cross_entropy(logits, y)` to  \n",
    "    `loss = corn_loss(logits, y, num_classes=self.num_classes)`\n",
    "    3. To obtain the class/rank labels from the logits, change  \n",
    "    `predicted_labels = torch.argmax(logits, dim=1)` to  \n",
    "    `predicted_labels = corn_label_from_logits(logits)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45761ea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoral_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corn_loss\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoral_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corn_label_from_logits\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# LightningModule that receives a PyTorch model as input\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m _PACKAGE_ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m     18\u001b[0m _PROJECT_ROOT \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(_PACKAGE_ROOT)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_stats_monitor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceStatsMonitor\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mearly_stopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "File \u001b[0;32m~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m(abc\u001b[38;5;241m.\u001b[39mABC):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"General utilities.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply_func\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m move_data_to_device  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AllGatherGrad, rank_zero_info, rank_zero_only  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     AMPType,\n\u001b[1;32m     22\u001b[0m     DeviceType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     ModelSummaryMode,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCHTEXT_AVAILABLE:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _compare_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, operator\u001b[38;5;241m.\u001b[39mge, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.9.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "from coral_pytorch.losses import corn_loss\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "# LightningModule that receives a PyTorch model as input\n",
    "class LightningRNN(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = model.input_dim\n",
    "        self.embedding_dim = model.embedding_dim\n",
    "        self.hidden_dim = model.hidden_dim\n",
    "        self.num_classes = model.num_classes        \n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        # The inherited PyTorch module\n",
    "        self.model = model\n",
    "\n",
    "        # Save settings and hyperparameters to the log directory\n",
    "        # but skip the model parameters\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "\n",
    "        # Set up attributes for computing the MAE\n",
    "        self.train_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.valid_mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.test_mae = torchmetrics.MeanAbsoluteError()\n",
    "        \n",
    "    # (Re)Defining the forward method is only necessary \n",
    "    # if you want to use a Trainer's .predict() method (optional)\n",
    "    def forward(self, text, text_length):\n",
    "        return self.model(text, text_length)\n",
    "        \n",
    "    # A common forward step to compute the loss and labels\n",
    "    # this is used for training, validation, and testing below\n",
    "    def _shared_step(self, batch):\n",
    "        \n",
    "        # These next 3 steps are unique and look a bit tricky due to\n",
    "        # how Torchtext's BucketIterator prepares the batches\n",
    "        # and how we use an LSTM with packed & padded text\n",
    "        # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME\n",
    "        # depend on the CSV file columns of the data file we load later.\n",
    "        features, text_length = batch.TEXT_COLUMN_NAME\n",
    "        true_labels = batch.LABEL_COLUMN_NAME\n",
    " \n",
    "        logits = self(features, text_length)\n",
    "\n",
    "        # Use CORN loss ---------------------------------------------------\n",
    "        # A regular classifier uses:\n",
    "        # loss = torch.nn.functional.cross_entropy(logits, true_labels)\n",
    "        loss = corn_loss(logits, true_labels,\n",
    "                         num_classes=self.model.num_classes)\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # CORN logits to labels -------------------------------------------\n",
    "        # A regular classifier uses:\n",
    "        # predicted_labels = torch.argmax(logits, dim=1)\n",
    "        predicted_labels = corn_label_from_logits(logits)\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.log(\"train_loss\", loss, batch_size=true_labels.shape[0])\n",
    "        self.train_mae(predicted_labels, true_labels)\n",
    "        self.log(\"train_mae\", self.train_mae, on_epoch=True, on_step=False,\n",
    "                 batch_size=true_labels.shape[0])\n",
    "        return loss  # this is passed to the optimzer for training\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.log(\"valid_loss\", loss, batch_size=true_labels.shape[0])\n",
    "        self.valid_mae(predicted_labels, true_labels)\n",
    "        self.log(\"valid_mae\", self.valid_mae,\n",
    "                 on_epoch=True, on_step=False, prog_bar=True,\n",
    "                 batch_size=true_labels.shape[0])\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_mae(predicted_labels, true_labels)\n",
    "        self.log(\"test_mae\", self.test_mae, on_epoch=True, on_step=False,\n",
    "                 batch_size=true_labels.shape[0])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18900d",
   "metadata": {},
   "source": [
    "## Setting up the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95f5af",
   "metadata": {},
   "source": [
    "- In this section, we are going to set up our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c43abd",
   "metadata": {},
   "source": [
    "### Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/Raschka-research-group/\"\n",
    "    \"corn-ordinal-neuralnet/main/datasets/\"\n",
    "    \"tripadvisor/tripadvisor_balanced.csv\")\n",
    "\n",
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "CSV_PATH = os.path.join(DATA_BASEPATH, 'tripadvisor_balanced.csv')\n",
    "data_df.to_csv(CSV_PATH, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "\n",
    "\n",
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize='spacy',  # default splits on whitespace\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    include_lengths=True\n",
    ")\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.long)\n",
    "\n",
    "fields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path=CSV_PATH, \n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=fields)\n",
    "\n",
    "train_data, test_data = dataset.split(\n",
    "    split_ratio=[0.8, 0.2],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.85, 0.15],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "train_loader, valid_loader, test_loader= \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sort_within_batch=True,  # necessary for packed_padded_sequence\n",
    "             sort_key=lambda x: len(x.TEXT_COLUMN_NAME),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82995976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset\n",
    "all_train_labels = []\n",
    "all_test_labels = []\n",
    "\n",
    "for features, labels in train_loader:  \n",
    "    all_train_labels.append(labels)\n",
    "all_train_labels = torch.cat(all_train_labels)\n",
    "    \n",
    "for features, labels in test_loader:  \n",
    "    all_test_labels.append(labels)\n",
    "all_test_labels = torch.cat(all_test_labels)\n",
    "\n",
    "print('Training labels:', torch.unique(all_train_labels))\n",
    "print('Training label distribution:', torch.bincount(all_train_labels))\n",
    "\n",
    "print('\\nTest labels:', torch.unique(all_test_labels))\n",
    "print('Test label distribution:', torch.bincount(all_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b1e04f",
   "metadata": {},
   "source": [
    "- Above, we can see that the dataset consists of 8 features, and there are 998 examples in total.\n",
    "- The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). \n",
    "- Notice also that the dataset is quite balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761628c",
   "metadata": {},
   "source": [
    "### Performance baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ab9d2",
   "metadata": {},
   "source": [
    "- Especially for imbalanced datasets, it's quite useful to compute a performance baseline.\n",
    "- In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that!\n",
    "- Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE).\n",
    "- So, if we use the mean absolute error, $\\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right|$, to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115cb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_labels = all_test_labels.float()\n",
    "avg_prediction = torch.median(all_test_labels)  # median minimizes MAE\n",
    "baseline_mae = torch.mean(torch.abs(all_test_labels - avg_prediction))\n",
    "print(f'Baseline MAE: {baseline_mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef229a99",
   "metadata": {},
   "source": [
    "- In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c942f1",
   "metadata": {},
   "source": [
    "### Setting up a `DataModule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d5df1",
   "metadata": {},
   "source": [
    "- There are three main ways we can prepare the dataset for Lightning. We can\n",
    "  1. make the dataset part of the model;\n",
    "  2. set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection;\n",
    "  3. create a `LightningDataModule`.\n",
    "- Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b25c6",
   "metadata": {},
   "source": [
    "## Training the model using the PyTorch Lightning Trainer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d1cbe",
   "metadata": {},
   "source": [
    "- Next, we initialize our `PyTorchRNN` model.\n",
    "- Also, we define a call back so that we can obtain the model with the best validation set performance after training.\n",
    "- PyTorch Lightning offers [many advanced logging services](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html) like Weights & Biases. Here, we will keep things simple and use the `CSVLogger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99547395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchRNN(\n",
    "    input_dim=len(TEXT.vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES)\n",
    "\n",
    "lightning_model = LightningRNN(\n",
    "    pytorch_model, learning_rate=LEARNING_RATE)\n",
    "\n",
    "callbacks = [ModelCheckpoint(\n",
    "    save_top_k=1, mode='min', monitor=\"valid_mae\")]  # save top 1 model \n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"rnn-corn-mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c54517",
   "metadata": {},
   "source": [
    "- Note that we disable warning as the `.log()` method of the `LightningModule` currently warns us that the batch size is inconsistent. This should not happen as we define the `batch_size` manually in the `self.log` calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). \n",
    "\n",
    "- Also note that the batch size is not inconsistent, its just that the `BucketIterator` in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf368d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, labels in train_loader:  \n",
    "    break\n",
    "\n",
    "print('Text length:', features[0].shape[0])\n",
    "print('Batch size (from text):', features[0].shape[1])\n",
    "print('Batch size (from labels):', labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a67e52",
   "metadata": {},
   "source": [
    "- Now it's time to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    progress_bar_refresh_rate=50,  # recommended for notebooks\n",
    "    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n",
    "    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=valid_loader)\n",
    "\n",
    "runtime = (time.time() - start_time)/60\n",
    "print(f\"Training took {runtime:.2f} min in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf312dc",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abfab9",
   "metadata": {},
   "source": [
    "- After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a [more advanced logger](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html) that does that for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "df_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n",
    "    grid=True, legend=True, xlabel='Epoch', ylabel='Loss')\n",
    "df_metrics[[\"train_mae\", \"valid_mae\"]].plot(\n",
    "    grid=True, legend=True, xlabel='Epoch', ylabel='MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d9f7d",
   "metadata": {},
   "source": [
    "- As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 8.\n",
    "- The `trainer` saved this model automatically for us, we which we can load from the checkpoint via the `ckpt_path='best'` argument; below we use the `trainer` instance to evaluate the best model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce79a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=lightning_model, dataloaders=test_loader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ff3b6",
   "metadata": {},
   "source": [
    "## Predicting labels of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5dcfa",
   "metadata": {},
   "source": [
    "- You can use the `trainer.predict` method on a new `DataLoader` or `DataModule` to apply the model to new data.\n",
    "- Alternatively, you can also manually load the best model from a checkpoint as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e18cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = trainer.checkpoint_callback.best_model_path\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = LightningRNN.load_from_checkpoint(\n",
    "    path, model=pytorch_model)\n",
    "\n",
    "lightning_model.to(torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "lightning_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f84717",
   "metadata": {},
   "source": [
    "- Note that our `PyTorchRNN`, which is passed to `LightningRNN` requires input arguments. However, this is automatically being taken care of since we used `self.save_hyperparameters()` in `LightningRNN`'s `__init__` method.\n",
    "- Now, below is an example applying the model manually. Here, pretend that the `test_dataloader` is a new data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_labels = []\n",
    "for batch in test_loader:\n",
    "    features, text_length = batch.TEXT_COLUMN_NAME\n",
    "\n",
    "    logits = lightning_model(features, text_length)\n",
    "    predicted_labels = corn_label_from_logits(logits)\n",
    "    all_predicted_labels.append(predicted_labels)\n",
    "    \n",
    "all_predicted_labels = torch.cat(all_predicted_labels)\n",
    "all_predicted_labels[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
