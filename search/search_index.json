{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>CORAL &amp; CORN implementations for ordinal regression with deep neural networks.</p> <p> </p> <p></p>"},{"location":"#about","title":"About","text":"<p>CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks.</p> <p></p> <p>Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks.</p> <p>This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch.</p> <p>If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: </p> <ul> <li>CORAL: https://github.com/Raschka-research-group/coral-cnn.</li> <li>CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet </li> </ul>"},{"location":"#references","title":"References","text":"<p>CORAL</p> <ul> <li>Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020).  Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation. Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008.</li> </ul> <p>CORN</p> <ul> <li>Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint;  https://arxiv.org/abs/2111.08851</li> </ul>"},{"location":"CHANGELOG/","title":"Release Notes","text":"<p>The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md.</p>"},{"location":"CHANGELOG/#140-07-17-2022","title":"1.4.0 (07-17-2022)","text":""},{"location":"CHANGELOG/#downloads","title":"Downloads","text":"<ul> <li>Source code (zip)</li> <li>Source code (tar.gz)</li> </ul>"},{"location":"CHANGELOG/#new-features","title":"New Features","text":"<ul> <li>Adds object-oriented versions of the losses: <code>coral_pytorch.losses.CoralLoss</code> and <code>coral_pytorch.losses.CornLoss</code>.</li> </ul>"},{"location":"CHANGELOG/#changes","title":"Changes","text":"<ul> <li> <ul> <li></li> </ul> </li> </ul>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#130-07-16-2022","title":"1.3.0 (07-16-2022)","text":""},{"location":"CHANGELOG/#downloads_1","title":"Downloads","text":"<ul> <li>Source code (zip)</li> <li>Source code (tar.gz)</li> </ul>"},{"location":"CHANGELOG/#new-features_1","title":"New Features","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#changes_1","title":"Changes","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fixes a bug where the normalization of the <code>corn_loss</code> different from the one proposed in the original paper. (#22) </li> </ul>"},{"location":"CHANGELOG/#120-11-17-2021","title":"1.2.0 (11-17-2021)","text":""},{"location":"CHANGELOG/#downloads_2","title":"Downloads","text":"<ul> <li>Source code (zip)</li> <li>Source code (tar.gz)</li> </ul>"},{"location":"CHANGELOG/#new-features_2","title":"New Features","text":"<ul> <li>Add CORN loss corresponding to the manuscript, \"Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities\"</li> </ul>"},{"location":"CHANGELOG/#changes_2","title":"Changes","text":"<ul> <li> <ul> <li></li> </ul> </li> </ul>"},{"location":"CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#110-04082021","title":"1.1.0 (04/08/2021)","text":""},{"location":"CHANGELOG/#downloads_3","title":"Downloads","text":"<ul> <li>Source code (zip)</li> <li>Source code (tar.gz)</li> </ul>"},{"location":"CHANGELOG/#new-features_3","title":"New Features","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#changes_3","title":"Changes","text":"<ul> <li>By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5)</li> </ul>"},{"location":"CHANGELOG/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>-</li> </ul>"},{"location":"CHANGELOG/#100-11152020","title":"1.0.0 (11/15/2020)","text":""},{"location":"CHANGELOG/#downloads_4","title":"Downloads","text":"<ul> <li>Source code (zip)</li> <li>Source code (tar.gz)</li> </ul>"},{"location":"CHANGELOG/#new-features_4","title":"New Features","text":"<ul> <li>First release.</li> </ul>"},{"location":"CHANGELOG/#changes_4","title":"Changes","text":"<ul> <li>First release.</li> </ul>"},{"location":"CHANGELOG/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>First release.</li> </ul>"},{"location":"citing/","title":"Citing","text":"<p>If you use CORAL or CORN  as part of your workflow in a scientific publication, please consider citing the corresponding paper:</p> <p>CORAL</p> <ul> <li>Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020).  Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation. Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008.</li> </ul> <pre><code>@article{coral2020,\ntitle={Rank consistent ordinal regression for neural networks with application to age estimation},\njournal={Pattern Recognition Letters},\nvolume={140},\npages={325-331},\nyear={2020},\nissn={0167-8655},\ndoi={https://doi.org/10.1016/j.patrec.2020.11.008},\nurl={http://www.sciencedirect.com/science/article/pii/S016786552030413X},\nauthor={Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka}\n}\n</code></pre> <p>CORN</p> <ul> <li>Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint;  https://arxiv.org/abs/2111.08851</li> </ul> <pre><code>@misc{shi2021deep,\n      title={Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities}, \n      author={Xintong Shi and Wenzhi Cao and Sebastian Raschka},\n      year={2021},\n      eprint={2111.08851},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"installation/","title":"Installing <code>coral_pytorch</code>","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p>Coral-pytorch requires the following software and packages:</p> <ul> <li>Python &gt;= 3.6</li> <li>PyTorch &gt;= 1.5.0</li> </ul>"},{"location":"installation/#pypi","title":"PyPI","text":"<p>You can install the latest stable release of <code>coral_pytorch</code> directly from Python's package index via <code>pip</code> by executing the following code from your command line:  </p> <pre><code>pip install coral_pytorch\n</code></pre>"},{"location":"installation/#latest-github-source-code","title":"Latest GitHub Source Code","text":"<p>You want to try out the latest features before they go live on PyPI? Install the <code>coral_pytorch</code> dev-version latest development version from the GitHub repository by executing</p> <pre><code>pip install git+git://github.com/rasbt/coral_pytorch.git\n</code></pre> <p></p> <p>Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command:</p> <pre><code>python setup.py install\n</code></pre>"},{"location":"license/","title":"MIT License","text":"<p>Copyright (c) 2020-2022 Sebastian Raschka</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/","title":"Corn label from logits","text":""},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/#corn_label_from_logits","title":"corn_label_from_logits","text":"<p>corn_label_from_logits(logits)</p> <p>Returns the predicted rank label from logits for a     network trained via the CORN loss.</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(n_examples, n_classes)</p> <p>Torch tensor consisting of logits returned by the neural net.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>labels</code> : torch.tensor, shape=(n_examples)</p> <p>Integer tensor containing the predicted rank (class) labels</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; # 2 training examples, 5 classes\n    &gt;&gt;&gt; logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850],\n    ...                        [65.667, 0.303, 11.500, -4.524]])\n    &gt;&gt;&gt; corn_label_from_logits(logits)\n    tensor([1, 3])\n</code></pre>"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/","title":"Label to levels","text":""},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/#label_to_levels","title":"label_to_levels","text":"<p>label_to_levels(label, num_classes, dtype=torch.float32)</p> <p>Converts integer class label to extended binary label vector</p> <p>Parameters</p> <ul> <li> <p><code>label</code> : int</p> <p>Class label to be converted into a extended binary vector. Should be smaller than num_classes-1.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector.</p> </li> <li> <p><code>dtype</code> : torch data type (default=torch.float32)</p> <p>Data type of the torch output vector for the extended binary labels.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>levels</code> : torch.tensor, shape=(num_classes-1,)</p> <p>Extended binary label vector. Type is determined by the <code>dtype</code> parameter.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; label_to_levels(0, num_classes=5)\n    tensor([0., 0., 0., 0.])\n    &gt;&gt;&gt; label_to_levels(1, num_classes=5)\n    tensor([1., 0., 0., 0.])\n    &gt;&gt;&gt; label_to_levels(3, num_classes=5)\n    tensor([1., 1., 1., 0.])\n    &gt;&gt;&gt; label_to_levels(4, num_classes=5)\n    tensor([1., 1., 1., 1.])\n</code></pre>"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/","title":"Levels from labelbatch","text":""},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/#levels_from_labelbatch","title":"levels_from_labelbatch","text":"<p>levels_from_labelbatch(labels, num_classes, dtype=torch.float32)</p> <p>Converts a list of integer class label to extended binary label vectors</p> <p>Parameters</p> <ul> <li> <p><code>labels</code> : list or 1D orch.tensor, shape=(num_labels,)</p> <p>A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector.</p> </li> <li> <p><code>dtype</code> : torch data type (default=torch.float32)</p> <p>Data type of the torch output vector for the extended binary labels.</p> </li> </ul> <p>Returns</p> <ul> <li><code>levels</code> : torch.tensor, shape=(num_labels, num_classes-1)</li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; levels_from_labelbatch(labels=[2, 1, 4], num_classes=5)\n    tensor([[1., 1., 0., 0.],\n    [1., 0., 0., 0.],\n    [1., 1., 1., 1.]])\n</code></pre>"},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/","title":"Proba to label","text":""},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/#proba_to_label","title":"proba_to_label","text":"<p>proba_to_label(probas)</p> <p>Converts predicted probabilities from extended binary format     to integer class labels</p> <p>Parameters</p> <ul> <li> <p><code>probas</code> : torch.tensor, shape(n_examples, n_labels)</p> <p>Torch tensor consisting of probabilities returned by CORAL model.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; # 3 training examples, 6 classes\n    &gt;&gt;&gt; probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295],\n    ...                        [0.496, 0.485, 0.267, 0.124, 0.058],\n    ...                        [0.985, 0.967, 0.920, 0.819, 0.506]])\n    &gt;&gt;&gt; proba_to_label(probas)\n    tensor([2, 0, 5])\n</code></pre>"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/","title":"CoralLayer","text":""},{"location":"api_modules/coral_pytorch.layers/CoralLayer/#corallayer","title":"CoralLayer","text":"<p>CoralLayer(size_in, num_classes, preinit_bias=True)</p> <p>Implements CORAL layer described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>size_in</code> : int</p> <p>Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features).</p> </li> <li> <p><code>num_classes</code> : int</p> <p>Number of classes in the dataset.</p> </li> <li> <p><code>preinit_bias</code> : bool (default=True)</p> <p>If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.</p> </li> </ul>"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/","title":"CoralLoss","text":""},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#coralloss","title":"CoralLoss","text":"<p>CoralLoss(reduction='mean')</p> <p>Computes the CORAL loss described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>reduction</code> : str or None (default='mean')</p> <p>If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,)</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import CoralLoss\n    &gt;&gt;&gt; levels = torch.tensor(\n    ...    [[1., 1., 0., 0.],\n    ...     [1., 0., 0., 0.],\n    ...    [1., 1., 1., 1.]])\n    &gt;&gt;&gt; logits = torch.tensor(\n    ...    [[2.1, 1.8, -2.1, -1.8],\n    ...     [1.9, -1., -1.5, -1.3],\n    ...     [1.9, 1.8, 1.7, 1.6]])\n    &gt;&gt;&gt; loss = CoralLoss()\n    &gt;&gt;&gt; loss(logits, levels)\n    tensor(0.6920)\n</code></pre>"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#methods","title":"Methods","text":"<p>add_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Adds a child module to the current module.</p> <pre><code>The module can be accessed as an attribute using the given name.\n\nArgs:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.\n</code></pre> <p>apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</p> <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)     as well as self. Typical use includes initializing the parameters of a model     (see also :ref:<code>nn-init-doc</code>).</p> <pre><code>Args:\nfn (:class:`Module` -&gt; None): function to be applied to each submodule\n\nReturns:\nModule: self\n\nExample::\n\n```\n&gt;&gt;&gt; @torch.no_grad()\n&gt;&gt;&gt; def init_weights(m):\n&gt;&gt;&gt;     print(m)\n&gt;&gt;&gt;     if type(m) == nn.Linear:\n&gt;&gt;&gt;         m.weight.fill_(1.0)\n&gt;&gt;&gt;         print(m.weight)\n&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n&gt;&gt;&gt; net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nSequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n</code></pre> <p>```</p> <p>bfloat16(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>buffers(recurse: bool = True) -&gt; Iterator[torch.Tensor]</p> <p>Returns an iterator over module buffers.</p> <pre><code>Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\ntorch.Tensor: module buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf), buf.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>children() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over immediate children modules.</p> <pre><code>Yields:\nModule: a child module\n</code></pre> <p>cpu(self: ~T) -&gt; ~T</p> <p>Moves all model parameters and buffers to the CPU.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the GPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>double(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>eval(self: ~T) -&gt; ~T</p> <p>Sets the module in evaluation mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.\n\nReturns:\nModule: self\n</code></pre> <p>extra_repr() -&gt; str</p> <p>Set the extra representation of the module</p> <pre><code>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.\n</code></pre> <p>float(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>forward(logits, levels, importance_weights=None)</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>Outputs of the CORAL layer.</p> </li> <li> <p><code>levels</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>True labels represented as extended binary vectors (via <code>coral_pytorch.dataset.levels_from_labelbatch</code>).</p> </li> <li> <p><code>importance_weights</code> : torch.tensor, shape=(num_classes-1,) (default=None)</p> <p>Optional weights for the different labels in levels. A tensor of ones, i.e., <code>torch.ones(num_classes-1, dtype=torch.float32)</code> will result in uniform weights that have the same effect as None.</p> </li> </ul> <p>get_buffer(target: str) -&gt; 'Tensor'</p> <p>Returns the buffer given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the buffer\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.Tensor: The buffer referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer\n</code></pre> <p>get_extra_state() -&gt; Any</p> <p>Returns any extra state to include in the module's state_dict.     Implement this and a corresponding :func:<code>set_extra_state</code> for your module     if you need to store extra state. This function is called when building the     module's <code>state_dict()</code>.</p> <pre><code>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.\n\nReturns:\nobject: Any extra state to store in the module's state_dict\n</code></pre> <p>get_parameter(target: str) -&gt; 'Parameter'</p> <p>Returns the parameter given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Parameter: The Parameter referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Parameter``\n</code></pre> <p>get_submodule(target: str) -&gt; 'Module'</p> <p>Returns the submodule given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>For example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\nA(\n(net_b): Module(\n(net_c): Module(\n(conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n)\n(linear): Linear(in_features=100, out_features=200, bias=True)\n)\n)\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.\n\nArgs:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Module: The submodule referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Module``\n</code></pre> <p>half(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the IPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>load_state_dict(state_dict: Mapping[str, Any], strict: bool = True)</p> <p>Copies parameters and buffers from :attr:<code>state_dict</code> into     this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then     the keys of :attr:<code>state_dict</code> must exactly match the keys returned     by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <pre><code>Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:`state_dict` match the keys returned by this module's\n:meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\nReturns:\n``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n</code></pre> <ul> <li>missing_keys is a list of str containing the missing keys</li> <li>unexpected_keys is a list of str containing the unexpected keys</li> </ul> <p>Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>modules() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over all modules in the network.</p> <pre><code>Yields:\nModule: a module in the network\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n</code></pre> <p>```</p> <p>named_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</p> <p>Returns an iterator over module buffers, yielding both the     name of the buffer as well as the buffer itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\nYields:\n(str, torch.Tensor): Tuple containing the name and buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;     if name in ['running_var']:\n&gt;&gt;&gt;         print(buf.size())\n</code></pre> <p>```</p> <p>named_children() -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</p> <p>Returns an iterator over immediate children modules, yielding both     the name of the module as well as the module itself.</p> <pre><code>Yields:\n(str, Module): Tuple containing a name and child module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n</code></pre> <p>```</p> <p>named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)</p> <p>Returns an iterator over all modules in the network, yielding     both the name of the module as well as the module itself.</p> <pre><code>Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not\n\nYields:\n(str, Module): Tuple of name and module\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))\n</code></pre> <p>```</p> <p>named_parameters(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</p> <p>Returns an iterator over module parameters, yielding both the     name of the parameter as well as the parameter itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.\n\nYields:\n(str, Parameter): Tuple containing the name and parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;     if name in ['bias']:\n&gt;&gt;&gt;         print(param.size())\n</code></pre> <p>```</p> <p>parameters(recurse: bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</p> <p>Returns an iterator over module parameters.</p> <pre><code>This is typically passed to an optimizer.\n\nArgs:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\nParameter: module parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for param in model.parameters():\n&gt;&gt;&gt;     print(type(param), param.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -&gt; None</p> <p>Adds a buffer to the module.</p> <pre><code>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nArgs:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If ``None``, then operations\nthat run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n</code></pre> <p>the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's</p> <p>:attr:<code>state_dict</code>.</p> <pre><code>Example::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))\n</code></pre> <p>```</p> <p>register_forward_hook(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward hook on the module.</p> <pre><code>The hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\nhook(module, args, output) -&gt; None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\nhook(module, args, kwargs, output) -&gt; None or modified output\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If ``True``, the provided ``hook`` will be fired\nbefore all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks\nregistered by this method.\nDefault: ``False``\nwith_kwargs (bool): If ``True``, the ``hook`` will be passed the\nkwargs given to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_forward_pre_hook(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward pre-hook on the module.</p> <pre><code>The hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\nhook(module, args) -&gt; None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\nhook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all\nhooks registered by this method.\nDefault: ``False``\nwith_kwargs (bool): If true, the ``hook`` will be passed the kwargs\ngiven to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\nhook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_pre_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward pre-hook on the module.</p> <pre><code>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\nhook(module, grad_output) -&gt; Tensor or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_load_state_dict_post_hook(hook)</p> <p>Registers a post hook to be run after module's <code>load_state_dict</code>     is called.</p> <pre><code>It should have the following signature::\nhook(module, incompatible_keys) -&gt; None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Alias for :func:<code>add_module</code>.</p> <p>register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -&gt; None</p> <p>Adds a parameter to the module.</p> <pre><code>The parameter can be accessed as an attribute using given name.\n\nArgs:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,\n</code></pre> <p>are ignored. If <code>None</code>, the parameter is not included in the     module's :attr:<code>state_dict</code>.</p> <p>register_state_dict_pre_hook(hook)</p> <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>,     and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered     hooks can be used to perform pre-processing before the <code>state_dict</code>     call is made.</p> <p>requires_grad_(self: ~T, requires_grad: bool = True) -&gt; ~T</p> <p>Change if autograd should record operations on parameters in this     module.</p> <pre><code>This method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\n\nArgs:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>set_extra_state(state: Any)</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state     found within the <code>state_dict</code>. Implement this function and a corresponding     :func:<code>get_extra_state</code> for your module if you need to store extra state within its     <code>state_dict</code>.</p> <pre><code>Args:\nstate (dict): Extra state from the `state_dict`\n</code></pre> <p>share_memory(self: ~T) -&gt; ~T</p> <p>See :meth:<code>torch.Tensor.share_memory_</code></p> <p>state_dict(args, destination=None, prefix='', keep_vars=False)*</p> <p>Returns a dictionary containing references to the whole state of the module.</p> <pre><code>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.\n\n.. warning::\nCurrently ``state_dict()`` also accepts positional arguments for\n``destination``, ``prefix`` and ``keep_vars`` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.\n\n.. warning::\nPlease avoid the use of argument ``destination`` as it is not\ndesigned for end-users.\n\nArgs:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ``OrderedDict`` will be created and returned.\nDefault: ``None``.\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ``''``.\nkeep_vars (bool, optional): by default the :class:`~torch.Tensor` s\nreturned in the state dict are detached from autograd. If it's\nset to ``True``, detaching will not be performed.\nDefault: ``False``.\n\nReturns:\ndict:\na dictionary containing a whole state of the module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n</code></pre> <p>```</p> <p>to(args, *kwargs)</p> <p>Moves and/or casts the parameters and buffers.</p> <pre><code>This can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:\n\n.. function:: to(dtype, non_blocking=False)\n:noindex:\n\n.. function:: to(tensor, non_blocking=False)\n:noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n:noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (:class:`torch.device`): the desired device of the parameters\nand buffers in this module\ndtype (:class:`torch.dtype`): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:`torch.memory_format`): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\n\nReturns:\nModule: self\n\nExamples::\n\n```\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16)\n\n&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n</code></pre> <p>```</p> <p>to_empty(self: ~T, , device: Union[str, torch.device]) -&gt; ~T*</p> <p>Moves the parameters and buffers to the specified device without copying storage.</p> <pre><code>Args:\ndevice (:class:`torch.device`): The desired device of the parameters\nand buffers in this module.\n\nReturns:\nModule: self\n</code></pre> <p>train(self: ~T, mode: bool = True) -&gt; ~T</p> <p>Sets the module in training mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nArgs:\nmode (bool): whether to set training mode (``True``) or evaluation\nmode (``False``). Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>type(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</p> <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nArgs:\ndst_type (type or string): the desired type\n\nReturns:\nModule: self\n</code></pre> <p>xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the XPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>zero_grad(set_to_none: bool = True) -&gt; None</p> <p>Sets gradients of all model parameters to zero. See similar function     under :class:<code>torch.optim.Optimizer</code> for more context.</p> <pre><code>Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:`torch.optim.Optimizer.zero_grad` for details.\n</code></pre>"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#properties","title":"Properties","text":""},{"location":"api_modules/coral_pytorch.losses/CornLoss/","title":"CornLoss","text":""},{"location":"api_modules/coral_pytorch.losses/CornLoss/#cornloss","title":"CornLoss","text":"<p>CornLoss(num_classes)</p> <p>Computes the CORN loss described in our forthcoming     'Deep Neural Networks for Rank Consistent Ordinal     Regression based on Conditional Probabilities'     manuscript.</p> <p>Parameters</p> <ul> <li> <p><code>num_classes</code> : int</p> <p>Number of unique class labels (class labels should start at 0).</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import corn_loss\n    &gt;&gt;&gt; # Consider 8 training examples\n    &gt;&gt;&gt; _  = torch.manual_seed(123)\n    &gt;&gt;&gt; X_train = torch.rand(8, 99)\n    &gt;&gt;&gt; y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n    &gt;&gt;&gt; NUM_CLASSES = 5\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def __init__(self):\n    &gt;&gt;&gt; corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def forward(self, X_train):\n    &gt;&gt;&gt; logits = corn_net(X_train)\n    &gt;&gt;&gt; logits.shape\n    torch.Size([8, 4])\n    &gt;&gt;&gt; corn_loss(logits, y_train, NUM_CLASSES)\n    tensor(0.7127, grad_fn=&lt;DivBackward0&gt;)\n</code></pre>"},{"location":"api_modules/coral_pytorch.losses/CornLoss/#methods","title":"Methods","text":"<p>add_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Adds a child module to the current module.</p> <pre><code>The module can be accessed as an attribute using the given name.\n\nArgs:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.\n</code></pre> <p>apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</p> <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)     as well as self. Typical use includes initializing the parameters of a model     (see also :ref:<code>nn-init-doc</code>).</p> <pre><code>Args:\nfn (:class:`Module` -&gt; None): function to be applied to each submodule\n\nReturns:\nModule: self\n\nExample::\n\n```\n&gt;&gt;&gt; @torch.no_grad()\n&gt;&gt;&gt; def init_weights(m):\n&gt;&gt;&gt;     print(m)\n&gt;&gt;&gt;     if type(m) == nn.Linear:\n&gt;&gt;&gt;         m.weight.fill_(1.0)\n&gt;&gt;&gt;         print(m.weight)\n&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n&gt;&gt;&gt; net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nSequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n</code></pre> <p>```</p> <p>bfloat16(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>buffers(recurse: bool = True) -&gt; Iterator[torch.Tensor]</p> <p>Returns an iterator over module buffers.</p> <pre><code>Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\ntorch.Tensor: module buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf), buf.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>children() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over immediate children modules.</p> <pre><code>Yields:\nModule: a child module\n</code></pre> <p>cpu(self: ~T) -&gt; ~T</p> <p>Moves all model parameters and buffers to the CPU.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the GPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>double(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>eval(self: ~T) -&gt; ~T</p> <p>Sets the module in evaluation mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.\n\nReturns:\nModule: self\n</code></pre> <p>extra_repr() -&gt; str</p> <p>Set the extra representation of the module</p> <pre><code>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.\n</code></pre> <p>float(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>forward(logits, y_train)</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(num_examples, num_classes-1)</p> <p>Outputs of the CORN layer.</p> </li> <li> <p><code>y_train</code> : torch.tensor, shape=(num_examples)</p> <p>Torch tensor containing the class labels.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value.</p> </li> </ul> <p>get_buffer(target: str) -&gt; 'Tensor'</p> <p>Returns the buffer given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the buffer\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.Tensor: The buffer referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer\n</code></pre> <p>get_extra_state() -&gt; Any</p> <p>Returns any extra state to include in the module's state_dict.     Implement this and a corresponding :func:<code>set_extra_state</code> for your module     if you need to store extra state. This function is called when building the     module's <code>state_dict()</code>.</p> <pre><code>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.\n\nReturns:\nobject: Any extra state to store in the module's state_dict\n</code></pre> <p>get_parameter(target: str) -&gt; 'Parameter'</p> <p>Returns the parameter given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Parameter: The Parameter referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Parameter``\n</code></pre> <p>get_submodule(target: str) -&gt; 'Module'</p> <p>Returns the submodule given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>For example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\nA(\n(net_b): Module(\n(net_c): Module(\n(conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n)\n(linear): Linear(in_features=100, out_features=200, bias=True)\n)\n)\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.\n\nArgs:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Module: The submodule referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Module``\n</code></pre> <p>half(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the IPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>load_state_dict(state_dict: Mapping[str, Any], strict: bool = True)</p> <p>Copies parameters and buffers from :attr:<code>state_dict</code> into     this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then     the keys of :attr:<code>state_dict</code> must exactly match the keys returned     by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <pre><code>Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:`state_dict` match the keys returned by this module's\n:meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\nReturns:\n``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n</code></pre> <ul> <li>missing_keys is a list of str containing the missing keys</li> <li>unexpected_keys is a list of str containing the unexpected keys</li> </ul> <p>Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>modules() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over all modules in the network.</p> <pre><code>Yields:\nModule: a module in the network\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n</code></pre> <p>```</p> <p>named_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</p> <p>Returns an iterator over module buffers, yielding both the     name of the buffer as well as the buffer itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\nYields:\n(str, torch.Tensor): Tuple containing the name and buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;     if name in ['running_var']:\n&gt;&gt;&gt;         print(buf.size())\n</code></pre> <p>```</p> <p>named_children() -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</p> <p>Returns an iterator over immediate children modules, yielding both     the name of the module as well as the module itself.</p> <pre><code>Yields:\n(str, Module): Tuple containing a name and child module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n</code></pre> <p>```</p> <p>named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)</p> <p>Returns an iterator over all modules in the network, yielding     both the name of the module as well as the module itself.</p> <pre><code>Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not\n\nYields:\n(str, Module): Tuple of name and module\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))\n</code></pre> <p>```</p> <p>named_parameters(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</p> <p>Returns an iterator over module parameters, yielding both the     name of the parameter as well as the parameter itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.\n\nYields:\n(str, Parameter): Tuple containing the name and parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;     if name in ['bias']:\n&gt;&gt;&gt;         print(param.size())\n</code></pre> <p>```</p> <p>parameters(recurse: bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</p> <p>Returns an iterator over module parameters.</p> <pre><code>This is typically passed to an optimizer.\n\nArgs:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\nParameter: module parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for param in model.parameters():\n&gt;&gt;&gt;     print(type(param), param.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -&gt; None</p> <p>Adds a buffer to the module.</p> <pre><code>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nArgs:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If ``None``, then operations\nthat run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n</code></pre> <p>the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's</p> <p>:attr:<code>state_dict</code>.</p> <pre><code>Example::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))\n</code></pre> <p>```</p> <p>register_forward_hook(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward hook on the module.</p> <pre><code>The hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\nhook(module, args, output) -&gt; None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\nhook(module, args, kwargs, output) -&gt; None or modified output\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If ``True``, the provided ``hook`` will be fired\nbefore all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks\nregistered by this method.\nDefault: ``False``\nwith_kwargs (bool): If ``True``, the ``hook`` will be passed the\nkwargs given to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_forward_pre_hook(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward pre-hook on the module.</p> <pre><code>The hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\nhook(module, args) -&gt; None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\nhook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all\nhooks registered by this method.\nDefault: ``False``\nwith_kwargs (bool): If true, the ``hook`` will be passed the kwargs\ngiven to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\nhook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_pre_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward pre-hook on the module.</p> <pre><code>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\nhook(module, grad_output) -&gt; Tensor or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_load_state_dict_post_hook(hook)</p> <p>Registers a post hook to be run after module's <code>load_state_dict</code>     is called.</p> <pre><code>It should have the following signature::\nhook(module, incompatible_keys) -&gt; None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Alias for :func:<code>add_module</code>.</p> <p>register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -&gt; None</p> <p>Adds a parameter to the module.</p> <pre><code>The parameter can be accessed as an attribute using given name.\n\nArgs:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,\n</code></pre> <p>are ignored. If <code>None</code>, the parameter is not included in the     module's :attr:<code>state_dict</code>.</p> <p>register_state_dict_pre_hook(hook)</p> <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>,     and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered     hooks can be used to perform pre-processing before the <code>state_dict</code>     call is made.</p> <p>requires_grad_(self: ~T, requires_grad: bool = True) -&gt; ~T</p> <p>Change if autograd should record operations on parameters in this     module.</p> <pre><code>This method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\n\nArgs:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>set_extra_state(state: Any)</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state     found within the <code>state_dict</code>. Implement this function and a corresponding     :func:<code>get_extra_state</code> for your module if you need to store extra state within its     <code>state_dict</code>.</p> <pre><code>Args:\nstate (dict): Extra state from the `state_dict`\n</code></pre> <p>share_memory(self: ~T) -&gt; ~T</p> <p>See :meth:<code>torch.Tensor.share_memory_</code></p> <p>state_dict(args, destination=None, prefix='', keep_vars=False)*</p> <p>Returns a dictionary containing references to the whole state of the module.</p> <pre><code>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.\n\n.. warning::\nCurrently ``state_dict()`` also accepts positional arguments for\n``destination``, ``prefix`` and ``keep_vars`` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.\n\n.. warning::\nPlease avoid the use of argument ``destination`` as it is not\ndesigned for end-users.\n\nArgs:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ``OrderedDict`` will be created and returned.\nDefault: ``None``.\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ``''``.\nkeep_vars (bool, optional): by default the :class:`~torch.Tensor` s\nreturned in the state dict are detached from autograd. If it's\nset to ``True``, detaching will not be performed.\nDefault: ``False``.\n\nReturns:\ndict:\na dictionary containing a whole state of the module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n</code></pre> <p>```</p> <p>to(args, *kwargs)</p> <p>Moves and/or casts the parameters and buffers.</p> <pre><code>This can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:\n\n.. function:: to(dtype, non_blocking=False)\n:noindex:\n\n.. function:: to(tensor, non_blocking=False)\n:noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n:noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (:class:`torch.device`): the desired device of the parameters\nand buffers in this module\ndtype (:class:`torch.dtype`): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:`torch.memory_format`): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\n\nReturns:\nModule: self\n\nExamples::\n\n```\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16)\n\n&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n</code></pre> <p>```</p> <p>to_empty(self: ~T, , device: Union[str, torch.device]) -&gt; ~T*</p> <p>Moves the parameters and buffers to the specified device without copying storage.</p> <pre><code>Args:\ndevice (:class:`torch.device`): The desired device of the parameters\nand buffers in this module.\n\nReturns:\nModule: self\n</code></pre> <p>train(self: ~T, mode: bool = True) -&gt; ~T</p> <p>Sets the module in training mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nArgs:\nmode (bool): whether to set training mode (``True``) or evaluation\nmode (``False``). Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>type(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</p> <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nArgs:\ndst_type (type or string): the desired type\n\nReturns:\nModule: self\n</code></pre> <p>xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the XPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>zero_grad(set_to_none: bool = True) -&gt; None</p> <p>Sets gradients of all model parameters to zero. See similar function     under :class:<code>torch.optim.Optimizer</code> for more context.</p> <pre><code>Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:`torch.optim.Optimizer.zero_grad` for details.\n</code></pre>"},{"location":"api_modules/coral_pytorch.losses/CornLoss/#properties","title":"Properties","text":""},{"location":"api_modules/coral_pytorch.losses/coral_loss/","title":"Coral loss","text":""},{"location":"api_modules/coral_pytorch.losses/coral_loss/#coral_loss","title":"coral_loss","text":"<p>coral_loss(logits, levels, importance_weights=None, reduction='mean')</p> <p>Computes the CORAL loss described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>Outputs of the CORAL layer.</p> </li> <li> <p><code>levels</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>True labels represented as extended binary vectors (via <code>coral_pytorch.dataset.levels_from_labelbatch</code>).</p> </li> <li> <p><code>importance_weights</code> : torch.tensor, shape=(num_classes-1,) (default=None)</p> <p>Optional weights for the different labels in levels. A tensor of ones, i.e., <code>torch.ones(num_classes-1, dtype=torch.float32)</code> will result in uniform weights that have the same effect as None.</p> </li> <li> <p><code>reduction</code> : str or None (default='mean')</p> <p>If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,)</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value (if <code>reduction='mean'</code> or '<code>sum'</code>) or a loss value for each data record (if <code>reduction=None</code>).</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import coral_loss\n    &gt;&gt;&gt; levels = torch.tensor(\n    ...    [[1., 1., 0., 0.],\n    ...     [1., 0., 0., 0.],\n    ...    [1., 1., 1., 1.]])\n    &gt;&gt;&gt; logits = torch.tensor(\n    ...    [[2.1, 1.8, -2.1, -1.8],\n    ...     [1.9, -1., -1.5, -1.3],\n    ...     [1.9, 1.8, 1.7, 1.6]])\n    &gt;&gt;&gt; coral_loss(logits, levels)\n    tensor(0.6920)\n</code></pre>"},{"location":"api_modules/coral_pytorch.losses/corn_loss/","title":"Corn loss","text":""},{"location":"api_modules/coral_pytorch.losses/corn_loss/#corn_loss","title":"corn_loss","text":"<p>corn_loss(logits, y_train, num_classes)</p> <p>Computes the CORN loss described in our forthcoming     'Deep Neural Networks for Rank Consistent Ordinal     Regression based on Conditional Probabilities'     manuscript.</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(num_examples, num_classes-1)</p> <p>Outputs of the CORN layer.</p> </li> <li> <p><code>y_train</code> : torch.tensor, shape=(num_examples)</p> <p>Torch tensor containing the class labels.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>Number of unique class labels (class labels should start at 0).</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import corn_loss\n    &gt;&gt;&gt; # Consider 8 training examples\n    &gt;&gt;&gt; _  = torch.manual_seed(123)\n    &gt;&gt;&gt; X_train = torch.rand(8, 99)\n    &gt;&gt;&gt; y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n    &gt;&gt;&gt; NUM_CLASSES = 5\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def __init__(self):\n    &gt;&gt;&gt; corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def forward(self, X_train):\n    &gt;&gt;&gt; logits = corn_net(X_train)\n    &gt;&gt;&gt; logits.shape\n    torch.Size([8, 4])\n    &gt;&gt;&gt; corn_loss(logits, y_train, NUM_CLASSES)\n    tensor(0.7127, grad_fn=&lt;DivBackward0&gt;)\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.dataset/","title":"coral_pytorch.dataset","text":"<p>coral_pytorch version: 1.4.0</p>"},{"location":"api_subpackages/coral_pytorch.dataset/#label_to_levels","title":"label_to_levels","text":"<p>label_to_levels(label, num_classes, dtype=torch.float32)</p> <p>Converts integer class label to extended binary label vector</p> <p>Parameters</p> <ul> <li> <p><code>label</code> : int</p> <p>Class label to be converted into a extended binary vector. Should be smaller than num_classes-1.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector.</p> </li> <li> <p><code>dtype</code> : torch data type (default=torch.float32)</p> <p>Data type of the torch output vector for the extended binary labels.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>levels</code> : torch.tensor, shape=(num_classes-1,)</p> <p>Extended binary label vector. Type is determined by the <code>dtype</code> parameter.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; label_to_levels(0, num_classes=5)\n    tensor([0., 0., 0., 0.])\n    &gt;&gt;&gt; label_to_levels(1, num_classes=5)\n    tensor([1., 0., 0., 0.])\n    &gt;&gt;&gt; label_to_levels(3, num_classes=5)\n    tensor([1., 1., 1., 0.])\n    &gt;&gt;&gt; label_to_levels(4, num_classes=5)\n    tensor([1., 1., 1., 1.])\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.dataset/#proba_to_label","title":"proba_to_label","text":"<p>proba_to_label(probas)</p> <p>Converts predicted probabilities from extended binary format     to integer class labels</p> <p>Parameters</p> <ul> <li> <p><code>probas</code> : torch.tensor, shape(n_examples, n_labels)</p> <p>Torch tensor consisting of probabilities returned by CORAL model.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; # 3 training examples, 6 classes\n    &gt;&gt;&gt; probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295],\n    ...                        [0.496, 0.485, 0.267, 0.124, 0.058],\n    ...                        [0.985, 0.967, 0.920, 0.819, 0.506]])\n    &gt;&gt;&gt; proba_to_label(probas)\n    tensor([2, 0, 5])\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.dataset/#corn_label_from_logits","title":"corn_label_from_logits","text":"<p>corn_label_from_logits(logits)</p> <p>Returns the predicted rank label from logits for a     network trained via the CORN loss.</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(n_examples, n_classes)</p> <p>Torch tensor consisting of logits returned by the neural net.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>labels</code> : torch.tensor, shape=(n_examples)</p> <p>Integer tensor containing the predicted rank (class) labels</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; # 2 training examples, 5 classes\n    &gt;&gt;&gt; logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850],\n    ...                        [65.667, 0.303, 11.500, -4.524]])\n    &gt;&gt;&gt; corn_label_from_logits(logits)\n    tensor([1, 3])\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.dataset/#levels_from_labelbatch","title":"levels_from_labelbatch","text":"<p>levels_from_labelbatch(labels, num_classes, dtype=torch.float32)</p> <p>Converts a list of integer class label to extended binary label vectors</p> <p>Parameters</p> <ul> <li> <p><code>labels</code> : list or 1D orch.tensor, shape=(num_labels,)</p> <p>A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector.</p> </li> <li> <p><code>dtype</code> : torch data type (default=torch.float32)</p> <p>Data type of the torch output vector for the extended binary labels.</p> </li> </ul> <p>Returns</p> <ul> <li><code>levels</code> : torch.tensor, shape=(num_labels, num_classes-1)</li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; levels_from_labelbatch(labels=[2, 1, 4], num_classes=5)\n    tensor([[1., 1., 0., 0.],\n    [1., 0., 0., 0.],\n    [1., 1., 1., 1.]])\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.layers/","title":"coral_pytorch.layers","text":"<p>coral_pytorch version: 1.4.0</p>"},{"location":"api_subpackages/coral_pytorch.layers/#corallayer","title":"CoralLayer","text":"<p>CoralLayer(size_in, num_classes, preinit_bias=True)</p> <p>Implements CORAL layer described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>size_in</code> : int</p> <p>Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features).</p> </li> <li> <p><code>num_classes</code> : int</p> <p>Number of classes in the dataset.</p> </li> <li> <p><code>preinit_bias</code> : bool (default=True)</p> <p>If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.</p> </li> </ul>"},{"location":"api_subpackages/coral_pytorch.losses/","title":"coral_pytorch.losses","text":"<p>coral_pytorch version: 1.4.0</p>"},{"location":"api_subpackages/coral_pytorch.losses/#cornloss","title":"CornLoss","text":"<p>CornLoss(num_classes)</p> <p>Computes the CORN loss described in our forthcoming     'Deep Neural Networks for Rank Consistent Ordinal     Regression based on Conditional Probabilities'     manuscript.</p> <p>Parameters</p> <ul> <li> <p><code>num_classes</code> : int</p> <p>Number of unique class labels (class labels should start at 0).</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import corn_loss\n    &gt;&gt;&gt; # Consider 8 training examples\n    &gt;&gt;&gt; _  = torch.manual_seed(123)\n    &gt;&gt;&gt; X_train = torch.rand(8, 99)\n    &gt;&gt;&gt; y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n    &gt;&gt;&gt; NUM_CLASSES = 5\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def __init__(self):\n    &gt;&gt;&gt; corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def forward(self, X_train):\n    &gt;&gt;&gt; logits = corn_net(X_train)\n    &gt;&gt;&gt; logits.shape\n    torch.Size([8, 4])\n    &gt;&gt;&gt; corn_loss(logits, y_train, NUM_CLASSES)\n    tensor(0.7127, grad_fn=&lt;DivBackward0&gt;)\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.losses/#methods","title":"Methods","text":"<p>add_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Adds a child module to the current module.</p> <pre><code>The module can be accessed as an attribute using the given name.\n\nArgs:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.\n</code></pre> <p>apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</p> <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)     as well as self. Typical use includes initializing the parameters of a model     (see also :ref:<code>nn-init-doc</code>).</p> <pre><code>Args:\nfn (:class:`Module` -&gt; None): function to be applied to each submodule\n\nReturns:\nModule: self\n\nExample::\n\n```\n&gt;&gt;&gt; @torch.no_grad()\n&gt;&gt;&gt; def init_weights(m):\n&gt;&gt;&gt;     print(m)\n&gt;&gt;&gt;     if type(m) == nn.Linear:\n&gt;&gt;&gt;         m.weight.fill_(1.0)\n&gt;&gt;&gt;         print(m.weight)\n&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n&gt;&gt;&gt; net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nSequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n</code></pre> <p>```</p> <p>bfloat16(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>buffers(recurse: bool = True) -&gt; Iterator[torch.Tensor]</p> <p>Returns an iterator over module buffers.</p> <pre><code>Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\ntorch.Tensor: module buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf), buf.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>children() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over immediate children modules.</p> <pre><code>Yields:\nModule: a child module\n</code></pre> <p>cpu(self: ~T) -&gt; ~T</p> <p>Moves all model parameters and buffers to the CPU.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the GPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>double(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>eval(self: ~T) -&gt; ~T</p> <p>Sets the module in evaluation mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.\n\nReturns:\nModule: self\n</code></pre> <p>extra_repr() -&gt; str</p> <p>Set the extra representation of the module</p> <pre><code>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.\n</code></pre> <p>float(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>forward(logits, y_train)</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(num_examples, num_classes-1)</p> <p>Outputs of the CORN layer.</p> </li> <li> <p><code>y_train</code> : torch.tensor, shape=(num_examples)</p> <p>Torch tensor containing the class labels.</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value.</p> </li> </ul> <p>get_buffer(target: str) -&gt; 'Tensor'</p> <p>Returns the buffer given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the buffer\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.Tensor: The buffer referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer\n</code></pre> <p>get_extra_state() -&gt; Any</p> <p>Returns any extra state to include in the module's state_dict.     Implement this and a corresponding :func:<code>set_extra_state</code> for your module     if you need to store extra state. This function is called when building the     module's <code>state_dict()</code>.</p> <pre><code>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.\n\nReturns:\nobject: Any extra state to store in the module's state_dict\n</code></pre> <p>get_parameter(target: str) -&gt; 'Parameter'</p> <p>Returns the parameter given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Parameter: The Parameter referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Parameter``\n</code></pre> <p>get_submodule(target: str) -&gt; 'Module'</p> <p>Returns the submodule given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>For example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\nA(\n(net_b): Module(\n(net_c): Module(\n(conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n)\n(linear): Linear(in_features=100, out_features=200, bias=True)\n)\n)\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.\n\nArgs:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Module: The submodule referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Module``\n</code></pre> <p>half(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the IPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>load_state_dict(state_dict: Mapping[str, Any], strict: bool = True)</p> <p>Copies parameters and buffers from :attr:<code>state_dict</code> into     this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then     the keys of :attr:<code>state_dict</code> must exactly match the keys returned     by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <pre><code>Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:`state_dict` match the keys returned by this module's\n:meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\nReturns:\n``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n</code></pre> <ul> <li>missing_keys is a list of str containing the missing keys</li> <li>unexpected_keys is a list of str containing the unexpected keys</li> </ul> <p>Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>modules() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over all modules in the network.</p> <pre><code>Yields:\nModule: a module in the network\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n</code></pre> <p>```</p> <p>named_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</p> <p>Returns an iterator over module buffers, yielding both the     name of the buffer as well as the buffer itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\nYields:\n(str, torch.Tensor): Tuple containing the name and buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;     if name in ['running_var']:\n&gt;&gt;&gt;         print(buf.size())\n</code></pre> <p>```</p> <p>named_children() -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</p> <p>Returns an iterator over immediate children modules, yielding both     the name of the module as well as the module itself.</p> <pre><code>Yields:\n(str, Module): Tuple containing a name and child module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n</code></pre> <p>```</p> <p>named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)</p> <p>Returns an iterator over all modules in the network, yielding     both the name of the module as well as the module itself.</p> <pre><code>Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not\n\nYields:\n(str, Module): Tuple of name and module\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))\n</code></pre> <p>```</p> <p>named_parameters(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</p> <p>Returns an iterator over module parameters, yielding both the     name of the parameter as well as the parameter itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.\n\nYields:\n(str, Parameter): Tuple containing the name and parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;     if name in ['bias']:\n&gt;&gt;&gt;         print(param.size())\n</code></pre> <p>```</p> <p>parameters(recurse: bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</p> <p>Returns an iterator over module parameters.</p> <pre><code>This is typically passed to an optimizer.\n\nArgs:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\nParameter: module parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for param in model.parameters():\n&gt;&gt;&gt;     print(type(param), param.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -&gt; None</p> <p>Adds a buffer to the module.</p> <pre><code>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nArgs:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If ``None``, then operations\nthat run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n</code></pre> <p>the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's</p> <p>:attr:<code>state_dict</code>.</p> <pre><code>Example::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))\n</code></pre> <p>```</p> <p>register_forward_hook(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward hook on the module.</p> <pre><code>The hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\nhook(module, args, output) -&gt; None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\nhook(module, args, kwargs, output) -&gt; None or modified output\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If ``True``, the provided ``hook`` will be fired\nbefore all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks\nregistered by this method.\nDefault: ``False``\nwith_kwargs (bool): If ``True``, the ``hook`` will be passed the\nkwargs given to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_forward_pre_hook(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward pre-hook on the module.</p> <pre><code>The hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\nhook(module, args) -&gt; None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\nhook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all\nhooks registered by this method.\nDefault: ``False``\nwith_kwargs (bool): If true, the ``hook`` will be passed the kwargs\ngiven to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\nhook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_pre_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward pre-hook on the module.</p> <pre><code>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\nhook(module, grad_output) -&gt; Tensor or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_load_state_dict_post_hook(hook)</p> <p>Registers a post hook to be run after module's <code>load_state_dict</code>     is called.</p> <pre><code>It should have the following signature::\nhook(module, incompatible_keys) -&gt; None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Alias for :func:<code>add_module</code>.</p> <p>register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -&gt; None</p> <p>Adds a parameter to the module.</p> <pre><code>The parameter can be accessed as an attribute using given name.\n\nArgs:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,\n</code></pre> <p>are ignored. If <code>None</code>, the parameter is not included in the     module's :attr:<code>state_dict</code>.</p> <p>register_state_dict_pre_hook(hook)</p> <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>,     and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered     hooks can be used to perform pre-processing before the <code>state_dict</code>     call is made.</p> <p>requires_grad_(self: ~T, requires_grad: bool = True) -&gt; ~T</p> <p>Change if autograd should record operations on parameters in this     module.</p> <pre><code>This method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\n\nArgs:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>set_extra_state(state: Any)</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state     found within the <code>state_dict</code>. Implement this function and a corresponding     :func:<code>get_extra_state</code> for your module if you need to store extra state within its     <code>state_dict</code>.</p> <pre><code>Args:\nstate (dict): Extra state from the `state_dict`\n</code></pre> <p>share_memory(self: ~T) -&gt; ~T</p> <p>See :meth:<code>torch.Tensor.share_memory_</code></p> <p>state_dict(args, destination=None, prefix='', keep_vars=False)*</p> <p>Returns a dictionary containing references to the whole state of the module.</p> <pre><code>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.\n\n.. warning::\nCurrently ``state_dict()`` also accepts positional arguments for\n``destination``, ``prefix`` and ``keep_vars`` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.\n\n.. warning::\nPlease avoid the use of argument ``destination`` as it is not\ndesigned for end-users.\n\nArgs:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ``OrderedDict`` will be created and returned.\nDefault: ``None``.\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ``''``.\nkeep_vars (bool, optional): by default the :class:`~torch.Tensor` s\nreturned in the state dict are detached from autograd. If it's\nset to ``True``, detaching will not be performed.\nDefault: ``False``.\n\nReturns:\ndict:\na dictionary containing a whole state of the module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n</code></pre> <p>```</p> <p>to(args, *kwargs)</p> <p>Moves and/or casts the parameters and buffers.</p> <pre><code>This can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:\n\n.. function:: to(dtype, non_blocking=False)\n:noindex:\n\n.. function:: to(tensor, non_blocking=False)\n:noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n:noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (:class:`torch.device`): the desired device of the parameters\nand buffers in this module\ndtype (:class:`torch.dtype`): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:`torch.memory_format`): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\n\nReturns:\nModule: self\n\nExamples::\n\n```\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16)\n\n&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n</code></pre> <p>```</p> <p>to_empty(self: ~T, , device: Union[str, torch.device]) -&gt; ~T*</p> <p>Moves the parameters and buffers to the specified device without copying storage.</p> <pre><code>Args:\ndevice (:class:`torch.device`): The desired device of the parameters\nand buffers in this module.\n\nReturns:\nModule: self\n</code></pre> <p>train(self: ~T, mode: bool = True) -&gt; ~T</p> <p>Sets the module in training mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nArgs:\nmode (bool): whether to set training mode (``True``) or evaluation\nmode (``False``). Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>type(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</p> <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nArgs:\ndst_type (type or string): the desired type\n\nReturns:\nModule: self\n</code></pre> <p>xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the XPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>zero_grad(set_to_none: bool = True) -&gt; None</p> <p>Sets gradients of all model parameters to zero. See similar function     under :class:<code>torch.optim.Optimizer</code> for more context.</p> <pre><code>Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:`torch.optim.Optimizer.zero_grad` for details.\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.losses/#properties","title":"Properties","text":""},{"location":"api_subpackages/coral_pytorch.losses/#coralloss","title":"CoralLoss","text":"<p>CoralLoss(reduction='mean')</p> <p>Computes the CORAL loss described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>reduction</code> : str or None (default='mean')</p> <p>If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,)</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import CoralLoss\n    &gt;&gt;&gt; levels = torch.tensor(\n    ...    [[1., 1., 0., 0.],\n    ...     [1., 0., 0., 0.],\n    ...    [1., 1., 1., 1.]])\n    &gt;&gt;&gt; logits = torch.tensor(\n    ...    [[2.1, 1.8, -2.1, -1.8],\n    ...     [1.9, -1., -1.5, -1.3],\n    ...     [1.9, 1.8, 1.7, 1.6]])\n    &gt;&gt;&gt; loss = CoralLoss()\n    &gt;&gt;&gt; loss(logits, levels)\n    tensor(0.6920)\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.losses/#methods_1","title":"Methods","text":"<p>add_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Adds a child module to the current module.</p> <pre><code>The module can be accessed as an attribute using the given name.\n\nArgs:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.\n</code></pre> <p>apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -&gt; ~T</p> <p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>)     as well as self. Typical use includes initializing the parameters of a model     (see also :ref:<code>nn-init-doc</code>).</p> <pre><code>Args:\nfn (:class:`Module` -&gt; None): function to be applied to each submodule\n\nReturns:\nModule: self\n\nExample::\n\n```\n&gt;&gt;&gt; @torch.no_grad()\n&gt;&gt;&gt; def init_weights(m):\n&gt;&gt;&gt;     print(m)\n&gt;&gt;&gt;     if type(m) == nn.Linear:\n&gt;&gt;&gt;         m.weight.fill_(1.0)\n&gt;&gt;&gt;         print(m.weight)\n&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n&gt;&gt;&gt; net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nSequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n</code></pre> <p>```</p> <p>bfloat16(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>buffers(recurse: bool = True) -&gt; Iterator[torch.Tensor]</p> <p>Returns an iterator over module buffers.</p> <pre><code>Args:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\ntorch.Tensor: module buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf), buf.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>children() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over immediate children modules.</p> <pre><code>Yields:\nModule: a child module\n</code></pre> <p>cpu(self: ~T) -&gt; ~T</p> <p>Moves all model parameters and buffers to the CPU.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the GPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>double(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>eval(self: ~T) -&gt; ~T</p> <p>Sets the module in evaluation mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.\n\nReturns:\nModule: self\n</code></pre> <p>extra_repr() -&gt; str</p> <p>Set the extra representation of the module</p> <pre><code>To print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.\n</code></pre> <p>float(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>forward(logits, levels, importance_weights=None)</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>Outputs of the CORAL layer.</p> </li> <li> <p><code>levels</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>True labels represented as extended binary vectors (via <code>coral_pytorch.dataset.levels_from_labelbatch</code>).</p> </li> <li> <p><code>importance_weights</code> : torch.tensor, shape=(num_classes-1,) (default=None)</p> <p>Optional weights for the different labels in levels. A tensor of ones, i.e., <code>torch.ones(num_classes-1, dtype=torch.float32)</code> will result in uniform weights that have the same effect as None.</p> </li> </ul> <p>get_buffer(target: str) -&gt; 'Tensor'</p> <p>Returns the buffer given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the buffer\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.Tensor: The buffer referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer\n</code></pre> <p>get_extra_state() -&gt; Any</p> <p>Returns any extra state to include in the module's state_dict.     Implement this and a corresponding :func:<code>set_extra_state</code> for your module     if you need to store extra state. This function is called when building the     module's <code>state_dict()</code>.</p> <pre><code>Note that extra state should be picklable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.\n\nReturns:\nobject: Any extra state to store in the module's state_dict\n</code></pre> <p>get_parameter(target: str) -&gt; 'Parameter'</p> <p>Returns the parameter given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>See the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Parameter: The Parameter referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Parameter``\n</code></pre> <p>get_submodule(target: str) -&gt; 'Module'</p> <p>Returns the submodule given by <code>target</code> if it exists,     otherwise throws an error.</p> <pre><code>For example, let's say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\nA(\n(net_b): Module(\n(net_c): Module(\n(conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n)\n(linear): Linear(in_features=100, out_features=200, bias=True)\n)\n)\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule(\"net_b.linear\")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule(\"net_b.net_c.conv\")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.\n\nArgs:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Module: The submodule referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Module``\n</code></pre> <p>half(self: ~T) -&gt; ~T</p> <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n</code></pre> <p>ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the IPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>load_state_dict(state_dict: Mapping[str, Any], strict: bool = True)</p> <p>Copies parameters and buffers from :attr:<code>state_dict</code> into     this module and its descendants. If :attr:<code>strict</code> is <code>True</code>, then     the keys of :attr:<code>state_dict</code> must exactly match the keys returned     by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.</p> <pre><code>Args:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:`state_dict` match the keys returned by this module's\n:meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\nReturns:\n``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n</code></pre> <ul> <li>missing_keys is a list of str containing the missing keys</li> <li>unexpected_keys is a list of str containing the unexpected keys</li> </ul> <p>Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>modules() -&gt; Iterator[ForwardRef('Module')]</p> <p>Returns an iterator over all modules in the network.</p> <pre><code>Yields:\nModule: a module in the network\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n</code></pre> <p>```</p> <p>named_buffers(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.Tensor]]</p> <p>Returns an iterator over module buffers, yielding both the     name of the buffer as well as the buffer itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool, optional): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module. Defaults to True.\nremove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n\nYields:\n(str, torch.Tensor): Tuple containing the name and buffer\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;     if name in ['running_var']:\n&gt;&gt;&gt;         print(buf.size())\n</code></pre> <p>```</p> <p>named_children() -&gt; Iterator[Tuple[str, ForwardRef('Module')]]</p> <p>Returns an iterator over immediate children modules, yielding both     the name of the module as well as the module itself.</p> <pre><code>Yields:\n(str, Module): Tuple containing a name and child module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n</code></pre> <p>```</p> <p>named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)</p> <p>Returns an iterator over all modules in the network, yielding     both the name of the module as well as the module itself.</p> <pre><code>Args:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not\n\nYields:\n(str, Module): Tuple of name and module\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n```\n&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))\n</code></pre> <p>```</p> <p>named_parameters(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -&gt; Iterator[Tuple[str, torch.nn.parameter.Parameter]]</p> <p>Returns an iterator over module parameters, yielding both the     name of the parameter as well as the parameter itself.</p> <pre><code>Args:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\nremove_duplicate (bool, optional): whether to remove the duplicated\nparameters in the result. Defaults to True.\n\nYields:\n(str, Parameter): Tuple containing the name and parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;     if name in ['bias']:\n&gt;&gt;&gt;         print(param.size())\n</code></pre> <p>```</p> <p>parameters(recurse: bool = True) -&gt; Iterator[torch.nn.parameter.Parameter]</p> <p>Returns an iterator over module parameters.</p> <pre><code>This is typically passed to an optimizer.\n\nArgs:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\nParameter: module parameter\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for param in model.parameters():\n&gt;&gt;&gt;     print(type(param), param.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> <p>```</p> <p>register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -&gt; None</p> <p>Adds a buffer to the module.</p> <pre><code>This is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nArgs:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If ``None``, then operations\nthat run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n</code></pre> <p>the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's</p> <p>:attr:<code>state_dict</code>.</p> <pre><code>Example::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))\n</code></pre> <p>```</p> <p>register_forward_hook(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward hook on the module.</p> <pre><code>The hook will be called every time after :func:`forward` has computed an output.\n\nIf ``with_kwargs`` is ``False`` or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\noutput. It can modify the input inplace but it will not have effect on\nforward since this is called after :func:`forward` is called. The hook\nshould have the following signature::\n\nhook(module, args, output) -&gt; None or modified output\n\nIf ``with_kwargs`` is ``True``, the forward hook will be passed the\n``kwargs`` given to the forward function and be expected to return the\noutput possibly modified. The hook should have the following signature::\n\nhook(module, args, kwargs, output) -&gt; None or modified output\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If ``True``, the provided ``hook`` will be fired\nbefore all existing ``forward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``forward`` hooks registered with\n:func:`register_module_forward_hook` will fire before all hooks\nregistered by this method.\nDefault: ``False``\nwith_kwargs (bool): If ``True``, the ``hook`` will be passed the\nkwargs given to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_forward_pre_hook(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], , prepend: bool = False, with_kwargs: bool = False) -&gt; torch.utils.hooks.RemovableHandle*</p> <p>Registers a forward pre-hook on the module.</p> <pre><code>The hook will be called every time before :func:`forward` is invoked.\n\n\nIf ``with_kwargs`` is false or not specified, the input contains only\nthe positional arguments given to the module. Keyword arguments won't be\npassed to the hooks and only to the ``forward``. The hook can modify the\ninput. User can either return a tuple or a single modified value in the\nhook. We will wrap the value into a tuple if a single value is returned\n(unless that value is already a tuple). The hook should have the\nfollowing signature::\n\nhook(module, args) -&gt; None or modified input\n\nIf ``with_kwargs`` is true, the forward pre-hook will be passed the\nkwargs given to the forward function. And if the hook modifies the\ninput, both the args and kwargs should be returned. The hook should have\nthe following signature::\n\nhook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\nArgs:\nhook (Callable): The user defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``forward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``forward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``forward_pre`` hooks registered with\n:func:`register_module_forward_pre_hook` will fire before all\nhooks registered by this method.\nDefault: ``False``\nwith_kwargs (bool): If true, the ``hook`` will be passed the kwargs\ngiven to the forward function.\nDefault: ``False``\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward hook on the module.</p> <pre><code>The hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\nhook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward`` hooks on\nthis :class:`torch.nn.modules.Module`. Note that global\n``backward`` hooks registered with\n:func:`register_module_full_backward_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_full_backward_pre_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -&gt; torch.utils.hooks.RemovableHandle</p> <p>Registers a backward pre-hook on the module.</p> <pre><code>The hook will be called every time the gradients for the module are computed.\nThe hook should have the following signature::\n\nhook(module, grad_output) -&gt; Tensor or None\n\nThe :attr:`grad_output` is a tuple. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the output that will be used in place of :attr:`grad_output` in\nsubsequent computations. Entries in :attr:`grad_output` will be ``None`` for\nall non-Tensor arguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nArgs:\nhook (Callable): The user-defined hook to be registered.\nprepend (bool): If true, the provided ``hook`` will be fired before\nall existing ``backward_pre`` hooks on this\n:class:`torch.nn.modules.Module`. Otherwise, the provided\n``hook`` will be fired after all existing ``backward_pre`` hooks\non this :class:`torch.nn.modules.Module`. Note that global\n``backward_pre`` hooks registered with\n:func:`register_module_full_backward_pre_hook` will fire before\nall hooks registered by this method.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_load_state_dict_post_hook(hook)</p> <p>Registers a post hook to be run after module's <code>load_state_dict</code>     is called.</p> <pre><code>It should have the following signature::\nhook(module, incompatible_keys) -&gt; None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearing out both missing and unexpected keys will avoid an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n</code></pre> <p>register_module(name: str, module: Optional[ForwardRef('Module')]) -&gt; None</p> <p>Alias for :func:<code>add_module</code>.</p> <p>register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -&gt; None</p> <p>Adds a parameter to the module.</p> <pre><code>The parameter can be accessed as an attribute using given name.\n\nArgs:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,\n</code></pre> <p>are ignored. If <code>None</code>, the parameter is not included in the     module's :attr:<code>state_dict</code>.</p> <p>register_state_dict_pre_hook(hook)</p> <p>These hooks will be called with arguments: <code>self</code>, <code>prefix</code>,     and <code>keep_vars</code> before calling <code>state_dict</code> on <code>self</code>. The registered     hooks can be used to perform pre-processing before the <code>state_dict</code>     call is made.</p> <p>requires_grad_(self: ~T, requires_grad: bool = True) -&gt; ~T</p> <p>Change if autograd should record operations on parameters in this     module.</p> <pre><code>This method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\n\nArgs:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>set_extra_state(state: Any)</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state     found within the <code>state_dict</code>. Implement this function and a corresponding     :func:<code>get_extra_state</code> for your module if you need to store extra state within its     <code>state_dict</code>.</p> <pre><code>Args:\nstate (dict): Extra state from the `state_dict`\n</code></pre> <p>share_memory(self: ~T) -&gt; ~T</p> <p>See :meth:<code>torch.Tensor.share_memory_</code></p> <p>state_dict(args, destination=None, prefix='', keep_vars=False)*</p> <p>Returns a dictionary containing references to the whole state of the module.</p> <pre><code>Both parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.\n\n.. warning::\nCurrently ``state_dict()`` also accepts positional arguments for\n``destination``, ``prefix`` and ``keep_vars`` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.\n\n.. warning::\nPlease avoid the use of argument ``destination`` as it is not\ndesigned for end-users.\n\nArgs:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ``OrderedDict`` will be created and returned.\nDefault: ``None``.\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ``''``.\nkeep_vars (bool, optional): by default the :class:`~torch.Tensor` s\nreturned in the state dict are detached from autograd. If it's\nset to ``True``, detaching will not be performed.\nDefault: ``False``.\n\nReturns:\ndict:\na dictionary containing a whole state of the module\n\nExample::\n\n```\n&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n</code></pre> <p>```</p> <p>to(args, *kwargs)</p> <p>Moves and/or casts the parameters and buffers.</p> <pre><code>This can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:\n\n.. function:: to(dtype, non_blocking=False)\n:noindex:\n\n.. function:: to(tensor, non_blocking=False)\n:noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n:noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (:class:`torch.device`): the desired device of the parameters\nand buffers in this module\ndtype (:class:`torch.dtype`): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:`torch.memory_format`): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\n\nReturns:\nModule: self\n\nExamples::\n\n```\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16)\n\n&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n</code></pre> <p>```</p> <p>to_empty(self: ~T, , device: Union[str, torch.device]) -&gt; ~T*</p> <p>Moves the parameters and buffers to the specified device without copying storage.</p> <pre><code>Args:\ndevice (:class:`torch.device`): The desired device of the parameters\nand buffers in this module.\n\nReturns:\nModule: self\n</code></pre> <p>train(self: ~T, mode: bool = True) -&gt; ~T</p> <p>Sets the module in training mode.</p> <pre><code>This has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nArgs:\nmode (bool): whether to set training mode (``True``) or evaluation\nmode (``False``). Default: ``True``.\n\nReturns:\nModule: self\n</code></pre> <p>type(self: ~T, dst_type: Union[torch.dtype, str]) -&gt; ~T</p> <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <pre><code>.. note::\nThis method modifies the module in-place.\n\nArgs:\ndst_type (type or string): the desired type\n\nReturns:\nModule: self\n</code></pre> <p>xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -&gt; ~T</p> <p>Moves all model parameters and buffers to the XPU.</p> <pre><code>This also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n</code></pre> <p>zero_grad(set_to_none: bool = True) -&gt; None</p> <p>Sets gradients of all model parameters to zero. See similar function     under :class:<code>torch.optim.Optimizer</code> for more context.</p> <pre><code>Args:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:`torch.optim.Optimizer.zero_grad` for details.\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.losses/#properties_1","title":"Properties","text":""},{"location":"api_subpackages/coral_pytorch.losses/#corn_loss","title":"corn_loss","text":"<p>corn_loss(logits, y_train, num_classes)</p> <p>Computes the CORN loss described in our forthcoming     'Deep Neural Networks for Rank Consistent Ordinal     Regression based on Conditional Probabilities'     manuscript.</p> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape=(num_examples, num_classes-1)</p> <p>Outputs of the CORN layer.</p> </li> <li> <p><code>y_train</code> : torch.tensor, shape=(num_examples)</p> <p>Torch tensor containing the class labels.</p> </li> <li> <p><code>num_classes</code> : int</p> <p>Number of unique class labels (class labels should start at 0).</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value.</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import corn_loss\n    &gt;&gt;&gt; # Consider 8 training examples\n    &gt;&gt;&gt; _  = torch.manual_seed(123)\n    &gt;&gt;&gt; X_train = torch.rand(8, 99)\n    &gt;&gt;&gt; y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n    &gt;&gt;&gt; NUM_CLASSES = 5\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def __init__(self):\n    &gt;&gt;&gt; corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; #\n    &gt;&gt;&gt; # def forward(self, X_train):\n    &gt;&gt;&gt; logits = corn_net(X_train)\n    &gt;&gt;&gt; logits.shape\n    torch.Size([8, 4])\n    &gt;&gt;&gt; corn_loss(logits, y_train, NUM_CLASSES)\n    tensor(0.7127, grad_fn=&lt;DivBackward0&gt;)\n</code></pre>"},{"location":"api_subpackages/coral_pytorch.losses/#coral_loss","title":"coral_loss","text":"<p>coral_loss(logits, levels, importance_weights=None, reduction='mean')</p> <p>Computes the CORAL loss described in</p> <pre><code>Cao, Mirjalili, and Raschka (2020)\n*Rank Consistent Ordinal Regression for Neural Networks\nwith Application to Age Estimation*\nPattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008\n</code></pre> <p>Parameters</p> <ul> <li> <p><code>logits</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>Outputs of the CORAL layer.</p> </li> <li> <p><code>levels</code> : torch.tensor, shape(num_examples, num_classes-1)</p> <p>True labels represented as extended binary vectors (via <code>coral_pytorch.dataset.levels_from_labelbatch</code>).</p> </li> <li> <p><code>importance_weights</code> : torch.tensor, shape=(num_classes-1,) (default=None)</p> <p>Optional weights for the different labels in levels. A tensor of ones, i.e., <code>torch.ones(num_classes-1, dtype=torch.float32)</code> will result in uniform weights that have the same effect as None.</p> </li> <li> <p><code>reduction</code> : str or None (default='mean')</p> <p>If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,)</p> </li> </ul> <p>Returns</p> <ul> <li> <p><code>loss</code> : torch.tensor</p> <p>A torch.tensor containing a single loss value (if <code>reduction='mean'</code> or '<code>sum'</code>) or a loss value for each data record (if <code>reduction=None</code>).</p> </li> </ul> <p>Examples</p> <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from coral_pytorch.losses import coral_loss\n    &gt;&gt;&gt; levels = torch.tensor(\n    ...    [[1., 1., 0., 0.],\n    ...     [1., 0., 0., 0.],\n    ...    [1., 1., 1., 1.]])\n    &gt;&gt;&gt; logits = torch.tensor(\n    ...    [[2.1, 1.8, -2.1, -1.8],\n    ...     [1.9, -1., -1.5, -1.3],\n    ...     [1.9, 1.8, 1.7, 1.6]])\n    &gt;&gt;&gt; coral_loss(logits, levels)\n    tensor(0.6920)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/","title":"CORAL MLP for predicting cement strength (cement_strength)","text":"<p>This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression. </p>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","title":"0 -- Obtaining and preparing the cement_strength dataset","text":"<p>We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv.</p> <p>First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN.</p> <p>This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column.</p> <pre><code>import pandas as pd\nimport numpy as np\n\n\ndata_df = pd.read_csv(\"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\")\n\ndata_df[\"response\"] = data_df[\"response\"]-1 # labels should start at 0\n\ndata_labels = data_df[\"response\"]\ndata_features = data_df.loc[:, [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\nprint('Number of features:', data_features.shape[1])\nprint('Number of examples:', data_features.shape[0])\nprint('Labels:', np.unique(data_labels.values))\n</code></pre> <pre><code>Number of features: 8\nNumber of examples: 998\nLabels: [0 1 2 3 4]\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#split-into-training-and-test-data","title":"Split into training and test data","text":"<pre><code>from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data_features.values,\n    data_labels.values,\n    test_size=0.2,\n    random_state=1,\n    stratify=data_labels.values)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#standardize-features","title":"Standardize features","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#1-setting-up-the-dataset-and-dataloader","title":"1 -- Setting up the dataset and dataloader","text":"<p>In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL.</p> <pre><code>import torch\n\n\n##########################\n### SETTINGS\n##########################\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.05\nnum_epochs = 20\nbatch_size = 128\n\n# Architecture\nNUM_CLASSES = 10\n\n# Other\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Training on', DEVICE)\n</code></pre> <pre><code>Training on cpu\n</code></pre> <pre><code>from torch.utils.data import Dataset\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, feature_array, label_array, dtype=np.float32):\n\n        self.features = feature_array.astype(np.float32)\n        self.labels = label_array\n\n    def __getitem__(self, index):\n        inputs = self.features[index]\n        label = self.labels[index]\n        return inputs, label\n\n    def __len__(self):\n        return self.labels.shape[0]\n</code></pre> <pre><code>import torch\nfrom torch.utils.data import DataLoader\n\n\n# Note transforms.ToTensor() scales input images\n# to 0-1 range\ntrain_dataset = MyDataset(X_train_std, y_train)\ntest_dataset = MyDataset(X_test_std, y_test)\n\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True, # want to shuffle the dataset\n                          num_workers=0) # number processes/CPUs to use\n\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         num_workers=0)\n\n# Checking the dataset\nfor inputs, labels in train_loader:  \n    print('Input batch dimensions:', inputs.shape)\n    print('Input label dimensions:', labels.shape)\n    break\n</code></pre> <pre><code>Input batch dimensions: torch.Size([128, 8])\nInput label dimensions: torch.Size([128])\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#2-equipping-mlp-with-coral-layer","title":"2 - Equipping MLP with CORAL layer","text":"<p>In this section, we are using the CoralLayer implemented in <code>coral_pytorch</code> to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer.</p> <p>Also, please use the <code>sigmoid</code> not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper).</p> <pre><code>from coral_pytorch.layers import CoralLayer\n\n\n\nclass MLP(torch.nn.Module):\n\n    def __init__(self, in_features, num_classes, num_hidden_1=300, num_hidden_2=300):\n        super().__init__()\n\n        self.my_network = torch.nn.Sequential(\n\n            # 1st hidden layer\n            torch.nn.Linear(in_features, num_hidden_1, bias=False),\n            torch.nn.LeakyReLU(),\n            torch.nn.Dropout(0.2),\n            torch.nn.BatchNorm1d(num_hidden_1),\n\n            # 2nd hidden layer\n            torch.nn.Linear(num_hidden_1, num_hidden_2, bias=False),\n            torch.nn.LeakyReLU(),\n            torch.nn.Dropout(0.2),\n            torch.nn.BatchNorm1d(num_hidden_2),\n        )\n\n        ### Specify CORAL layer\n        self.fc = CoralLayer(size_in=num_hidden_2, num_classes=num_classes)\n        ###--------------------------------------------------------------------###\n\n    def forward(self, x):\n        x = self.my_network(x)\n\n        ##### Use CORAL layer #####\n        logits =  self.fc(x)\n        probas = torch.sigmoid(logits)\n        ###--------------------------------------------------------------------###\n\n        return logits, probas\n\n\n\ntorch.manual_seed(random_seed)\nmodel = MLP(in_features=8, num_classes=NUM_CLASSES)\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#3-using-the-coral-loss-for-model-training","title":"3 - Using the CORAL loss for model training","text":"<p>During training, all you need to do is to </p> <p>1) convert the integer class labels into the extended binary label format using the <code>levels_from_labelbatch</code> provided via <code>coral_pytorch</code>:</p> <pre><code>        levels = levels_from_labelbatch(class_labels, \n                                        num_classes=NUM_CLASSES)\n</code></pre> <p>2) Apply the CORAL loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>        loss = coral_loss(logits, levels)\n</code></pre> <pre><code>from coral_pytorch.dataset import levels_from_labelbatch\nfrom coral_pytorch.losses import coral_loss\n\n\nfor epoch in range(num_epochs):\n\n    model = model.train()\n    for batch_idx, (features, class_labels) in enumerate(train_loader):\n\n        ##### Convert class labels for CORAL\n        levels = levels_from_labelbatch(class_labels, \n                                        num_classes=NUM_CLASSES)\n        ###--------------------------------------------------------------------###\n\n        features = features.to(DEVICE)\n        levels = levels.to(DEVICE)\n        logits, probas = model(features)\n\n        #### CORAL loss \n        loss = coral_loss(logits, levels)\n        ###--------------------------------------------------------------------###   \n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 200:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n                   %(epoch+1, num_epochs, batch_idx, \n                     len(train_loader), loss))\n</code></pre> <pre><code>Epoch: 001/020 | Batch 000/007 | Loss: 1.0222\nEpoch: 002/020 | Batch 000/007 | Loss: 1.1131\nEpoch: 003/020 | Batch 000/007 | Loss: 0.9594\nEpoch: 004/020 | Batch 000/007 | Loss: 0.9661\nEpoch: 005/020 | Batch 000/007 | Loss: 0.9792\nEpoch: 006/020 | Batch 000/007 | Loss: 1.0311\nEpoch: 007/020 | Batch 000/007 | Loss: 0.9157\nEpoch: 008/020 | Batch 000/007 | Loss: 0.8542\nEpoch: 009/020 | Batch 000/007 | Loss: 0.9652\nEpoch: 010/020 | Batch 000/007 | Loss: 0.9483\nEpoch: 011/020 | Batch 000/007 | Loss: 0.8316\nEpoch: 012/020 | Batch 000/007 | Loss: 0.9067\nEpoch: 013/020 | Batch 000/007 | Loss: 1.0139\nEpoch: 014/020 | Batch 000/007 | Loss: 0.8505\nEpoch: 015/020 | Batch 000/007 | Loss: 0.8289\nEpoch: 016/020 | Batch 000/007 | Loss: 0.8277\nEpoch: 017/020 | Batch 000/007 | Loss: 0.7669\nEpoch: 018/020 | Batch 000/007 | Loss: 0.8366\nEpoch: 019/020 | Batch 000/007 | Loss: 0.7514\nEpoch: 020/020 | Batch 000/007 | Loss: 0.8221\n</code></pre> <pre><code>from coral_pytorch.dataset import proba_to_label\n\n\ndef compute_mae_and_mse(model, data_loader, device):\n\n    with torch.no_grad():\n\n        mae, mse, acc, num_examples = 0., 0., 0., 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits, probas = model(features)\n            predicted_labels = proba_to_label(probas).float()\n\n            num_examples += targets.size(0)\n            mae += torch.sum(torch.abs(predicted_labels - targets))\n            mse += torch.sum((predicted_labels - targets)**2)\n\n        mae = mae / num_examples\n        mse = mse / num_examples\n        return mae, mse\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_cement/#4-evaluate-model","title":"4 -- Evaluate model","text":"<p>Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures.</p> <p>For this, we are going to use the <code>proba_to_label</code> utility function from <code>coral_pytorch</code> to convert the probabilities back to the orginal label.</p> <pre><code>from coral_pytorch.dataset import proba_to_label\n\n\ndef compute_mae_and_mse(model, data_loader, device):\n\n    with torch.no_grad():\n\n        mae, mse, acc, num_examples = 0., 0., 0., 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits, probas = model(features)\n            predicted_labels = proba_to_label(probas).float()\n\n            num_examples += targets.size(0)\n            mae += torch.sum(torch.abs(predicted_labels - targets))\n            mse += torch.sum((predicted_labels - targets)**2)\n\n        mae = mae / num_examples\n        mse = mse / num_examples\n        return mae, mse\n</code></pre> <pre><code>train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\ntest_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n</code></pre> <pre><code>print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}')\nprint(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}')\n</code></pre> <pre><code>Mean absolute error (train/test): 0.27 | 0.34\nMean squared error (train/test): 0.28 | 0.34\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_mnist/","title":"CORAL CNN for predicting handwritten digits (MNIST)","text":"<p>This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</p>"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#1-setting-up-the-dataset-and-dataloader","title":"1 -- Setting up the dataset and dataloader","text":"<p>In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL.</p> <pre><code>import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n##########################\n### SETTINGS\n##########################\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.05\nnum_epochs = 10\nbatch_size = 128\n\n# Architecture\nNUM_CLASSES = 10\n\n# Other\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Training on', DEVICE)\n\n##########################\n### MNIST DATASET\n##########################\n\n\n# Note transforms.ToTensor() scales input images\n# to 0-1 range\ntrain_dataset = datasets.MNIST(root='../data', \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='../data', \n                              train=False, \n                              transform=transforms.ToTensor())\n\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=batch_size, \n                          drop_last=True,\n                          shuffle=True)\n\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=batch_size, \n                         drop_last=True,\n                         shuffle=False)\n\n# Checking the dataset\nfor images, labels in train_loader:  \n    print('Image batch dimensions:', images.shape)\n    print('Image label dimensions:', labels.shape)\n    break\n</code></pre> <pre><code>Training on cpu\nImage batch dimensions: torch.Size([128, 1, 28, 28])\nImage label dimensions: torch.Size([128])\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#2-equipping-cnn-with-coral-layer","title":"2 - Equipping CNN with CORAL layer","text":"<p>In this section, we are using the CoralLayer implemented in <code>coral_pytorch</code> to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer.</p> <p>Using the <code>Sequential</code> API, we specify the CORAl layer as </p> <pre><code>        self.fc = CoralLayer(size_in=294, num_classes=num_classes)\n</code></pre> <p>This is because the convolutional and pooling layers </p> <pre><code>            torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)),\n            torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)))\n</code></pre> <p>produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function</p> <pre><code>        logits =  self.fc(x)\n        probas = torch.sigmoid(logits)\n</code></pre> <p>please use the <code>sigmoid</code> not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper).</p> <pre><code>from coral_pytorch.layers import CoralLayer\n\n\n\nclass ConvNet(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)),\n            torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)))\n\n        ### Specify CORAL layer\n        self.fc = CoralLayer(size_in=294, num_classes=num_classes)\n        ###--------------------------------------------------------------------###\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1) # flatten\n\n        ##### Use CORAL layer #####\n        logits =  self.fc(x)\n        probas = torch.sigmoid(logits)\n        ###--------------------------------------------------------------------###\n\n        return logits, probas\n\n\n\ntorch.manual_seed(random_seed)\nmodel = ConvNet(num_classes=NUM_CLASSES)\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters())\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#3-using-the-coral-loss-for-model-training","title":"3 - Using the CORAL loss for model training","text":"<p>During training, all you need to do is to </p> <p>1) convert the integer class labels into the extended binary label format using the <code>levels_from_labelbatch</code> provided via <code>coral_pytorch</code>:</p> <pre><code>        levels = levels_from_labelbatch(class_labels, \n                                        num_classes=NUM_CLASSES)\n</code></pre> <p>2) Apply the CORAL loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>        loss = coral_loss(logits, levels)\n</code></pre> <pre><code>from coral_pytorch.dataset import levels_from_labelbatch\nfrom coral_pytorch.losses import coral_loss\n\n\nfor epoch in range(num_epochs):\n\n    model = model.train()\n    for batch_idx, (features, class_labels) in enumerate(train_loader):\n\n        ##### Convert class labels for CORAL\n        levels = levels_from_labelbatch(class_labels, \n                                        num_classes=NUM_CLASSES)\n        ###--------------------------------------------------------------------###\n\n        features = features.to(DEVICE)\n        levels = levels.to(DEVICE)\n        logits, probas = model(features)\n\n        #### CORAL loss \n        loss = coral_loss(logits, levels)\n        ###--------------------------------------------------------------------###   \n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 200:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n                   %(epoch+1, num_epochs, batch_idx, \n                     len(train_loader), loss))\n</code></pre> <pre><code>Epoch: 001/010 | Batch 000/468 | Loss: 5.9835\n\n\n/Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nEpoch: 001/010 | Batch 200/468 | Loss: 4.2022\nEpoch: 001/010 | Batch 400/468 | Loss: 3.6785\nEpoch: 002/010 | Batch 000/468 | Loss: 3.5811\nEpoch: 002/010 | Batch 200/468 | Loss: 3.0574\nEpoch: 002/010 | Batch 400/468 | Loss: 3.3966\nEpoch: 003/010 | Batch 000/468 | Loss: 2.9386\nEpoch: 003/010 | Batch 200/468 | Loss: 2.9354\nEpoch: 003/010 | Batch 400/468 | Loss: 3.0238\nEpoch: 004/010 | Batch 000/468 | Loss: 2.7420\nEpoch: 004/010 | Batch 200/468 | Loss: 2.5817\nEpoch: 004/010 | Batch 400/468 | Loss: 2.5847\nEpoch: 005/010 | Batch 000/468 | Loss: 2.6086\nEpoch: 005/010 | Batch 200/468 | Loss: 2.4370\nEpoch: 005/010 | Batch 400/468 | Loss: 2.4903\nEpoch: 006/010 | Batch 000/468 | Loss: 2.3428\nEpoch: 006/010 | Batch 200/468 | Loss: 2.4846\nEpoch: 006/010 | Batch 400/468 | Loss: 2.3392\nEpoch: 007/010 | Batch 000/468 | Loss: 2.4983\nEpoch: 007/010 | Batch 200/468 | Loss: 2.4828\nEpoch: 007/010 | Batch 400/468 | Loss: 2.2048\nEpoch: 008/010 | Batch 000/468 | Loss: 2.3902\nEpoch: 008/010 | Batch 200/468 | Loss: 2.2189\nEpoch: 008/010 | Batch 400/468 | Loss: 2.1895\nEpoch: 009/010 | Batch 000/468 | Loss: 2.2189\nEpoch: 009/010 | Batch 200/468 | Loss: 2.1120\nEpoch: 009/010 | Batch 400/468 | Loss: 2.1923\nEpoch: 010/010 | Batch 000/468 | Loss: 2.1188\nEpoch: 010/010 | Batch 200/468 | Loss: 2.0416\nEpoch: 010/010 | Batch 400/468 | Loss: 1.9729\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#4-evaluate-model","title":"4 -- Evaluate model","text":"<p>Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures.</p> <p>For this, we are going to use the <code>proba_to_label</code> utility function from <code>coral_pytorch</code> to convert the probabilities back to the orginal label.</p> <pre><code>from coral_pytorch.dataset import proba_to_label\n\n\ndef compute_mae_and_mse(model, data_loader, device):\n\n    with torch.no_grad():\n\n        mae, mse, acc, num_examples = 0., 0., 0., 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits, probas = model(features)\n            predicted_labels = proba_to_label(probas).float()\n\n            num_examples += targets.size(0)\n            mae += torch.sum(torch.abs(predicted_labels - targets))\n            mse += torch.sum((predicted_labels - targets)**2)\n\n        mae = mae / num_examples\n        mse = mse / num_examples\n        return mae, mse\n</code></pre> <pre><code>train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\ntest_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n</code></pre> <pre><code>print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}')\nprint(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}')\n</code></pre> <pre><code>Mean absolute error (train/test): 3.45 | 3.34\nMean squared error (train/test): 18.00 | 16.91\n</code></pre> <p>Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.</p>"},{"location":"tutorials/pure_pytorch/CORN_cement/","title":"CORN MLP for predicting cement strength (cement_strength)","text":"<p>This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression. </p>"},{"location":"tutorials/pure_pytorch/CORN_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","title":"0 -- Obtaining and preparing the cement_strength dataset","text":"<p>We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv.</p> <p>First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN.</p> <p>This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column.</p> <pre><code>import pandas as pd\nimport numpy as np\n\n\ndata_df = pd.read_csv(\"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\")\n\ndata_df[\"response\"] = data_df[\"response\"]-1 # labels should start at 0\n\ndata_labels = data_df[\"response\"]\ndata_features = data_df.loc[:, [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\nprint('Number of features:', data_features.shape[1])\nprint('Number of examples:', data_features.shape[0])\nprint('Labels:', np.unique(data_labels.values))\n</code></pre> <pre><code>Number of features: 8\nNumber of examples: 998\nLabels: [0 1 2 3 4]\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#split-into-training-and-test-data","title":"Split into training and test data","text":"<pre><code>from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data_features.values,\n    data_labels.values,\n    test_size=0.2,\n    random_state=1,\n    stratify=data_labels.values)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#standardize-features","title":"Standardize features","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\n\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#1-setting-up-the-dataset-and-dataloader","title":"1 -- Setting up the dataset and dataloader","text":"<p>In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. </p> <pre><code>import torch\n\n\n##########################\n### SETTINGS\n##########################\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.001\nnum_epochs = 20\nbatch_size = 128\n\n# Architecture\nNUM_CLASSES = 5\n\n# Other\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Training on', DEVICE)\n</code></pre> <pre><code>Training on cuda:0\n</code></pre> <pre><code>from torch.utils.data import Dataset\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, feature_array, label_array, dtype=np.float32):\n\n        self.features = feature_array.astype(np.float32)\n        self.labels = label_array\n\n    def __getitem__(self, index):\n        inputs = self.features[index]\n        label = self.labels[index]\n        return inputs, label\n\n    def __len__(self):\n        return self.labels.shape[0]\n</code></pre> <pre><code>import torch\nfrom torch.utils.data import DataLoader\n\n\n# Note transforms.ToTensor() scales input images\n# to 0-1 range\ntrain_dataset = MyDataset(X_train_std, y_train)\ntest_dataset = MyDataset(X_test_std, y_test)\n\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True, # want to shuffle the dataset\n                          num_workers=0) # number processes/CPUs to use\n\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         num_workers=0)\n\n# Checking the dataset\nfor inputs, labels in train_loader:  \n    print('Input batch dimensions:', inputs.shape)\n    print('Input label dimensions:', labels.shape)\n    break\n</code></pre> <pre><code>Input batch dimensions: torch.Size([128, 8])\nInput label dimensions: torch.Size([128])\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#2-equipping-mlp-with-a-corn-layer","title":"2 - Equipping MLP with a CORN layer","text":"<p>In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper).</p> <pre><code>class MLP(torch.nn.Module):\n\n    def __init__(self, in_features, num_classes, num_hidden_1=300, num_hidden_2=300):\n        super().__init__()\n\n        self.my_network = torch.nn.Sequential(\n\n            # 1st hidden layer\n            torch.nn.Linear(in_features, num_hidden_1, bias=False),\n            torch.nn.LeakyReLU(),\n            torch.nn.Dropout(0.2),\n            torch.nn.BatchNorm1d(num_hidden_1),\n\n            # 2nd hidden layer\n            torch.nn.Linear(num_hidden_1, num_hidden_2, bias=False),\n            torch.nn.LeakyReLU(),\n            torch.nn.Dropout(0.2),\n            torch.nn.BatchNorm1d(num_hidden_2),\n\n            ### Specify CORN layer\n            torch.nn.Linear(num_hidden_2, (num_classes-1))\n            ###--------------------------------------------------------------------###\n        )\n\n    def forward(self, x):\n        logits = self.my_network(x)\n        return logits\n\n\n\ntorch.manual_seed(random_seed)\nmodel = MLP(in_features=8, num_classes=NUM_CLASSES)\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#3-using-the-corn-loss-for-model-training","title":"3 - Using the CORN loss for model training","text":"<p>During training, all you need to do is to use the <code>corn_loss</code> provided via <code>coral_pytorch</code>. The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). </p> <pre><code>from coral_pytorch.losses import corn_loss\n\n\nfor epoch in range(num_epochs):\n\n    model = model.train()\n    for batch_idx, (features, class_labels) in enumerate(train_loader):\n\n        class_labels = class_labels.to(DEVICE)\n        features = features.to(DEVICE)\n        logits = model(features)\n\n        #### CORN loss \n        loss = corn_loss(logits, class_labels, NUM_CLASSES)\n        ###--------------------------------------------------------------------###   \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 200:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n                   %(epoch+1, num_epochs, batch_idx, \n                     len(train_loader), loss))\n</code></pre> <pre><code>Epoch: 001/020 | Batch 000/007 | Cost: 0.7095\nEpoch: 002/020 | Batch 000/007 | Cost: 0.5793\nEpoch: 003/020 | Batch 000/007 | Cost: 0.5107\nEpoch: 004/020 | Batch 000/007 | Cost: 0.4893\nEpoch: 005/020 | Batch 000/007 | Cost: 0.4294\nEpoch: 006/020 | Batch 000/007 | Cost: 0.3942\nEpoch: 007/020 | Batch 000/007 | Cost: 0.3905\nEpoch: 008/020 | Batch 000/007 | Cost: 0.3877\nEpoch: 009/020 | Batch 000/007 | Cost: 0.3327\nEpoch: 010/020 | Batch 000/007 | Cost: 0.3442\nEpoch: 011/020 | Batch 000/007 | Cost: 0.3513\nEpoch: 012/020 | Batch 000/007 | Cost: 0.3395\nEpoch: 013/020 | Batch 000/007 | Cost: 0.3272\nEpoch: 014/020 | Batch 000/007 | Cost: 0.3372\nEpoch: 015/020 | Batch 000/007 | Cost: 0.2994\nEpoch: 016/020 | Batch 000/007 | Cost: 0.3409\nEpoch: 017/020 | Batch 000/007 | Cost: 0.3158\nEpoch: 018/020 | Batch 000/007 | Cost: 0.2988\nEpoch: 019/020 | Batch 000/007 | Cost: 0.2793\nEpoch: 020/020 | Batch 000/007 | Cost: 0.2516\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_cement/#4-evaluate-model","title":"4 -- Evaluate model","text":"<p>Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures.</p> <p>For this, we are going to use the <code>corn_label_from_logits</code> utility function from <code>coral_pytorch</code> to convert the probabilities back to the orginal label.</p> <pre><code>from coral_pytorch.dataset import corn_label_from_logits\n\n\ndef compute_mae_and_mse(model, data_loader, device):\n\n    with torch.no_grad():\n\n        mae, mse, acc, num_examples = 0., 0., 0., 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            predicted_labels = corn_label_from_logits(logits).float()\n\n            num_examples += targets.size(0)\n            mae += torch.sum(torch.abs(predicted_labels - targets))\n            mse += torch.sum((predicted_labels - targets)**2)\n\n        mae = mae / num_examples\n        mse = mse / num_examples\n        return mae, mse\n</code></pre> <pre><code>train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\ntest_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n</code></pre> <pre><code>print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}')\nprint(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}')\n</code></pre> <pre><code>Mean absolute error (train/test): 0.29 | 0.36\nMean squared error (train/test): 0.34 | 0.39\n</code></pre> <p>Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.</p>"},{"location":"tutorials/pure_pytorch/CORN_cement/#5-rank-probabilities-from-logits","title":"5 -- Rank probabilities from logits","text":"<p>To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the <code>corn_label_from_logits</code> we used above.</p> <pre><code>logits = model(features)\n\nwith torch.no_grad():\n    probas = torch.sigmoid(logits)\n    probas = torch.cumprod(probas, dim=1)\n    print(probas)\n</code></pre> <pre><code>tensor([[8.4400e-01, 1.1552e-01, 2.4885e-02, 2.1235e-02],\n        [9.6955e-01, 9.6440e-01, 7.9017e-01, 4.0131e-01],\n        [9.6926e-01, 9.6164e-01, 2.8837e-01, 1.1151e-01],\n        [2.7557e-01, 1.7854e-03, 1.3533e-04, 6.4534e-05],\n        [4.4200e-04, 2.9050e-05, 1.8071e-05, 8.5216e-06],\n        [4.1626e-02, 6.8911e-06, 1.1300e-06, 1.1232e-06],\n        [9.5031e-01, 3.2661e-01, 7.6083e-03, 3.6258e-03],\n        [9.8467e-01, 9.0953e-01, 4.3580e-01, 3.9399e-01],\n        [8.0870e-01, 1.9610e-01, 1.9341e-02, 1.6238e-03],\n        [9.6289e-01, 7.2809e-01, 2.1034e-01, 1.4426e-01],\n        [9.8087e-01, 3.4986e-01, 7.5893e-03, 2.1336e-04],\n        [8.3218e-02, 2.9795e-04, 8.8117e-05, 7.7257e-05],\n        [6.4886e-01, 3.3336e-01, 1.7751e-01, 1.1291e-01],\n        [8.0380e-01, 5.5894e-03, 3.1419e-04, 2.4602e-04],\n        [9.3716e-01, 9.3670e-01, 9.3338e-01, 8.3394e-01],\n        [9.0723e-01, 9.0255e-01, 8.7473e-01, 4.9182e-01],\n        [9.8959e-01, 3.3517e-01, 5.4329e-02, 1.7331e-03],\n        [9.6824e-01, 8.0327e-01, 2.5958e-01, 8.4942e-03],\n        [9.6470e-01, 9.1665e-01, 6.9238e-01, 3.8931e-01],\n        [9.6623e-01, 9.6491e-01, 9.4429e-01, 4.3117e-01],\n        [8.0910e-02, 1.5353e-04, 2.7122e-05, 2.1541e-05],\n        [9.9247e-01, 8.6671e-01, 6.3087e-01, 6.6279e-02],\n        [8.8915e-01, 2.5603e-02, 1.8793e-03, 1.5186e-03],\n        [6.2060e-01, 1.8354e-01, 4.0813e-02, 2.1553e-02],\n        [9.5856e-01, 9.5805e-01, 9.2657e-01, 1.6030e-01],\n        [9.9292e-01, 6.5836e-01, 1.8671e-01, 6.0837e-02],\n        [1.0555e-01, 4.6840e-03, 1.1164e-03, 1.7749e-04],\n        [9.6029e-01, 4.0485e-01, 3.0195e-02, 2.0155e-03],\n        [9.8264e-01, 9.1183e-01, 4.3322e-01, 2.3925e-03],\n        [8.9595e-01, 3.6590e-01, 3.0114e-02, 1.9936e-03]], device='cuda:0')\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_mnist/","title":"CORN CNN for predicting handwritten digits (MNIST)","text":"<p>This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</p>"},{"location":"tutorials/pure_pytorch/CORN_mnist/#1-setting-up-the-dataset-and-dataloader","title":"1 -- Setting up the dataset and dataloader","text":"<p>In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN.</p> <pre><code>import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n##########################\n### SETTINGS\n##########################\n\n# Hyperparameters\nrandom_seed = 1\nlearning_rate = 0.05\nnum_epochs = 10\nbatch_size = 128\n\n# Architecture\nNUM_CLASSES = 10 \n\n# Other\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Training on', DEVICE)\n\n##########################\n### MNIST DATASET\n##########################\n\n\n# Note transforms.ToTensor() scales input images\n# to 0-1 range\ntrain_dataset = datasets.MNIST(root='../data', \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='../data', \n                              train=False, \n                              transform=transforms.ToTensor())\n\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=batch_size, \n                          drop_last=True,\n                          shuffle=True)\n\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=batch_size, \n                         drop_last=True,\n                         shuffle=False)\n\n# Checking the dataset\nfor images, labels in train_loader:  \n    print('Image batch dimensions:', images.shape)\n    print('Image label dimensions:', labels.shape)\n    break\n</code></pre> <pre><code>Training on cuda:0\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n\n\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n\n\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n\n\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n\n\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nImage batch dimensions: torch.Size([128, 1, 28, 28])\nImage label dimensions: torch.Size([128])\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_mnist/#2-equipping-cnn-with-a-corn-layer","title":"2 - Equipping CNN with a CORN layer","text":"<p>In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper).</p> <pre><code>class ConvNet(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)),\n            torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1),\n            torch.nn.MaxPool2d((2, 2), (2, 2)))\n\n        ### Specify CORN layer\n        self.output_layer = torch.nn.Linear(in_features=294, out_features=num_classes-1)\n        ###--------------------------------------------------------------------###\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1) # flatten\n\n        ##### Use CORN layer #####\n        logits =  self.output_layer(x)\n        ###--------------------------------------------------------------------###\n\n        return logits\n\n\n\ntorch.manual_seed(random_seed)\nmodel = ConvNet(num_classes=NUM_CLASSES)\nmodel.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters())\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_mnist/#3-using-the-corn-loss-for-model-training","title":"3 - Using the CORN loss for model training","text":"<p>During training, all you need to do is to use the <code>corn_loss</code> provided via <code>coral_pytorch</code>. The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). </p> <pre><code>from coral_pytorch.losses import corn_loss\n\n\nfor epoch in range(num_epochs):\n\n    model = model.train()\n    for batch_idx, (features, class_labels) in enumerate(train_loader):\n\n        class_labels = class_labels.to(DEVICE)\n        features = features.to(DEVICE)\n        logits = model(features)\n\n        #### CORN loss \n        loss = corn_loss(logits, class_labels, NUM_CLASSES)\n        ###--------------------------------------------------------------------###   \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ### LOGGING\n        if not batch_idx % 200:\n            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n                   %(epoch+1, num_epochs, batch_idx, \n                     len(train_loader), loss))\n</code></pre> <pre><code>Epoch: 001/010 | Batch 000/468 | Cost: 0.6896\nEpoch: 001/010 | Batch 200/468 | Cost: 0.1449\nEpoch: 001/010 | Batch 400/468 | Cost: 0.0761\nEpoch: 002/010 | Batch 000/468 | Cost: 0.0927\nEpoch: 002/010 | Batch 200/468 | Cost: 0.0679\nEpoch: 002/010 | Batch 400/468 | Cost: 0.0714\nEpoch: 003/010 | Batch 000/468 | Cost: 0.0593\nEpoch: 003/010 | Batch 200/468 | Cost: 0.0516\nEpoch: 003/010 | Batch 400/468 | Cost: 0.0470\nEpoch: 004/010 | Batch 000/468 | Cost: 0.0301\nEpoch: 004/010 | Batch 200/468 | Cost: 0.0417\nEpoch: 004/010 | Batch 400/468 | Cost: 0.0366\nEpoch: 005/010 | Batch 000/468 | Cost: 0.0449\nEpoch: 005/010 | Batch 200/468 | Cost: 0.0380\nEpoch: 005/010 | Batch 400/468 | Cost: 0.0141\nEpoch: 006/010 | Batch 000/468 | Cost: 0.0272\nEpoch: 006/010 | Batch 200/468 | Cost: 0.0267\nEpoch: 006/010 | Batch 400/468 | Cost: 0.0405\nEpoch: 007/010 | Batch 000/468 | Cost: 0.0649\nEpoch: 007/010 | Batch 200/468 | Cost: 0.0253\nEpoch: 007/010 | Batch 400/468 | Cost: 0.0215\nEpoch: 008/010 | Batch 000/468 | Cost: 0.0389\nEpoch: 008/010 | Batch 200/468 | Cost: 0.0297\nEpoch: 008/010 | Batch 400/468 | Cost: 0.0343\nEpoch: 009/010 | Batch 000/468 | Cost: 0.0249\nEpoch: 009/010 | Batch 200/468 | Cost: 0.0498\nEpoch: 009/010 | Batch 400/468 | Cost: 0.0300\nEpoch: 010/010 | Batch 000/468 | Cost: 0.0201\nEpoch: 010/010 | Batch 200/468 | Cost: 0.0290\nEpoch: 010/010 | Batch 400/468 | Cost: 0.0303\n</code></pre>"},{"location":"tutorials/pure_pytorch/CORN_mnist/#4-evaluate-model","title":"4 -- Evaluate model","text":"<p>Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures.</p> <p>For this, we are going to use the <code>corn_label_from_logits</code> utility function from <code>coral_pytorch</code> to convert the probabilities back to the orginal label.</p> <pre><code>from coral_pytorch.dataset import corn_label_from_logits\n\n\ndef compute_mae_and_mse(model, data_loader, device):\n\n    with torch.no_grad():\n\n        mae, mse, acc, num_examples = 0., 0., 0., 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            predicted_labels = corn_label_from_logits(logits).float()\n\n            num_examples += targets.size(0)\n            mae += torch.sum(torch.abs(predicted_labels - targets))\n            mse += torch.sum((predicted_labels - targets)**2)\n\n        mae = mae / num_examples\n        mse = mse / num_examples\n        return mae, mse\n</code></pre> <pre><code>train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE)\ntest_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE)\n</code></pre> <pre><code>print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}')\nprint(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}')\n</code></pre> <pre><code>Mean absolute error (train/test): 0.15 | 0.15\nMean squared error (train/test): 0.69 | 0.74\n</code></pre> <p>Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.</p>"},{"location":"tutorials/pure_pytorch/CORN_mnist/#5-rank-probabilities-from-logits","title":"5 -- Rank probabilities from logits","text":"<p>To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the <code>corn_label_from_logits</code> we used above.</p> <pre><code>logits = model(features)\n\nwith torch.no_grad():\n    probas = torch.sigmoid(logits)\n    probas = torch.cumprod(probas, dim=1)\n    print(probas)\n</code></pre> <pre><code>tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 9.9986e-01, 9.9941e-01,\n         2.5950e-08],\n        [1.0000e+00, 1.0000e+00, 9.9315e-01,  ..., 9.8477e-01, 9.8476e-01,\n         9.7987e-08],\n        [9.1224e-01, 9.1223e-01, 9.1223e-01,  ..., 8.5374e-01, 8.5216e-01,\n         1.6753e-03],\n        ...,\n        [9.9812e-01, 9.9811e-01, 9.9811e-01,  ..., 9.8991e-01, 9.8968e-01,\n         4.1033e-03],\n        [9.9979e-01, 9.9979e-01, 9.9979e-01,  ..., 1.5020e-02, 1.5015e-02,\n         2.7997e-04],\n        [7.7070e-07, 7.7070e-07, 7.5224e-07,  ..., 7.6964e-08, 7.6941e-08,\n         6.1278e-13]], device='cuda:0')\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/","title":"Finetuning a DistilBERT with CORN Loss for Ordinal Regression","text":"<pre><code># pip install transformers\n</code></pre> <pre><code># pip install datasets\n</code></pre> <pre><code># pip install lightning\n</code></pre> <pre><code>%load_ext watermark\n%watermark -p torch,transformers,datasets,lightning,coral_pytorch\n</code></pre> <pre><code>torch        : 2.0.0+cu118\ntransformers : 4.26.1\ndatasets     : 2.9.0\nlightning    : 2.0.0\ncoral_pytorch: 1.4.0\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/#1-loading-the-dataset","title":"1 Loading the Dataset","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/Raschka-research-group/\"\n    \"corn-ordinal-neuralnet/main/datasets/\"\n    \"tripadvisor/tripadvisor_balanced.csv\")\n\ndf.tail()\n</code></pre> TEXT_COLUMN_NAME LABEL_COLUMN_NAME 6995 beautiful hotel, stay punta cana majestic colo... 5 6996 stay, n't stay, stayed week april, weather ama... 5 6997 stay hotel fantastic, great location, looked n... 5 6998 birthday meal havnt stayed hotel staying barce... 5 6999 great hotel great location stayed royal magda ... 5 <p>Basic datasets analysis and sanity checks</p> <pre><code>print(\"Class distribution:\")\nnp.bincount(df[\"LABEL_COLUMN_NAME\"].values)\n</code></pre> <pre><code>Class distribution:\n\n\n\n\n\narray([   0, 1400, 1400, 1400, 1400, 1400])\n</code></pre> <pre><code>df[\"LABEL_COLUMN_NAME\"] = df[\"LABEL_COLUMN_NAME\"] - 1\nnp.bincount(df[\"LABEL_COLUMN_NAME\"].values)\n</code></pre> <pre><code>array([1400, 1400, 1400, 1400, 1400])\n</code></pre> <p>Performance baseline</p> <pre><code>data_labels = df[\"LABEL_COLUMN_NAME\"]\n\navg_prediction = np.median(data_labels.values)  # median minimizes MAE\nbaseline_mae = np.mean(np.abs(data_labels.values - avg_prediction))\nprint(f'Baseline MAE: {baseline_mae:.2f}')\n</code></pre> <pre><code>Baseline MAE: 1.20\n</code></pre> <p>Split data into training, validation, and test sets</p> <pre><code>df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n\ntrain_idx = int(df_shuffled.shape[0]*0.7)\nval_idx = int(df_shuffled.shape[0]*0.1) \n\ndf_train = df_shuffled.iloc[:train_idx]\ndf_val = df_shuffled.iloc[train_idx:(train_idx+val_idx)]\ndf_test = df_shuffled.iloc[(train_idx+val_idx):]\n\ndf_train.to_csv(\"train.csv\", index=False, encoding=\"utf-8\")\ndf_val.to_csv(\"validation.csv\", index=False, encoding=\"utf-8\")\ndf_test.to_csv(\"test.csv\", index=False, encoding=\"utf-8\")\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/#2-tokenization-and-numericalization","title":"2 Tokenization and Numericalization","text":"<p>Load the dataset via <code>load_dataset</code></p> <pre><code>from datasets import load_dataset\n\nmy_dataset = load_dataset(\n    \"csv\",\n    data_files={\n        \"train\": \"train.csv\",\n        \"validation\": \"validation.csv\",\n        \"test\": \"test.csv\",\n    },\n)\n\nprint(my_dataset)\n</code></pre> <pre><code>Using custom data configuration default-c2106402015b5d25\n\n\nDownloading and preparing dataset csv/default to /home/sebastian/.cache/huggingface/datasets/csv/default-c2106402015b5d25/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n\n\n\nDownloading data files:   0%|          | 0/3 [00:00&lt;?, ?it/s]\n\n\n\nExtracting data files:   0%|          | 0/3 [00:00&lt;?, ?it/s]\n\n\n\nGenerating train split: 0 examples [00:00, ? examples/s]\n\n\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n\n\n\nGenerating validation split: 0 examples [00:00, ? examples/s]\n\n\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n\n\n\nGenerating test split: 0 examples [00:00, ? examples/s]\n\n\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n\n\nDataset csv downloaded and prepared to /home/sebastian/.cache/huggingface/datasets/csv/default-c2106402015b5d25/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n\n\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['index', 'TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME'],\n        num_rows: 4900\n    })\n    validation: Dataset({\n        features: ['index', 'TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME'],\n        num_rows: 700\n    })\n    test: Dataset({\n        features: ['index', 'TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME'],\n        num_rows: 1400\n    })\n})\n</code></pre> <p>Tokenize the dataset</p> <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nprint(\"Tokenizer input max length:\", tokenizer.model_max_length)\nprint(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)\n</code></pre> <pre><code>Tokenizer input max length: 512\nTokenizer vocabulary size: 30522\n</code></pre> <pre><code>def tokenize_text(batch):\n    return tokenizer(batch[\"TEXT_COLUMN_NAME\"], truncation=True, padding=True)\n</code></pre> <pre><code>data_tokenized = my_dataset.map(tokenize_text, batched=True, batch_size=None)\n</code></pre> <pre><code>  0%|          | 0/1 [00:00&lt;?, ?ba/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?ba/s]\n\n\n\n  0%|          | 0/1 [00:00&lt;?, ?ba/s]\n</code></pre> <pre><code>data_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"LABEL_COLUMN_NAME\"])\n</code></pre> <pre><code>import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/#3-set-up-dataloaders","title":"3 Set Up DataLoaders","text":"<pre><code>from torch.utils.data import DataLoader, Dataset\n\n\nclass MyDataset(Dataset):\n    def __init__(self, dataset_dict, partition_key=\"train\"):\n        self.partition = dataset_dict[partition_key]\n\n    def __getitem__(self, index):\n        return self.partition[index]\n\n    def __len__(self):\n        return self.partition.num_rows\n</code></pre> <pre><code>train_dataset = MyDataset(data_tokenized, partition_key=\"train\")\nval_dataset = MyDataset(data_tokenized, partition_key=\"validation\")\ntest_dataset = MyDataset(data_tokenized, partition_key=\"test\")\n\nNUM_WORKERS = 0\n\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=12,\n    shuffle=True, \n    num_workers=NUM_WORKERS\n)\n\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=12,\n    num_workers=NUM_WORKERS\n)\n\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=12,\n    num_workers=NUM_WORKERS\n)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/#4-initializing-distilbert","title":"4 Initializing DistilBERT","text":"<pre><code>from transformers import AutoModelForSequenceClassification\n\n\nNUM_CLASSES = np.bincount(df[\"LABEL_COLUMN_NAME\"].values).shape[0]\nprint(\"Number of classes:\", NUM_CLASSES)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=NUM_CLASSES)\n</code></pre> <pre><code>Number of classes: 5\n\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</code></pre> <pre><code>model\n</code></pre> <pre><code>DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/distilbert-corn-tripadvisor/#5-finetuning","title":"5 Finetuning","text":"<p>Wrap in LightningModule for Training</p> <pre><code>import lightning as L\nimport torch\nimport torchmetrics\n\nfrom coral_pytorch.losses import corn_loss\nfrom coral_pytorch.dataset import corn_label_from_logits\n\n\n\nclass LightningModel(L.LightningModule):\n    def __init__(self, model, num_classes, learning_rate=5e-5):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        self.model = model\n\n        self.num_classes = num_classes\n\n        self.train_mae = torchmetrics.MeanAbsoluteError()\n        self.val_mae = torchmetrics.MeanAbsoluteError()\n        self.test_mae = torchmetrics.MeanAbsoluteError()\n\n    def forward(self, input_ids, attention_mask, labels):\n        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"LABEL_COLUMN_NAME\"]) \n\n        loss = corn_loss(outputs[\"logits\"], batch[\"LABEL_COLUMN_NAME\"],\n                         num_classes=self.num_classes)\n\n        self.log(\"train_loss\", loss)\n\n        predicted_labels = corn_label_from_logits(outputs[\"logits\"])\n        self.test_mae(predicted_labels, batch[\"LABEL_COLUMN_NAME\"])\n        self.log(\"train_mae\", self.train_mae, prog_bar=True)\n\n        return loss  # this is passed to the optimizer for training\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"LABEL_COLUMN_NAME\"])\n\n        loss = corn_loss(outputs[\"logits\"], batch[\"LABEL_COLUMN_NAME\"],\n                         num_classes=self.num_classes)        \n        self.log(\"val_loss\", loss, prog_bar=True)\n\n        predicted_labels = corn_label_from_logits(outputs[\"logits\"])\n        self.val_mae(predicted_labels, batch[\"LABEL_COLUMN_NAME\"])\n        self.log(\"val_mae\", self.val_mae, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"LABEL_COLUMN_NAME\"])        \n\n        predicted_labels = corn_label_from_logits(outputs[\"logits\"])\n        self.test_mae(predicted_labels, batch[\"LABEL_COLUMN_NAME\"])\n        self.log(\"test_mae\", self.test_mae, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre> <pre><code>lightning_model = LightningModel(model, num_classes=NUM_CLASSES)\n</code></pre> <pre><code>from lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import CSVLogger\n\n\ncallbacks = [\n    ModelCheckpoint(\n        save_top_k=1, mode=\"min\", monitor=\"val_mae\"\n    )  # save top 1 model\n]\nlogger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n</code></pre> <pre><code>trainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=10,\n)\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n</code></pre> <pre><code>GPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory logs/my-model/version_0/checkpoints exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n\n  | Name      | Type                                | Params\n------------------------------------------------------------------\n0 | model     | DistilBertForSequenceClassification | 67.0 M\n1 | train_mae | MeanAbsoluteError                   | 0     \n2 | val_mae   | MeanAbsoluteError                   | 0     \n3 | test_mae  | MeanAbsoluteError                   | 0     \n------------------------------------------------------------------\n67.0 M    Trainable params\n0         Non-trainable params\n67.0 M    Total params\n267.829   Total estimated model params size (MB)\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/my-model/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n  rank_zero_warn(\n\n\n\nSanity Checking: 0it [00:00, ?it/s]\n\n\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 255 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 255 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\nTraining: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n`Trainer.fit` stopped: `max_epochs=3` reached.\n</code></pre> <pre><code>trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\n</code></pre> <pre><code>You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nRestoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\nLoaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n  rank_zero_warn(\n/home/sebastian/miniforge3/envs/lightning2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 255 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\nTesting: 0it [00:00, ?it/s]\n</code></pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_mae          \u2502    0.3761734664440155     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre><code>[{'test_mae': 0.3761734664440155}]\n</code></pre> <pre><code>trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\n</code></pre> <pre><code>You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nRestoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\nLoaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\n\n\n\nTesting: 0it [00:00, ?it/s]\n</code></pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_mae          \u2502    0.38999998569488525    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre><code>[{'test_mae': 0.38999998569488525}]\n</code></pre> <pre><code>trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")\n</code></pre> <pre><code>You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nRestoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\nLoaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=818-v1.ckpt\n\n\n\nTesting: 0it [00:00, ?it/s]\n</code></pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_mae          \u2502    0.4214285612106323     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre><code>[{'test_mae': 0.4214285612106323}]\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/","title":"A Multilayer Perceptron for Ordinal Regression using CORAL -- Cement Dataset","text":"<p>In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper:</p> <ul> <li>Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020):  Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation.  Pattern Recognition Letters. 140, 325-331</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#general-settings-and-hyperparameters","title":"General settings and hyperparameters","text":"<ul> <li>Here, we specify some general hyperparameter values and general settings</li> <li>Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch</li> </ul> <pre><code>BATCH_SIZE = 32\nNUM_EPOCHS = 200\nLEARNING_RATE = 0.01\nNUM_WORKERS = 0\n\nDATA_BASEPATH = \"./data\"\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","title":"Converting a regular classifier into a CORAL ordinal regression model","text":"<p>Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes:</p> <p>1) We replace the output layer  </p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n</code></pre> <p>by a CORAL layer (available through <code>coral_pytorch</code>):</p> <pre><code>output_layer = CoralLayer(size_in=hidden_units[-1], num_classes=num_classes)`\n</code></pre> <p>2)</p> <p>Convert the integer class labels into the extended binary label format using the <code>levels_from_labelbatch</code> provided via <code>coral_pytorch</code>:</p> <pre><code>levels = levels_from_labelbatch(class_labels, \n                                num_classes=num_classes)\n</code></pre> <p>3) </p> <p>Swap the cross entropy loss from PyTorch,</p> <pre><code>torch.nn.functional.cross_entropy(logits, true_labels)\n</code></pre> <p>with the CORAL loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>loss = coral_loss(logits, levels)\n</code></pre> <p>4)</p> <p>In a regular classifier, we usually obtain the predicted class labels as follows:</p> <pre><code>predicted_labels = torch.argmax(logits, dim=1)\n</code></pre> <p>Replace this with the following code to convert the predicted probabilities into the predicted labels:</p> <pre><code>predicted_labels = proba_to_label(probas)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","title":"Implementing a <code>MultiLayerPerceptron</code> using PyTorch Lightning's <code>LightningModule</code>","text":"<ul> <li>In this section, we set up the main model architecture using the <code>LightningModule</code> from PyTorch Lightning.</li> <li>We start with defining our <code>MultiLayerPerceptron</code> model in pure PyTorch, and then we use it in the <code>LightningModule</code> to get all the extra benefits that PyTorch Lightning provides.</li> <li>Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the <code>CoralLayer</code>\".</li> </ul> <pre><code>import torch\nfrom coral_pytorch.layers import CoralLayer\n\n\n# Regular PyTorch Module\nclass MultiLayerPerceptron(torch.nn.Module):\n    def __init__(self, input_size, hidden_units, num_classes):\n        super().__init__()\n\n        # num_classes is used by the CORAL loss function\n        self.num_classes = num_classes\n\n        # Initialize MLP layers\n        all_layers = []\n        for hidden_unit in hidden_units:\n            layer = torch.nn.Linear(input_size, hidden_unit)\n            all_layers.append(layer)\n            all_layers.append(torch.nn.ReLU())\n            input_size = hidden_unit\n\n        # CORAL: output layer -------------------------------------------\n        # Regular classifier would use the following output layer:\n        # output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n\n        # We replace it by the CORAL layer:\n        output_layer = CoralLayer(size_in=hidden_units[-1],\n                                  num_classes=num_classes)\n        # ----------------------------------------------------------------\n\n        all_layers.append(output_layer)\n        self.model = torch.nn.Sequential(*all_layers)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n</code></pre> <ul> <li>In our <code>LightningModule</code> we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later.</li> <li>Note that we make changes 2) (<code>levels_from_labelbatch</code>), 3) (<code>coral_loss</code>), and 4) (<code>proba_to_label</code>) to implement a CORAL model instead of a regular classifier:</li> </ul> <pre><code>from coral_pytorch.losses import coral_loss\nfrom coral_pytorch.dataset import levels_from_labelbatch\nfrom coral_pytorch.dataset import proba_to_label\n\nimport pytorch_lightning as pl\nimport torchmetrics\n\n\n# LightningModule that receives a PyTorch model as input\nclass LightningMLP(pl.LightningModule):\n    def __init__(self, model, learning_rate):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        # The inherited PyTorch module\n        self.model = model\n\n        # Save settings and hyperparameters to the log directory\n        # but skip the model parameters\n        self.save_hyperparameters(ignore=['model'])\n\n        # Set up attributes for computing the MAE\n        self.train_mae = torchmetrics.MeanAbsoluteError()\n        self.valid_mae = torchmetrics.MeanAbsoluteError()\n        self.test_mae = torchmetrics.MeanAbsoluteError()\n\n    # Defining the forward method is only necessary \n    # if you want to use a Trainer's .predict() method (optional)\n    def forward(self, x):\n        return self.model(x)\n\n    # A common forward step to compute the loss and labels\n    # this is used for training, validation, and testing below\n    def _shared_step(self, batch):\n        features, true_labels = batch\n\n        # Convert class labels for CORAL ------------------------\n        levels = levels_from_labelbatch(\n            true_labels, num_classes=self.model.num_classes)\n        # -------------------------------------------------------\n\n        logits = self(features)\n\n        # CORAL Loss --------------------------------------------\n        # A regular classifier uses:\n        # loss = torch.nn.functional.cross_entropy(logits, true_labels)\n        loss = coral_loss(logits, levels.type_as(logits))\n        # -------------------------------------------------------\n\n        # CORAL Prediction to label -----------------------------\n        # A regular classifier uses:\n        # predicted_labels = torch.argmax(logits, dim=1)\n        probas = torch.sigmoid(logits)\n        predicted_labels = proba_to_label(probas)\n        # -------------------------------------------------------\n        return loss, true_labels, predicted_labels\n\n    def training_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"train_loss\", loss)\n        self.train_mae(predicted_labels, true_labels)\n        self.log(\"train_mae\", self.train_mae, on_epoch=True, on_step=False)\n        return loss  # this is passed to the optimzer for training\n\n    def validation_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"valid_loss\", loss)\n        self.valid_mae(predicted_labels, true_labels)\n        self.log(\"valid_mae\", self.valid_mae,\n                 on_epoch=True, on_step=False, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        _, true_labels, predicted_labels = self._shared_step(batch)\n        self.test_mae(predicted_labels, true_labels)\n        self.log(\"test_mae\", self.test_mae, on_epoch=True, on_step=False)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-the-dataset","title":"Setting up the dataset","text":"<ul> <li>In this section, we are going to set up our dataset.</li> <li>We start by downloading and taking a look at the Cement dataset:</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#inspecting-the-dataset","title":"Inspecting the dataset","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n\ndata_df = pd.read_csv(\"https://raw.githubusercontent.com/gagolews/\"\n                      \"ordinal_regression_data/master/cement_strength.csv\")\ndata_df[\"response\"] = data_df[\"response\"]-1  # labels should start at 0\n\ndata_labels = data_df[\"response\"]\ndata_features = data_df.loc[:, [\n    \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\ndata_df.head()\n</code></pre> response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 <pre><code>print('Number of features:', data_features.shape[1])\nprint('Number of examples:', data_features.shape[0])\nprint('Labels:', np.unique(data_labels.values))\nprint('Label distribution:', np.bincount(data_labels))\n</code></pre> <pre><code>Number of features: 8\nNumber of examples: 998\nLabels: [0 1 2 3 4]\nLabel distribution: [196 310 244 152  96]\n</code></pre> <ul> <li>Above, we can see that the dataset consists of 8 features, and there are 998 examples in total.</li> <li>The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). </li> <li>Notice also that the dataset is quite imbalanced.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#performance-baseline","title":"Performance baseline","text":"<ul> <li>Especially for imbalanced datasets, it's quite useful to compute a performance baseline.</li> <li>In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that!</li> <li>Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE).</li> <li>So, if we use the mean absolute error, , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median:</li> </ul> <pre><code>avg_prediction = np.median(data_labels.values)  # median minimizes MAE\nbaseline_mae = np.mean(np.abs(data_labels.values - avg_prediction))\nprint(f'Baseline MAE: {baseline_mae:.2f}')\n</code></pre> <pre><code>Baseline MAE: 1.03\n</code></pre> <ul> <li>In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of &gt; 1 is certainly a bad model.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#creating-a-dataset-class","title":"Creating a <code>Dataset</code> class","text":"<ul> <li>Next, let us set up a data loading mechanism for our model.</li> <li>Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets.</li> <li>First, we define a PyTorch <code>Dataset</code> class that returns the features (inputs) and labels:</li> </ul> <pre><code>from torch.utils.data import Dataset\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, feature_array, label_array, dtype=np.float32):\n        self.features = feature_array.astype(dtype)\n        self.labels = label_array\n\n    def __getitem__(self, index):\n        inputs = self.features[index]\n        label = self.labels[index]\n        return inputs, label\n\n    def __len__(self):\n        return self.features.shape[0]\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-a-datamodule","title":"Setting up a <code>DataModule</code>","text":"<ul> <li>There are three main ways we can prepare the dataset for Lightning. We can</li> <li>make the dataset part of the model;</li> <li>set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection;</li> <li>create a LightningDataModule.</li> <li>Here, we are going to use approach 3, which is the most organized approach. The <code>LightningDataModule</code> consists of several self-explanatory methods as we can see below:</li> </ul> <pre><code>import os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, data_path='./'):\n        super().__init__()\n        self.data_path = data_path\n\n    def prepare_data(self):\n        data_df = pd.read_csv(\n            'https://raw.githubusercontent.com/gagolews/'\n            'ordinal_regression_data/master/cement_strength.csv')\n        data_df.to_csv(\n            os.path.join(self.data_path, 'cement_strength.csv'), index=None)\n        return\n\n    def setup(self, stage=None):\n        data_df = pd.read_csv(\n            os.path.join(self.data_path, 'cement_strength.csv'))\n        data_df[\"response\"] = data_df[\"response\"]-1  # labels should start at 0\n        self.data_labels = data_df[\"response\"]\n        self.data_features = data_df.loc[:, [\n            \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\n        # Split into\n        # 70% train, 10% validation, 20% testing\n\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            self.data_features.values,\n            self.data_labels.values,\n            test_size=0.2,\n            random_state=1,\n            stratify=self.data_labels.values)\n\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_temp,\n            y_temp,\n            test_size=0.1,\n            random_state=1,\n            stratify=y_temp)\n\n        # Standardize features\n        sc = StandardScaler()\n        X_train_std = sc.fit_transform(X_train)\n        X_valid_std = sc.transform(X_valid)\n        X_test_std = sc.transform(X_test)\n\n        self.train = MyDataset(X_train_std, y_train)\n        self.valid = MyDataset(X_valid_std, y_valid)\n        self.test = MyDataset(X_test_std, y_test)\n\n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS,\n                          drop_last=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.valid, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS)\n\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS)\n</code></pre> <ul> <li>Note that the <code>prepare_data</code> method is usually used for steps that only need to be executed once, for example, downloading the dataset; the <code>setup</code> method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. </li> <li>Next, lets initialize the <code>DataModule</code>; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):</li> </ul> <pre><code>torch.manual_seed(1) \ndata_module = DataModule(data_path=DATA_BASEPATH)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","title":"Training the model using the PyTorch Lightning Trainer class","text":"<ul> <li>Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer).</li> <li>We wrap the model in our <code>LightningMLP</code> so that we can use PyTorch Lightning's powerful <code>Trainer</code> API.</li> <li>Also, we define a callback so that we can obtain the model with the best validation set performance after training.</li> <li>Note PyTorch Lightning offers many advanced logging services like Weights &amp; Biases. However, here, we will keep things simple and use the <code>CSVLogger</code>:</li> </ul> <pre><code>from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger\n\n\npytorch_model = MultiLayerPerceptron(\n    input_size=data_features.shape[1],\n    hidden_units=(24, 16),\n    num_classes=np.bincount(data_labels).shape[0])\n\nlightning_model = LightningMLP(\n    model=pytorch_model,\n    learning_rate=LEARNING_RATE)\n\n\ncallbacks = [ModelCheckpoint(\n    save_top_k=1, mode=\"min\", monitor=\"valid_mae\")]  # save top 1 model \nlogger = CSVLogger(save_dir=\"logs/\", name=\"mlp-coral-cement\")\n</code></pre> <ul> <li>Now it's time to train our model:</li> </ul> <pre><code>import time\n\n\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS,\n    callbacks=callbacks,\n    progress_bar_refresh_rate=50,  # recommended for notebooks\n    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n    logger=logger,\n    deterministic=True,\n    log_every_n_steps=10)\n\nstart_time = time.time()\ntrainer.fit(model=lightning_model, datamodule=data_module)\n\nruntime = (time.time() - start_time)/60\nprint(f\"Training took {runtime:.2f} min in total.\")\n</code></pre> <pre><code>GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type                 | Params\n---------------------------------------------------\n0 | model     | MultiLayerPerceptron | 636   \n1 | train_mae | MeanAbsoluteError    | 0     \n2 | valid_mae | MeanAbsoluteError    | 0     \n3 | test_mae  | MeanAbsoluteError    | 0     \n---------------------------------------------------\n636       Trainable params\n0         Non-trainable params\n636       Total params\n0.003     Total estimated model params size (MB)\n\n\n\nValidation sanity check: 0it [00:00, ?it/s]\n\n\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\nTraining: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\nTraining took 0.94 min in total.\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#evaluating-the-model","title":"Evaluating the model","text":"<ul> <li>After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you):</li> </ul> <pre><code>metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='Loss')\ndf_metrics[[\"train_mae\", \"valid_mae\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='MAE')\n</code></pre> <pre><code>&lt;AxesSubplot:xlabel='Epoch', ylabel='MAE'&gt;\n</code></pre> <ul> <li>As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 110.</li> <li>The <code>trainer</code> saved this model automatically for us, we which we can load from the checkpoint via the <code>ckpt_path='best'</code> argument; below we use the <code>trainer</code> instance to evaluate the best model on the test set:</li> </ul> <pre><code>trainer.test(model=lightning_model, datamodule=data_module, ckpt_path='best')\n</code></pre> <pre><code>Restoring states from the checkpoint path at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoaded model weights from checkpoint at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\nTesting: 0it [00:00, ?it/s]\n\n\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_mae': 0.25}\n--------------------------------------------------------------------------------\n\n\n\n\n\n[{'test_mae': 0.25}]\n</code></pre> <ul> <li>The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#predicting-labels-of-new-data","title":"Predicting labels of new data","text":"<ul> <li>You can use the <code>trainer.predict</code> method on a new <code>DataLoader</code> or <code>DataModule</code> to apply the model to new data.</li> <li>Alternatively, you can also manually load the best model from a checkpoint as shown below:</li> </ul> <pre><code>path = trainer.checkpoint_callback.best_model_path\nprint(path)\n</code></pre> <pre><code>logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt\n</code></pre> <pre><code>lightning_model = LightningMLP.load_from_checkpoint(\n    path, model=pytorch_model)\nlightning_model.eval();\n</code></pre> <ul> <li>Note that our <code>MultilayerPerceptron</code>, which is passed to <code>LightningMLP</code> requires input arguments. However, this is automatically being taken care of since we used <code>self.save_hyperparameters()</code> in <code>LightningMLP</code>'s <code>__init__</code> method.</li> <li>Now, below is an example applying the model manually. Here, pretend that the <code>test_dataloader</code> is a new data loader.</li> </ul> <pre><code>test_dataloader = data_module.test_dataloader()\n\nall_predicted_labels = []\nfor batch in test_dataloader:\n    features, _ = batch\n    logits = lightning_model(features)\n    probas = torch.sigmoid(logits)\n    predicted_labels = proba_to_label(probas)\n    all_predicted_labels.append(predicted_labels)\n\nall_predicted_labels = torch.cat(all_predicted_labels)\nall_predicted_labels[:5]\n</code></pre> <pre><code>tensor([0, 4, 0, 3, 1])\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/","title":"A Convolutional Neural Net for Ordinal Regression using CORAL -- MNIST Dataset","text":"<p>In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper:</p> <ul> <li>Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020):  Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation.  Pattern Recognition Letters. 140, 325-331</li> </ul> <p>Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</p>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#general-settings-and-hyperparameters","title":"General settings and hyperparameters","text":"<ul> <li>Here, we specify some general hyperparameter values and general settings</li> <li>Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting <code>NUM_WORKERS = 0</code> instead.</li> </ul> <pre><code>BATCH_SIZE = 256\nNUM_EPOCHS = 20\nLEARNING_RATE = 0.005\nNUM_WORKERS = 4\n\nDATA_BASEPATH = \"./data\"\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","title":"Converting a regular classifier into a CORAL ordinal regression model","text":"<p>Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes:</p> <p>1) We replace the output layer  </p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n</code></pre> <p>by a CORAL layer (available through <code>coral_pytorch</code>):</p> <pre><code>output_layer = CoralLayer(size_in=hidden_units[-1], num_classes=num_classes)`\n</code></pre> <p>2)</p> <p>Convert the integer class labels into the extended binary label format using the <code>levels_from_labelbatch</code> provided via <code>coral_pytorch</code>:</p> <pre><code>levels = levels_from_labelbatch(class_labels, \n                                num_classes=num_classes)\n</code></pre> <p>3) </p> <p>Swap the cross entropy loss from PyTorch,</p> <pre><code>torch.nn.functional.cross_entropy(logits, true_labels)\n</code></pre> <p>with the CORAL loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>loss = coral_loss(logits, levels)\n</code></pre> <p>4)</p> <p>In a regular classifier, we usually obtain the predicted class labels as follows:</p> <pre><code>predicted_labels = torch.argmax(logits, dim=1)\n</code></pre> <p>Replace this with the following code to convert the predicted probabilities into the predicted labels:</p> <pre><code>predicted_labels = proba_to_label(probas)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","title":"Implementing a <code>ConvNet</code> using PyTorch Lightning's <code>LightningModule</code>","text":"<ul> <li>In this section, we set up the main model architecture using the <code>LightningModule</code> from PyTorch Lightning.</li> <li>We start with defining our convolutional neural network <code>ConvNet</code> model in pure PyTorch, and then we use it in the <code>LightningModule</code> to get all the extra benefits that PyTorch Lightning provides.</li> <li>Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the <code>CoralLayer</code>\".</li> </ul> <pre><code>import torch\nfrom coral_pytorch.layers import CoralLayer\n\n\n# Regular PyTorch Module\nclass ConvNet(torch.nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n\n        # num_classes is used by the corn loss function\n        self.num_classes = num_classes\n\n        # Initialize CNN layers\n        all_layers = [\n            torch.nn.Conv2d(in_channels=in_channels, out_channels=3, \n                            kernel_size=(3, 3), stride=(1, 1), \n                            padding=1),\n            torch.nn.MaxPool2d(kernel_size=(2, 2),  stride=(2, 2)),\n            torch.nn.Conv2d(in_channels=3, out_channels=6, \n                            kernel_size=(3, 3), stride=(1, 1), \n                            padding=1),\n            torch.nn.MaxPool2d(kernel_size=(2, 2),  stride=(2, 2)),\n            torch.nn.Flatten()\n        ]\n\n        # CORAL: output layer -------------------------------------------\n        # Regular classifier would use the following output layer:\n        # output_layer = torch.nn.Linear(294, num_classes)\n\n        # We replace it by the CORAL layer:\n        output_layer = CoralLayer(size_in=294,\n                                  num_classes=num_classes)\n        # ----------------------------------------------------------------\n\n        all_layers.append(output_layer)\n        self.model = torch.nn.Sequential(*all_layers)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n</code></pre> <ul> <li>In our <code>LightningModule</code> we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later.</li> <li>Note that we make changes 2) (<code>levels_from_labelbatch</code>), 3) (<code>coral_loss</code>), and 4) (<code>proba_to_label</code>) to implement a CORAL model instead of a regular classifier:</li> </ul> <pre><code>from coral_pytorch.losses import coral_loss\nfrom coral_pytorch.dataset import levels_from_labelbatch\nfrom coral_pytorch.dataset import proba_to_label\n\nimport pytorch_lightning as pl\nimport torchmetrics\n\n\n# LightningModule that receives a PyTorch model as input\nclass LightningCNN(pl.LightningModule):\n    def __init__(self, model, learning_rate):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        # The inherited PyTorch module\n        self.model = model\n\n        # Save settings and hyperparameters to the log directory\n        # but skip the model parameters\n        self.save_hyperparameters(ignore=['model'])\n\n        # Set up attributes for computing the MAE\n        self.train_mae = torchmetrics.MeanAbsoluteError()\n        self.valid_mae = torchmetrics.MeanAbsoluteError()\n        self.test_mae = torchmetrics.MeanAbsoluteError()\n\n    # Defining the forward method is only necessary \n    # if you want to use a Trainer's .predict() method (optional)\n    def forward(self, x):\n        return self.model(x)\n\n    # A common forward step to compute the loss and labels\n    # this is used for training, validation, and testing below\n    def _shared_step(self, batch):\n        features, true_labels = batch\n        logits = self(features)\n\n        # Convert class labels for CORAL ------------------------\n        levels = levels_from_labelbatch(\n            true_labels, num_classes=self.model.num_classes).type_as(logits)\n        # -------------------------------------------------------\n\n        logits = self(features)\n\n        # CORAL Loss --------------------------------------------\n        # A regular classifier uses:\n        # loss = torch.nn.functional.cross_entropy(logits, true_labels)\n        loss = coral_loss(logits, levels)\n        # -------------------------------------------------------\n\n        # CORAL Prediction to label -----------------------------\n        # A regular classifier uses:\n        # predicted_labels = torch.argmax(logits, dim=1)\n        probas = torch.sigmoid(logits)\n        predicted_labels = proba_to_label(probas)\n        # -------------------------------------------------------\n        return loss, true_labels, predicted_labels\n\n    def training_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"train_loss\", loss)\n        self.train_mae(predicted_labels, true_labels)\n        self.log(\"train_mae\", self.train_mae, on_epoch=True, on_step=False)\n        return loss  # this is passed to the optimzer for training\n\n    def validation_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"valid_loss\", loss)\n        self.valid_mae(predicted_labels, true_labels)\n        self.log(\"valid_mae\", self.valid_mae,\n                 on_epoch=True, on_step=False, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.test_mae(predicted_labels, true_labels)\n        self.log(\"test_mae\", self.test_mae, on_epoch=True, on_step=False)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nModuleNotFoundError                       Traceback (most recent call last)\n\nInput In [4], in &lt;cell line: 5&gt;()\n      2 from coral_pytorch.dataset import levels_from_labelbatch\n      3 from coral_pytorch.dataset import proba_to_label\n----&gt; 5 import pytorch_lightning as pl\n      6 import torchmetrics\n      9 # LightningModule that receives a PyTorch model as input\n\n\nFile ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in &lt;module&gt;\n     17 _PACKAGE_ROOT = os.path.dirname(__file__)\n     18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT)\n---&gt; 20 from pytorch_lightning.callbacks import Callback  # noqa: E402\n     21 from pytorch_lightning.core import LightningDataModule, LightningModule  # noqa: E402\n     22 from pytorch_lightning.trainer import Trainer  # noqa: E402\n\n\nFile ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in &lt;module&gt;\n      1 # Copyright The PyTorch Lightning team.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n---&gt; 14 from pytorch_lightning.callbacks.base import Callback\n     15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor\n     16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n\nFile ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in &lt;module&gt;\n     23 from torch.optim import Optimizer\n     25 import pytorch_lightning as pl\n---&gt; 26 from pytorch_lightning.utilities.types import STEP_OUTPUT\n     29 class Callback(abc.ABC):\n     30     r\"\"\"\n     31     Abstract base class used to build new callbacks.\n     32 \n     33     Subclass this class and override any of the relevant hooks\n     34     \"\"\"\n\n\nFile ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in &lt;module&gt;\n     14 \"\"\"General utilities.\"\"\"\n     16 import numpy\n---&gt; 18 from pytorch_lightning.utilities.apply_func import move_data_to_device  # noqa: F401\n     19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only  # noqa: F401\n     20 from pytorch_lightning.utilities.enums import (  # noqa: F401\n     21     AMPType,\n     22     DeviceType,\n   (...)\n     26     ModelSummaryMode,\n     27 )\n\n\nFile ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in &lt;module&gt;\n     28 if _TORCHTEXT_AVAILABLE:\n     29     if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"):\n---&gt; 30         from torchtext.legacy.data import Batch\n     31     else:\n     32         from torchtext.data import Batch\n\n\nModuleNotFoundError: No module named 'torchtext.legacy'\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-the-dataset","title":"Setting up the dataset","text":"<ul> <li>In this section, we are going to set up our dataset.</li> <li>Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#inspecting-the-dataset","title":"Inspecting the dataset","text":"<pre><code>import torch\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n\ntrain_dataset = datasets.MNIST(root=DATA_BASEPATH, \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=BATCH_SIZE, \n                          num_workers=NUM_WORKERS,\n                          drop_last=True,\n                          shuffle=True)\n\ntest_dataset = datasets.MNIST(root=DATA_BASEPATH, \n                              train=False,\n                              transform=transforms.ToTensor())\n\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=BATCH_SIZE,\n                         num_workers=NUM_WORKERS,\n                         drop_last=False,\n                         shuffle=False)\n\n# Checking the dataset\nall_train_labels = []\nall_test_labels = []\n\nfor images, labels in train_loader:  \n    all_train_labels.append(labels)\nall_train_labels = torch.cat(all_train_labels)\n\nfor images, labels in test_loader:  \n    all_test_labels.append(labels)\nall_test_labels = torch.cat(all_test_labels)\n</code></pre> <pre><code>print('Training labels:', torch.unique(all_train_labels))\nprint('Training label distribution:', torch.bincount(all_train_labels))\n\nprint('\\nTest labels:', torch.unique(all_test_labels))\nprint('Test label distribution:', torch.bincount(all_test_labels))\n</code></pre> <ul> <li>Above, we can see that the dataset consists of 8 features, and there are 998 examples in total.</li> <li>The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). </li> <li>Notice also that the dataset is quite imbalanced.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#performance-baseline","title":"Performance baseline","text":"<ul> <li>Especially for imbalanced datasets, it's quite useful to compute a performance baseline.</li> <li>In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that!</li> <li>Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE).</li> <li>So, if we use the mean absolute error, , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median:</li> </ul> <pre><code>all_test_labels = all_test_labels.float()\navg_prediction = torch.median(all_test_labels)  # median minimizes MAE\nbaseline_mae = torch.mean(torch.abs(all_test_labels - avg_prediction))\nprint(f'Baseline MAE: {baseline_mae:.2f}')\n</code></pre> <ul> <li>In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of &gt; 2.52 is certainly a bad model.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-a-datamodule","title":"Setting up a <code>DataModule</code>","text":"<ul> <li>There are three main ways we can prepare the dataset for Lightning. We can</li> <li>make the dataset part of the model;</li> <li>set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection;</li> <li>create a LightningDataModule.</li> <li>Here, we are going to use approach 3, which is the most organized approach. The <code>LightningDataModule</code> consists of several self-explanatory methods as we can see below:</li> </ul> <pre><code>import os\n\nfrom torch.utils.data.dataset import random_split\nfrom torch.utils.data import DataLoader\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, data_path='./'):\n        super().__init__()\n        self.data_path = data_path\n\n    def prepare_data(self):\n        datasets.MNIST(root=self.data_path,\n                       download=True)\n        return\n\n    def setup(self, stage=None):\n        # Note transforms.ToTensor() scales input images\n        # to 0-1 range\n        train = datasets.MNIST(root=self.data_path, \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=False)\n\n        self.test = datasets.MNIST(root=self.data_path, \n                                   train=False, \n                                   transform=transforms.ToTensor(),\n                                   download=False)\n\n        self.train, self.valid = random_split(train, lengths=[55000, 5000])\n\n    def train_dataloader(self):\n        train_loader = DataLoader(dataset=self.train, \n                                  batch_size=BATCH_SIZE, \n                                  drop_last=True,\n                                  shuffle=True,\n                                  num_workers=NUM_WORKERS)\n        return train_loader\n\n    def val_dataloader(self):\n        valid_loader = DataLoader(dataset=self.valid, \n                                  batch_size=BATCH_SIZE, \n                                  drop_last=False,\n                                  shuffle=False,\n                                  num_workers=NUM_WORKERS)\n        return valid_loader\n\n    def test_dataloader(self):\n        test_loader = DataLoader(dataset=self.test, \n                                 batch_size=BATCH_SIZE, \n                                 drop_last=False,\n                                 shuffle=False,\n                                 num_workers=NUM_WORKERS)\n        return test_loader\n</code></pre> <ul> <li>Note that the <code>prepare_data</code> method is usually used for steps that only need to be executed once, for example, downloading the dataset; the <code>setup</code> method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. </li> <li>Next, lets initialize the <code>DataModule</code>; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):</li> </ul> <pre><code>torch.manual_seed(1) \ndata_module = DataModule(data_path=DATA_BASEPATH)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","title":"Training the model using the PyTorch Lightning Trainer class","text":"<ul> <li>Next, we initialize our CNN (<code>ConvNet</code>) model.</li> <li>Also, we define a call back so that we can obtain the model with the best validation set performance after training.</li> <li>PyTorch Lightning offers many advanced logging services like Weights &amp; Biases. Here, we will keep things simple and use the <code>CSVLogger</code>:</li> </ul> <pre><code>from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger\n\n\npytorch_model = ConvNet(\n    in_channels=1,\n    num_classes=torch.unique(all_test_labels).shape[0])\n\nlightning_model = LightningCNN(\n    model=pytorch_model,\n    learning_rate=LEARNING_RATE)\n\ncallbacks = [ModelCheckpoint(\n    save_top_k=1, mode='min', monitor=\"valid_mae\")]  # save top 1 model \nlogger = CSVLogger(save_dir=\"logs/\", name=\"cnn-coral-mnist\")\n</code></pre> <ul> <li>Now it's time to train our model:</li> </ul> <pre><code>import time\n\n\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS,\n    callbacks=callbacks,\n    progress_bar_refresh_rate=50,  # recommended for notebooks\n    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n    logger=logger,\n    deterministic=True,\n    log_every_n_steps=10)\n\nstart_time = time.time()\ntrainer.fit(model=lightning_model, datamodule=data_module)\n\nruntime = (time.time() - start_time)/60\nprint(f\"Training took {runtime:.2f} min in total.\")\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#evaluating-the-model","title":"Evaluating the model","text":"<ul> <li>After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you):</li> </ul> <pre><code>import pandas as pd\n\n\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='Loss')\ndf_metrics[[\"train_mae\", \"valid_mae\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='MAE')\n</code></pre> <ul> <li>It's hard to tell what the best model (based on the lowest validation set MAE) is in this case, but no worries, the <code>trainer</code> saved this model automatically for us, we which we can load from the checkpoint via the <code>ckpt_path='best'</code> argument; below we use the <code>trainer</code> instance to evaluate the best model on the test set:</li> </ul> <pre><code>trainer.test(model=lightning_model, datamodule=data_module, ckpt_path='best')\n</code></pre> <ul> <li>The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#predicting-labels-of-new-data","title":"Predicting labels of new data","text":"<ul> <li>You can use the <code>trainer.predict</code> method on a new <code>DataLoader</code> or <code>DataModule</code> to apply the model to new data.</li> <li>Alternatively, you can also manually load the best model from a checkpoint as shown below:</li> </ul> <pre><code>path = trainer.checkpoint_callback.best_model_path\nprint(path)\n</code></pre> <pre><code>lightning_model = LightningCNN.load_from_checkpoint(\n    path, model=pytorch_model)\nlightning_model.eval();\n</code></pre> <ul> <li>Note that our <code>ConvNet</code>, which is passed to <code>LightningCNN</code> requires input arguments. However, this is automatically being taken care of since we used <code>self.save_hyperparameters()</code> in <code>LightningCNN</code>'s <code>__init__</code> method.</li> <li>Now, below is an example applying the model manually. Here, pretend that the <code>test_dataloader</code> is a new data loader.</li> </ul> <pre><code>test_dataloader = data_module.test_dataloader()\n\nall_predicted_labels = []\nfor batch in test_dataloader:\n    features, _ = batch\n    logits = lightning_model(features)\n    probas = torch.sigmoid(logits)\n    predicted_labels = proba_to_label(probas)\n    all_predicted_labels.append(predicted_labels)\n\nall_predicted_labels = torch.cat(all_predicted_labels)\nall_predicted_labels[:5]\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/","title":"A Multilayer Perceptron for Ordinal Regression using CORN -- Cement Dataset","text":"<p>In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint:</p> <ul> <li>Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint;  https://arxiv.org/abs/2111.08851</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#general-settings-and-hyperparameters","title":"General settings and hyperparameters","text":"<ul> <li>Here, we specify some general hyperparameter values and general settings</li> <li>Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch</li> </ul> <pre><code>BATCH_SIZE = 128\nNUM_EPOCHS = 20\nLEARNING_RATE = 0.1\nNUM_WORKERS = 0\n\nDATA_BASEPATH = \"./\"\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","title":"Converting a regular classifier into a CORN ordinal regression model","text":"<p>Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes:</p> <p>1)</p> <p>Consider the following output layer used by a neural network classifier:</p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n</code></pre> <p>In CORN we reduce the number of classes by 1:</p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes-1)\n</code></pre> <p>2) </p> <p>We swap the cross entropy loss from PyTorch,</p> <pre><code>torch.nn.functional.cross_entropy(logits, true_labels)\n</code></pre> <p>with the CORN loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>loss = corn_loss(logits, true_labels,\n                 num_classes=num_classes)\n</code></pre> <p>Note that we pass <code>num_classes</code> instead of <code>num_classes-1</code>  to the <code>corn_loss</code> as it takes care of the rest internally.</p> <p>3)</p> <p>In a regular classifier, we usually obtain the predicted class labels as follows:</p> <pre><code>predicted_labels = torch.argmax(logits, dim=1)\n</code></pre> <p>In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels:</p> <pre><code>predicted_labels = corn_label_from_logits(logits)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","title":"Implementing a <code>MultiLayerPerceptron</code> using PyTorch Lightning's <code>LightningModule</code>","text":"<ul> <li>In this section, we set up the main model architecture using the <code>LightningModule</code> from PyTorch Lightning.</li> <li>We start with defining our <code>MultiLayerPerceptron</code> model in pure PyTorch, and then we use it in the <code>LightningModule</code> to get all the extra benefits that PyTorch Lightning provides.</li> </ul> <pre><code>import torch\n\n\n# Regular PyTorch Module\nclass MultiLayerPerceptron(torch.nn.Module):\n    def __init__(self, input_size, hidden_units, num_classes):\n        super().__init__()\n\n        # num_classes is used by the corn loss function\n        self.num_classes = num_classes\n\n        # Initialize MLP layers\n        all_layers = []\n        for hidden_unit in hidden_units:\n            layer = torch.nn.Linear(input_size, hidden_unit)\n            all_layers.append(layer)\n            all_layers.append(torch.nn.ReLU())\n            input_size = hidden_unit\n\n        # CORN output layer -------------------------------------------\n        # Regular classifier would use num_classes instead of \n        # num_classes-1 below\n        output_layer = torch.nn.Linear(hidden_units[-1], num_classes-1)\n        # -------------------------------------------------------------\n\n        all_layers.append(output_layer)\n        self.model = torch.nn.Sequential(*all_layers)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n</code></pre> <ul> <li>In our <code>LightningModule</code> we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later.</li> <li>Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes:<ol> <li>Instead of using <code>num_classes</code> in the output layer, use <code>num_classes-1</code> as shown above</li> <li>Change the loss from  <code>loss = torch.nn.functional.cross_entropy(logits, y)</code> to <code>loss = corn_loss(logits, y, num_classes=self.num_classes)</code></li> <li>To obtain the class/rank labels from the logits, change <code>predicted_labels = torch.argmax(logits, dim=1)</code> to <code>predicted_labels = corn_label_from_logits(logits)</code></li> </ol> </li> </ul> <pre><code>from coral_pytorch.losses import corn_loss\nfrom coral_pytorch.dataset import corn_label_from_logits\n\nimport pytorch_lightning as pl\nimport torchmetrics\n\n\n# LightningModule that receives a PyTorch model as input\nclass LightningMLP(pl.LightningModule):\n    def __init__(self, model, learning_rate):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        # The inherited PyTorch module\n        self.model = model\n\n        # Save settings and hyperparameters to the log directory\n        # but skip the model parameters\n        self.save_hyperparameters(ignore=['model'])\n\n        # Set up attributes for computing the MAE\n        self.train_mae = torchmetrics.MeanAbsoluteError()\n        self.valid_mae = torchmetrics.MeanAbsoluteError()\n        self.test_mae = torchmetrics.MeanAbsoluteError()\n\n    # Defining the forward method is only necessary \n    # if you want to use a Trainer's .predict() method (optional)\n    def forward(self, x):\n        return self.model(x)\n\n    # A common forward step to compute the loss and labels\n    # this is used for training, validation, and testing below\n    def _shared_step(self, batch):\n        features, true_labels = batch\n        logits = self(features)\n\n        # Use CORN loss --------------------------------------\n        # A regular classifier uses:\n        # loss = torch.nn.functional.cross_entropy(logits, y)\n        loss = corn_loss(logits, true_labels,\n                         num_classes=self.model.num_classes)\n        # ----------------------------------------------------\n\n        # CORN logits to labels ------------------------------\n        # A regular classifier uses:\n        # predicted_labels = torch.argmax(logits, dim=1)\n        predicted_labels = corn_label_from_logits(logits)\n        # ----------------------------------------------------\n\n        return loss, true_labels, predicted_labels\n\n    def training_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"train_loss\", loss)\n        self.train_mae(predicted_labels, true_labels)\n        self.log(\"train_mae\", self.train_mae, on_epoch=True, on_step=False)\n        return loss  # this is passed to the optimzer for training\n\n    def validation_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"valid_loss\", loss)\n        self.valid_mae(predicted_labels, true_labels)\n        self.log(\"valid_mae\", self.valid_mae,\n                 on_epoch=True, on_step=False, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.test_mae(predicted_labels, true_labels)\n        self.log(\"test_mae\", self.test_mae, on_epoch=True, on_step=False)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-the-dataset","title":"Setting up the dataset","text":"<ul> <li>In this section, we are going to set up our dataset.</li> <li>We start by downloading and taking a look at the Cement dataset:</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#inspecting-the-dataset","title":"Inspecting the dataset","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n\ndata_df = pd.read_csv(\"https://raw.githubusercontent.com/gagolews/\"\n                      \"ordinal_regression_data/master/cement_strength.csv\")\ndata_df[\"response\"] = data_df[\"response\"]-1  # labels should start at 0\n\ndata_labels = data_df[\"response\"]\ndata_features = data_df.loc[:, [\n    \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\ndata_df.head()\n</code></pre> response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 <pre><code>print('Number of features:', data_features.shape[1])\nprint('Number of examples:', data_features.shape[0])\nprint('Labels:', np.unique(data_labels.values))\nprint('Label distribution:', np.bincount(data_labels))\n</code></pre> <pre><code>Number of features: 8\nNumber of examples: 998\nLabels: [0 1 2 3 4]\nLabel distribution: [196 310 244 152  96]\n</code></pre> <ul> <li>Above, we can see that the dataset consists of 8 features, and there are 998 examples in total.</li> <li>The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). </li> <li>Notice also that the dataset is quite imbalanced.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#performance-baseline","title":"Performance baseline","text":"<ul> <li>Especially for imbalanced datasets, it's quite useful to compute a performance baseline.</li> <li>In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that!</li> <li>Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE).</li> <li>So, if we use the mean absolute error, , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median:</li> </ul> <pre><code>avg_prediction = np.median(data_labels.values)  # median minimizes MAE\nbaseline_mae = np.mean(np.abs(data_labels.values - avg_prediction))\nprint(f'Baseline MAE: {baseline_mae:.2f}')\n</code></pre> <pre><code>Baseline MAE: 1.03\n</code></pre> <ul> <li>In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of &gt; 1 is certainly a bad model.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#creating-a-dataset-class","title":"Creating a <code>Dataset</code> class","text":"<ul> <li>Next, let us set up a data loading mechanism for our model.</li> <li>Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets.</li> <li>First, we define a PyTorch <code>Dataset</code> class that returns the features (inputs) and labels:</li> </ul> <pre><code>from torch.utils.data import Dataset\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, feature_array, label_array, dtype=np.float32):\n        self.features = feature_array.astype(dtype)\n        self.labels = label_array\n\n    def __getitem__(self, index):\n        inputs = self.features[index]\n        label = self.labels[index]\n        return inputs, label\n\n    def __len__(self):\n        return self.features.shape[0]\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-a-datamodule","title":"Setting up a <code>DataModule</code>","text":"<ul> <li>There are three main ways we can prepare the dataset for Lightning. We can</li> <li>make the dataset part of the model;</li> <li>set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection;</li> <li>create a LightningDataModule.</li> <li>Here, we are going to use approach 3, which is the most organized approach. The <code>LightningDataModule</code> consists of several self-explanatory methods as we can see below:</li> </ul> <pre><code>import os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, data_path='./'):\n        super().__init__()\n        self.data_path = data_path\n\n    def prepare_data(self):\n        data_df = pd.read_csv(\n            'https://raw.githubusercontent.com/gagolews/'\n            'ordinal_regression_data/master/cement_strength.csv')\n        data_df.to_csv(\n            os.path.join(self.data_path, 'cement_strength.csv'), index=None)\n        return\n\n    def setup(self, stage=None):\n        data_df = pd.read_csv(\n            os.path.join(self.data_path, 'cement_strength.csv'))\n        data_df[\"response\"] = data_df[\"response\"]-1  # labels should start at 0\n        self.data_labels = data_df[\"response\"]\n        self.data_features = data_df.loc[:, [\n            \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]]\n\n        # Split into\n        # 70% train, 10% validation, 20% testing\n\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            self.data_features.values,\n            self.data_labels.values,\n            test_size=0.2,\n            random_state=1,\n            stratify=self.data_labels.values)\n\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_temp,\n            y_temp,\n            test_size=0.1,\n            random_state=1,\n            stratify=y_temp)\n\n        # Standardize features\n        sc = StandardScaler()\n        X_train_std = sc.fit_transform(X_train)\n        X_valid_std = sc.transform(X_valid)\n        X_test_std = sc.transform(X_test)\n\n        self.train = MyDataset(X_train_std, y_train)\n        self.valid = MyDataset(X_valid_std, y_valid)\n        self.test = MyDataset(X_test_std, y_test)\n\n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS,\n                          drop_last=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.valid, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS)\n\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=BATCH_SIZE,\n                          num_workers=NUM_WORKERS)\n</code></pre> <ul> <li>Note that the <code>prepare_data</code> method is usually used for steps that only need to be executed once, for example, downloading the dataset; the <code>setup</code> method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. </li> <li>Next, lets initialize the <code>DataModule</code>; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):</li> </ul> <pre><code>torch.manual_seed(1) \ndata_module = DataModule(data_path=DATA_BASEPATH)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","title":"Training the model using the PyTorch Lightning Trainer class","text":"<ul> <li>Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer).</li> <li>We wrap the model in our <code>LightningMLP</code> so that we can use PyTorch Lightning's powerful <code>Trainer</code> API.</li> <li>Also, we define a callback so that we can obtain the model with the best validation set performance after training.</li> <li>Note PyTorch Lightning offers many advanced logging services like Weights &amp; Biases. However, here, we will keep things simple and use the <code>CSVLogger</code>:</li> </ul> <pre><code>from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger\n\n\npytorch_model = MultiLayerPerceptron(\n    input_size=data_features.shape[1],\n    hidden_units=(40, 20),\n    num_classes=np.bincount(data_labels).shape[0])\n\nlightning_model = LightningMLP(\n    model=pytorch_model,\n    learning_rate=LEARNING_RATE)\n\n\ncallbacks = [ModelCheckpoint(\n    save_top_k=1, mode=\"min\", monitor=\"valid_mae\")]  # save top 1 model \nlogger = CSVLogger(save_dir=\"logs/\", name=\"mlp-corn-cement\")\n</code></pre> <ul> <li>Now it's time to train our model:</li> </ul> <pre><code>import time\n\n\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS,\n    callbacks=callbacks,\n    progress_bar_refresh_rate=50,  # recommended for notebooks\n    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n    logger=logger,\n    deterministic=True,\n    log_every_n_steps=10)\n\nstart_time = time.time()\ntrainer.fit(model=lightning_model, datamodule=data_module)\n\nruntime = (time.time() - start_time)/60\nprint(f\"Training took {runtime:.2f} min in total.\")\n</code></pre> <pre><code>/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n  rank_zero_deprecation(\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type                 | Params\n---------------------------------------------------\n0 | model     | MultiLayerPerceptron | 1.3 K \n1 | train_mae | MeanAbsoluteError    | 0     \n2 | valid_mae | MeanAbsoluteError    | 0     \n3 | test_mae  | MeanAbsoluteError    | 0     \n---------------------------------------------------\n1.3 K     Trainable params\n0         Non-trainable params\n1.3 K     Total params\n0.005     Total estimated model params size (MB)\n\n\n\nValidation sanity check: 0it [00:00, ?it/s]\n\n\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n\n\n\nTraining: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\nTraining took 0.08 min in total.\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#evaluating-the-model","title":"Evaluating the model","text":"<ul> <li>After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you):</li> </ul> <pre><code>metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='Loss')\ndf_metrics[[\"train_mae\", \"valid_mae\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='MAE')\n</code></pre> <pre><code>&lt;AxesSubplot:xlabel='Epoch', ylabel='MAE'&gt;\n</code></pre> <ul> <li>As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 175.</li> <li>The <code>trainer</code> saved this model automatically for us, we which we can load from the checkpoint via the <code>ckpt_path='best'</code> argument; below we use the <code>trainer</code> instance to evaluate the best model on the test set:</li> </ul> <pre><code>trainer.test(model=lightning_model, datamodule=data_module, ckpt_path='best')\n</code></pre> <pre><code>Restoring states from the checkpoint path at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoaded model weights from checkpoint at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt\n/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\nTesting: 0it [00:00, ?it/s]\n\n\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_mae': 0.30000001192092896}\n--------------------------------------------------------------------------------\n\n\n\n\n\n[{'test_mae': 0.30000001192092896}]\n</code></pre> <ul> <li>The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#predicting-labels-of-new-data","title":"Predicting labels of new data","text":"<ul> <li>You can use the <code>trainer.predict</code> method on a new <code>DataLoader</code> or <code>DataModule</code> to apply the model to new data.</li> <li>Alternatively, you can also manually load the best model from a checkpoint as shown below:</li> </ul> <pre><code>path = trainer.checkpoint_callback.best_model_path\nprint(path)\n</code></pre> <pre><code>logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt\n</code></pre> <pre><code>lightning_model = LightningMLP.load_from_checkpoint(\n    path, model=pytorch_model)\nlightning_model.eval();\n</code></pre> <ul> <li>Note that our <code>MultilayerPerceptron</code>, which is passed to <code>LightningMLP</code> requires input arguments. However, this is automatically being taken care of since we used <code>self.save_hyperparameters()</code> in <code>LightningMLP</code>'s <code>__init__</code> method.</li> <li>Now, below is an example applying the model manually. Here, pretend that the <code>test_dataloader</code> is a new data loader.</li> </ul> <pre><code>test_dataloader = data_module.test_dataloader()\n\nall_predicted_labels = []\nfor batch in test_dataloader:\n    features, _ = batch\n    logits = lightning_model(features)\n    predicted_labels = corn_label_from_logits(logits)\n    all_predicted_labels.append(predicted_labels)\n\nall_predicted_labels = torch.cat(all_predicted_labels)\nall_predicted_labels[:5]\n</code></pre> <pre><code>tensor([0, 3, 1, 2, 0])\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/","title":"A Convolutional Neural Net for Ordinal Regression using CORN -- MNIST Dataset","text":"<p>In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint:</p> <ul> <li>Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint;  https://arxiv.org/abs/2111.08851</li> </ul> <p>Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</p>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#general-settings-and-hyperparameters","title":"General settings and hyperparameters","text":"<ul> <li>Here, we specify some general hyperparameter values and general settings</li> <li>Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting <code>NUM_WORKERS = 0</code> instead.</li> </ul> <pre><code>BATCH_SIZE = 256\nNUM_EPOCHS = 20\nLEARNING_RATE = 0.005\nNUM_WORKERS = 4\n\nDATA_BASEPATH = \"./\"\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","title":"Converting a regular classifier into a CORN ordinal regression model","text":"<p>Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes:</p> <p>1)</p> <p>Consider the following output layer used by a neural network classifier:</p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes)\n</code></pre> <p>In CORN we reduce the number of classes by 1:</p> <pre><code>output_layer = torch.nn.Linear(hidden_units[-1], num_classes-1)\n</code></pre> <p>2) </p> <p>We swap the cross entropy loss from PyTorch,</p> <pre><code>torch.nn.functional.cross_entropy(logits, true_labels)\n</code></pre> <p>with the CORN loss (also provided via <code>coral_pytorch</code>):</p> <pre><code>loss = corn_loss(logits, true_labels,\n                 num_classes=num_classes)\n</code></pre> <p>Note that we pass <code>num_classes</code> instead of <code>num_classes-1</code>  to the <code>corn_loss</code> as it takes care of the rest internally.</p> <p>3)</p> <p>In a regular classifier, we usually obtain the predicted class labels as follows:</p> <pre><code>predicted_labels = torch.argmax(logits, dim=1)\n</code></pre> <p>In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels:</p> <pre><code>predicted_labels = corn_label_from_logits(logits)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","title":"Implementing a <code>ConvNet</code> using PyTorch Lightning's <code>LightningModule</code>","text":"<ul> <li>In this section, we set up the main model architecture using the <code>LightningModule</code> from PyTorch Lightning.</li> <li>We start with defining our convolutional neural network <code>ConvNet</code> model in pure PyTorch, and then we use it in the <code>LightningModule</code> to get all the extra benefits that PyTorch Lightning provides.</li> </ul> <pre><code>import torch\n\n\n# Regular PyTorch Module\nclass ConvNet(torch.nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n\n        # num_classes is used by the corn loss function\n        self.num_classes = num_classes\n\n        # Initialize CNN layers\n        all_layers = [\n            torch.nn.Conv2d(in_channels=in_channels, out_channels=3, \n                            kernel_size=(3, 3), stride=(1, 1), \n                            padding=1),\n            torch.nn.MaxPool2d(kernel_size=(2, 2),  stride=(2, 2)),\n            torch.nn.Conv2d(in_channels=3, out_channels=6, \n                            kernel_size=(3, 3), stride=(1, 1), \n                            padding=1),\n            torch.nn.MaxPool2d(kernel_size=(2, 2),  stride=(2, 2)),\n            torch.nn.Flatten()\n        ]\n\n        # CORN output layer --------------------------------------\n        # Regular classifier would use num_classes instead of \n        # num_classes-1 below\n        output_layer = torch.nn.Linear(294, num_classes-1)\n        # ---------------------------------------------------------\n\n        all_layers.append(output_layer)\n        self.model = torch.nn.Sequential(*all_layers)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n</code></pre> <ul> <li>In our <code>LightningModule</code> we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later.</li> <li>Given a CNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes:<ol> <li>Instead of using <code>num_classes</code> in the output layer, use <code>num_classes-1</code> as shown above</li> <li>Change the loss from  <code>loss = torch.nn.functional.cross_entropy(logits, y)</code> to <code>loss = corn_loss(logits, y, num_classes=self.num_classes)</code></li> <li>To obtain the class/rank labels from the logits, change <code>predicted_labels = torch.argmax(logits, dim=1)</code> to <code>predicted_labels = corn_label_from_logits(logits)</code></li> </ol> </li> </ul> <pre><code>from coral_pytorch.losses import corn_loss\nfrom coral_pytorch.dataset import corn_label_from_logits\n\nimport pytorch_lightning as pl\nimport torchmetrics\n\n\n# LightningModule that receives a PyTorch model as input\nclass LightningCNN(pl.LightningModule):\n    def __init__(self, model, learning_rate):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        # The inherited PyTorch module\n        self.model = model\n\n        # Save settings and hyperparameters to the log directory\n        # but skip the model parameters\n        self.save_hyperparameters(ignore=['model'])\n\n        # Set up attributes for computing the MAE\n        self.train_mae = torchmetrics.MeanAbsoluteError()\n        self.valid_mae = torchmetrics.MeanAbsoluteError()\n        self.test_mae = torchmetrics.MeanAbsoluteError()\n\n    # Defining the forward method is only necessary \n    # if you want to use a Trainer's .predict() method (optional)\n    def forward(self, x):\n        return self.model(x)\n\n    # A common forward step to compute the loss and labels\n    # this is used for training, validation, and testing below\n    def _shared_step(self, batch):\n        features, true_labels = batch\n        logits = self(features)\n\n        # Use CORN loss --------------------------------------\n        # A regular classifier uses:\n        # loss = torch.nn.functional.cross_entropy(logits, y)\n        loss = corn_loss(logits, true_labels,\n                         num_classes=self.model.num_classes)\n        # ----------------------------------------------------\n\n        # CORN logits to labels ------------------------------\n        # A regular classifier uses:\n        # predicted_labels = torch.argmax(logits, dim=1)\n        predicted_labels = corn_label_from_logits(logits)\n        # ----------------------------------------------------\n\n        return loss, true_labels, predicted_labels\n\n    def training_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"train_loss\", loss)\n        self.train_mae(predicted_labels, true_labels)\n        self.log(\"train_mae\", self.train_mae, on_epoch=True, on_step=False)\n        return loss  # this is passed to the optimzer for training\n\n    def validation_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.log(\"valid_loss\", loss)\n        self.valid_mae(predicted_labels, true_labels)\n        self.log(\"valid_mae\", self.valid_mae,\n                 on_epoch=True, on_step=False, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, true_labels, predicted_labels = self._shared_step(batch)\n        self.test_mae(predicted_labels, true_labels)\n        self.log(\"test_mae\", self.test_mae, on_epoch=True, on_step=False)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-the-dataset","title":"Setting up the dataset","text":"<ul> <li>In this section, we are going to set up our dataset.</li> <li>Please note that MNIST is not an ordinal dataset. The reason why we use MNIST in this tutorial is that it is included in the PyTorch's <code>torchvision</code> library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#inspecting-the-dataset","title":"Inspecting the dataset","text":"<pre><code>import torch\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n\ntrain_dataset = datasets.MNIST(root=DATA_BASEPATH, \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, \n                          batch_size=BATCH_SIZE, \n                          num_workers=NUM_WORKERS,\n                          drop_last=True,\n                          shuffle=True)\n\ntest_dataset = datasets.MNIST(root=DATA_BASEPATH, \n                              train=False,\n                              transform=transforms.ToTensor())\n\ntest_loader = DataLoader(dataset=test_dataset, \n                         batch_size=BATCH_SIZE,\n                         num_workers=NUM_WORKERS,\n                         drop_last=False,\n                         shuffle=False)\n\n# Checking the dataset\nall_train_labels = []\nall_test_labels = []\n\nfor images, labels in train_loader:  \n    all_train_labels.append(labels)\nall_train_labels = torch.cat(all_train_labels)\n\nfor images, labels in test_loader:  \n    all_test_labels.append(labels)\nall_test_labels = torch.cat(all_test_labels)\n</code></pre> <pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n\n\nExtracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n\n\nExtracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n\n\nExtracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n\n\nExtracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n</code></pre> <pre><code>print('Training labels:', torch.unique(all_train_labels))\nprint('Training label distribution:', torch.bincount(all_train_labels))\n\nprint('\\nTest labels:', torch.unique(all_test_labels))\nprint('Test label distribution:', torch.bincount(all_test_labels))\n</code></pre> <pre><code>Training labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nTraining label distribution: tensor([5911, 6730, 5949, 6125, 5832, 5410, 5911, 6254, 5841, 5941])\n\nTest labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nTest label distribution: tensor([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009])\n</code></pre> <ul> <li>Above, we can see that the dataset consists of 8 features, and there are 998 examples in total.</li> <li>The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). </li> <li>Notice also that the dataset is quite imbalanced.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#performance-baseline","title":"Performance baseline","text":"<ul> <li>Especially for imbalanced datasets, it's quite useful to compute a performance baseline.</li> <li>In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that!</li> <li>Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE).</li> <li>So, if we use the mean absolute error, , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median:</li> </ul> <pre><code>all_test_labels = all_test_labels.float()\navg_prediction = torch.median(all_test_labels)  # median minimizes MAE\nbaseline_mae = torch.mean(torch.abs(all_test_labels - avg_prediction))\nprint(f'Baseline MAE: {baseline_mae:.2f}')\n</code></pre> <pre><code>Baseline MAE: 2.52\n</code></pre> <ul> <li>In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of &gt; 2.52 is certainly a bad model.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-a-datamodule","title":"Setting up a <code>DataModule</code>","text":"<ul> <li>There are three main ways we can prepare the dataset for Lightning. We can</li> <li>make the dataset part of the model;</li> <li>set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection;</li> <li>create a LightningDataModule.</li> <li>Here, we are going to use approach 3, which is the most organized approach. The <code>LightningDataModule</code> consists of several self-explanatory methods as we can see below:</li> </ul> <pre><code>import os\n\nfrom torch.utils.data.dataset import random_split\nfrom torch.utils.data import DataLoader\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, data_path='./'):\n        super().__init__()\n        self.data_path = data_path\n\n    def prepare_data(self):\n        datasets.MNIST(root=self.data_path,\n                       download=True)\n        return\n\n    def setup(self, stage=None):\n        # Note transforms.ToTensor() scales input images\n        # to 0-1 range\n        train = datasets.MNIST(root=self.data_path, \n                               train=True, \n                               transform=transforms.ToTensor(),\n                               download=False)\n\n        self.test = datasets.MNIST(root=self.data_path, \n                                   train=False, \n                                   transform=transforms.ToTensor(),\n                                   download=False)\n\n        self.train, self.valid = random_split(train, lengths=[55000, 5000])\n\n    def train_dataloader(self):\n        train_loader = DataLoader(dataset=self.train, \n                                  batch_size=BATCH_SIZE, \n                                  drop_last=True,\n                                  shuffle=True,\n                                  num_workers=NUM_WORKERS)\n        return train_loader\n\n    def val_dataloader(self):\n        valid_loader = DataLoader(dataset=self.valid, \n                                  batch_size=BATCH_SIZE, \n                                  drop_last=False,\n                                  shuffle=False,\n                                  num_workers=NUM_WORKERS)\n        return valid_loader\n\n    def test_dataloader(self):\n        test_loader = DataLoader(dataset=self.test, \n                                 batch_size=BATCH_SIZE, \n                                 drop_last=False,\n                                 shuffle=False,\n                                 num_workers=NUM_WORKERS)\n        return test_loader\n</code></pre> <ul> <li>Note that the <code>prepare_data</code> method is usually used for steps that only need to be executed once, for example, downloading the dataset; the <code>setup</code> method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. </li> <li>Next, lets initialize the <code>DataModule</code>; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):</li> </ul> <pre><code>torch.manual_seed(1) \ndata_module = DataModule(data_path=DATA_BASEPATH)\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","title":"Training the model using the PyTorch Lightning Trainer class","text":"<ul> <li>Next, we initialize our CNN (<code>ConvNet</code>) model.</li> <li>Also, we define a call back so that we can obtain the model with the best validation set performance after training.</li> <li>PyTorch Lightning offers many advanced logging services like Weights &amp; Biases. Here, we will keep things simple and use the <code>CSVLogger</code>:</li> </ul> <pre><code>from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger\n\n\npytorch_model = ConvNet(\n    in_channels=1,\n    num_classes=torch.unique(all_test_labels).shape[0])\n\nlightning_model = LightningCNN(\n    pytorch_model, learning_rate=LEARNING_RATE)\n\ncallbacks = [ModelCheckpoint(\n    save_top_k=1, mode='min', monitor=\"valid_mae\")]  # save top 1 model \nlogger = CSVLogger(save_dir=\"logs/\", name=\"cnn-corn-mnist\")\n</code></pre> <ul> <li>Now it's time to train our model:</li> </ul> <pre><code>import time\n\n\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS,\n    callbacks=callbacks,\n    progress_bar_refresh_rate=50,  # recommended for notebooks\n    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n    logger=logger,\n    deterministic=True,\n    log_every_n_steps=10)\n\nstart_time = time.time()\ntrainer.fit(model=lightning_model, datamodule=data_module)\n\nruntime = (time.time() - start_time)/60\nprint(f\"Training took {runtime:.2f} min in total.\")\n</code></pre> <pre><code>/home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n  rank_zero_deprecation(\nGPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nMissing logger folder: logs/cnn-corn-mnist\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name      | Type              | Params\n------------------------------------------------\n0 | model     | ConvNet           | 2.9 K \n1 | train_mae | MeanAbsoluteError | 0     \n2 | valid_mae | MeanAbsoluteError | 0     \n3 | test_mae  | MeanAbsoluteError | 0     \n------------------------------------------------\n2.9 K     Trainable params\n0         Non-trainable params\n2.9 K     Total params\n0.011     Total estimated model params size (MB)\n\n\n\nSanity Checking: 0it [00:00, ?it/s]\n\n\n\nTraining: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\n\nValidation: 0it [00:00, ?it/s]\n\n\nTraining took 1.38 min in total.\n</code></pre>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#evaluating-the-model","title":"Evaluating the model","text":"<ul> <li>After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you):</li> </ul> <pre><code>import pandas as pd\n\n\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='Loss')\ndf_metrics[[\"train_mae\", \"valid_mae\"]].plot(\n    grid=True, legend=True, xlabel='Epoch', ylabel='MAE')\n</code></pre> <pre><code>&lt;AxesSubplot:xlabel='Epoch', ylabel='MAE'&gt;\n</code></pre> <ul> <li>As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 16.</li> <li>The <code>trainer</code> saved this model automatically for us, we which we can load from the checkpoint via the <code>ckpt_path='best'</code> argument; below we use the <code>trainer</code> instance to evaluate the best model on the test set:</li> </ul> <pre><code>trainer.test(model=lightning_model, datamodule=data_module, ckpt_path='best')\n</code></pre> <pre><code>Restoring states from the checkpoint path at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoaded model weights from checkpoint at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt\n\n\n\nTesting: 0it [00:00, ?it/s]\n</code></pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_mae          \u2502    0.11959999799728394    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre><code>[{'test_mae': 0.11959999799728394}]\n</code></pre> <ul> <li>The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.</li> </ul>"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#predicting-labels-of-new-data","title":"Predicting labels of new data","text":"<ul> <li>You can use the <code>trainer.predict</code> method on a new <code>DataLoader</code> or <code>DataModule</code> to apply the model to new data.</li> <li>Alternatively, you can also manually load the best model from a checkpoint as shown below:</li> </ul> <pre><code>path = trainer.checkpoint_callback.best_model_path\nprint(path)\n</code></pre> <pre><code>logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt\n</code></pre> <pre><code>lightning_model = LightningCNN.load_from_checkpoint(path, model=pytorch_model)\nlightning_model.eval();\n</code></pre> <ul> <li>Note that our <code>ConvNet</code>, which is passed to <code>LightningCNN</code> requires input arguments. However, this is automatically being taken care of since we used <code>self.save_hyperparameters()</code> in <code>LightningCNN</code>'s <code>__init__</code> method.</li> <li>Now, below is an example applying the model manually. Here, pretend that the <code>test_dataloader</code> is a new data loader.</li> </ul> <pre><code>test_dataloader = data_module.test_dataloader()\n\nall_predicted_labels = []\nfor batch in test_dataloader:\n    features, _ = batch\n    logits = lightning_model(features)\n    predicted_labels = corn_label_from_logits(logits)\n    all_predicted_labels.append(predicted_labels)\n\nall_predicted_labels = torch.cat(all_predicted_labels)\nall_predicted_labels[:5]\n</code></pre> <pre><code>tensor([7, 2, 1, 0, 4])\n</code></pre>"}]}