{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CORAL & CORN implementations for ordinal regression with deep neural networks. About CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet References CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"Home"},{"location":"#about","text":"CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet","title":"About"},{"location":"#references","text":"CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"References"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md. 1.3.0 (07-16-2022) Downloads Source code (zip) Source code (tar.gz) New Features - Changes Fixes a bug where the normalization of the corn_loss different from the one proposed in the original paper. ( #22 ) Bug Fixes - 1.2.0 (11-17-2021) Downloads Source code (zip) Source code (tar.gz) New Features Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \" Changes Bug Fixes - 1.1.0 (04/08/2021) Downloads Source code (zip) Source code (tar.gz) New Features - Changes By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 ) Bug Fixes - 1.0.0 (11/15/2020) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Changelog"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md.","title":"Release Notes"},{"location":"CHANGELOG/#130-07-16-2022","text":"","title":"1.3.0 (07-16-2022)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes","text":"Fixes a bug where the normalization of the corn_loss different from the one proposed in the original paper. ( #22 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#120-11-17-2021","text":"","title":"1.2.0 (11-17-2021)"},{"location":"CHANGELOG/#downloads_1","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_1","text":"Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \"","title":"New Features"},{"location":"CHANGELOG/#changes_1","text":"","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#110-04082021","text":"","title":"1.1.0 (04/08/2021)"},{"location":"CHANGELOG/#downloads_2","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_2","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes_2","text":"By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_2","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#100-11152020","text":"","title":"1.0.0 (11/15/2020)"},{"location":"CHANGELOG/#downloads_3","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_3","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes_3","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_3","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use CORAL or CORN as part of your workflow in a scientific publication, please consider citing the corresponding paper: CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title={Rank consistent ordinal regression for neural networks with application to age estimation}, journal={Pattern Recognition Letters}, volume={140}, pages={325-331}, year={2020}, issn={0167-8655}, doi={https://doi.org/10.1016/j.patrec.2020.11.008}, url={http://www.sciencedirect.com/science/article/pii/S016786552030413X}, author={Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka} } CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 @misc{shi2021deep, title={Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities}, author={Xintong Shi and Wenzhi Cao and Sebastian Raschka}, year={2021}, eprint={2111.08851}, archivePrefix={arXiv}, primaryClass={cs.LG} }","title":"Citing"},{"location":"installation/","text":"Installing coral_pytorch Requirements Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0 PyPI You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Installation"},{"location":"installation/#installing-coral_pytorch","text":"","title":"Installing coral_pytorch"},{"location":"installation/#requirements","text":"Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Latest GitHub Source Code"},{"location":"license/","text":"MIT License Copyright (c) 2020-2022 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2020-2022 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"tutorials/pure_pytorch/CORAL_cement/","text":"CORAL MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"CORAL MLP model for tabular data (Cement dataset)"},{"location":"tutorials/pure_pytorch/CORAL_cement/#coral-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression.","title":"CORAL MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/pure_pytorch/CORAL_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/pure_pytorch/CORAL_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/pure_pytorch/CORAL_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/pure_pytorch/CORAL_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORAL_cement/#2-equipping-mlp-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with CORAL layer"},{"location":"tutorials/pure_pytorch/CORAL_cement/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/pure_pytorch/CORAL_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORAL_mnist/","text":"CORAL CNN for predicting handwritten digits (MNIST) This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"CORAL CNN model for image data (MNIST dataset)"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#coral-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORAL CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#2-equipping-cnn-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with CORAL layer"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_cement/","text":"CORN MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cuda:0 from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with a CORN layer In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 0.7095 Epoch: 002/020 | Batch 000/007 | Cost: 0.5793 Epoch: 003/020 | Batch 000/007 | Cost: 0.5107 Epoch: 004/020 | Batch 000/007 | Cost: 0.4893 Epoch: 005/020 | Batch 000/007 | Cost: 0.4294 Epoch: 006/020 | Batch 000/007 | Cost: 0.3942 Epoch: 007/020 | Batch 000/007 | Cost: 0.3905 Epoch: 008/020 | Batch 000/007 | Cost: 0.3877 Epoch: 009/020 | Batch 000/007 | Cost: 0.3327 Epoch: 010/020 | Batch 000/007 | Cost: 0.3442 Epoch: 011/020 | Batch 000/007 | Cost: 0.3513 Epoch: 012/020 | Batch 000/007 | Cost: 0.3395 Epoch: 013/020 | Batch 000/007 | Cost: 0.3272 Epoch: 014/020 | Batch 000/007 | Cost: 0.3372 Epoch: 015/020 | Batch 000/007 | Cost: 0.2994 Epoch: 016/020 | Batch 000/007 | Cost: 0.3409 Epoch: 017/020 | Batch 000/007 | Cost: 0.3158 Epoch: 018/020 | Batch 000/007 | Cost: 0.2988 Epoch: 019/020 | Batch 000/007 | Cost: 0.2793 Epoch: 020/020 | Batch 000/007 | Cost: 0.2516 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.29 | 0.36 Mean squared error (train/test): 0.34 | 0.39 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[8.4400e-01, 1.1552e-01, 2.4885e-02, 2.1235e-02], [9.6955e-01, 9.6440e-01, 7.9017e-01, 4.0131e-01], [9.6926e-01, 9.6164e-01, 2.8837e-01, 1.1151e-01], [2.7557e-01, 1.7854e-03, 1.3533e-04, 6.4534e-05], [4.4200e-04, 2.9050e-05, 1.8071e-05, 8.5216e-06], [4.1626e-02, 6.8911e-06, 1.1300e-06, 1.1232e-06], [9.5031e-01, 3.2661e-01, 7.6083e-03, 3.6258e-03], [9.8467e-01, 9.0953e-01, 4.3580e-01, 3.9399e-01], [8.0870e-01, 1.9610e-01, 1.9341e-02, 1.6238e-03], [9.6289e-01, 7.2809e-01, 2.1034e-01, 1.4426e-01], [9.8087e-01, 3.4986e-01, 7.5893e-03, 2.1336e-04], [8.3218e-02, 2.9795e-04, 8.8117e-05, 7.7257e-05], [6.4886e-01, 3.3336e-01, 1.7751e-01, 1.1291e-01], [8.0380e-01, 5.5894e-03, 3.1419e-04, 2.4602e-04], [9.3716e-01, 9.3670e-01, 9.3338e-01, 8.3394e-01], [9.0723e-01, 9.0255e-01, 8.7473e-01, 4.9182e-01], [9.8959e-01, 3.3517e-01, 5.4329e-02, 1.7331e-03], [9.6824e-01, 8.0327e-01, 2.5958e-01, 8.4942e-03], [9.6470e-01, 9.1665e-01, 6.9238e-01, 3.8931e-01], [9.6623e-01, 9.6491e-01, 9.4429e-01, 4.3117e-01], [8.0910e-02, 1.5353e-04, 2.7122e-05, 2.1541e-05], [9.9247e-01, 8.6671e-01, 6.3087e-01, 6.6279e-02], [8.8915e-01, 2.5603e-02, 1.8793e-03, 1.5186e-03], [6.2060e-01, 1.8354e-01, 4.0813e-02, 2.1553e-02], [9.5856e-01, 9.5805e-01, 9.2657e-01, 1.6030e-01], [9.9292e-01, 6.5836e-01, 1.8671e-01, 6.0837e-02], [1.0555e-01, 4.6840e-03, 1.1164e-03, 1.7749e-04], [9.6029e-01, 4.0485e-01, 3.0195e-02, 2.0155e-03], [9.8264e-01, 9.1183e-01, 4.3322e-01, 2.3925e-03], [8.9595e-01, 3.6590e-01, 3.0114e-02, 1.9936e-03]], device='cuda:0')","title":"CORN MLP model for tabular data (Cement dataset)"},{"location":"tutorials/pure_pytorch/CORN_cement/#corn-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression.","title":"CORN MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/pure_pytorch/CORN_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/pure_pytorch/CORN_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/pure_pytorch/CORN_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/pure_pytorch/CORN_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cuda:0 from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORN_cement/#2-equipping-mlp-with-a-corn-layer","text":"In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with a CORN layer"},{"location":"tutorials/pure_pytorch/CORN_cement/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 0.7095 Epoch: 002/020 | Batch 000/007 | Cost: 0.5793 Epoch: 003/020 | Batch 000/007 | Cost: 0.5107 Epoch: 004/020 | Batch 000/007 | Cost: 0.4893 Epoch: 005/020 | Batch 000/007 | Cost: 0.4294 Epoch: 006/020 | Batch 000/007 | Cost: 0.3942 Epoch: 007/020 | Batch 000/007 | Cost: 0.3905 Epoch: 008/020 | Batch 000/007 | Cost: 0.3877 Epoch: 009/020 | Batch 000/007 | Cost: 0.3327 Epoch: 010/020 | Batch 000/007 | Cost: 0.3442 Epoch: 011/020 | Batch 000/007 | Cost: 0.3513 Epoch: 012/020 | Batch 000/007 | Cost: 0.3395 Epoch: 013/020 | Batch 000/007 | Cost: 0.3272 Epoch: 014/020 | Batch 000/007 | Cost: 0.3372 Epoch: 015/020 | Batch 000/007 | Cost: 0.2994 Epoch: 016/020 | Batch 000/007 | Cost: 0.3409 Epoch: 017/020 | Batch 000/007 | Cost: 0.3158 Epoch: 018/020 | Batch 000/007 | Cost: 0.2988 Epoch: 019/020 | Batch 000/007 | Cost: 0.2793 Epoch: 020/020 | Batch 000/007 | Cost: 0.2516","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/pure_pytorch/CORN_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.29 | 0.36 Mean squared error (train/test): 0.34 | 0.39 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_cement/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[8.4400e-01, 1.1552e-01, 2.4885e-02, 2.1235e-02], [9.6955e-01, 9.6440e-01, 7.9017e-01, 4.0131e-01], [9.6926e-01, 9.6164e-01, 2.8837e-01, 1.1151e-01], [2.7557e-01, 1.7854e-03, 1.3533e-04, 6.4534e-05], [4.4200e-04, 2.9050e-05, 1.8071e-05, 8.5216e-06], [4.1626e-02, 6.8911e-06, 1.1300e-06, 1.1232e-06], [9.5031e-01, 3.2661e-01, 7.6083e-03, 3.6258e-03], [9.8467e-01, 9.0953e-01, 4.3580e-01, 3.9399e-01], [8.0870e-01, 1.9610e-01, 1.9341e-02, 1.6238e-03], [9.6289e-01, 7.2809e-01, 2.1034e-01, 1.4426e-01], [9.8087e-01, 3.4986e-01, 7.5893e-03, 2.1336e-04], [8.3218e-02, 2.9795e-04, 8.8117e-05, 7.7257e-05], [6.4886e-01, 3.3336e-01, 1.7751e-01, 1.1291e-01], [8.0380e-01, 5.5894e-03, 3.1419e-04, 2.4602e-04], [9.3716e-01, 9.3670e-01, 9.3338e-01, 8.3394e-01], [9.0723e-01, 9.0255e-01, 8.7473e-01, 4.9182e-01], [9.8959e-01, 3.3517e-01, 5.4329e-02, 1.7331e-03], [9.6824e-01, 8.0327e-01, 2.5958e-01, 8.4942e-03], [9.6470e-01, 9.1665e-01, 6.9238e-01, 3.8931e-01], [9.6623e-01, 9.6491e-01, 9.4429e-01, 4.3117e-01], [8.0910e-02, 1.5353e-04, 2.7122e-05, 2.1541e-05], [9.9247e-01, 8.6671e-01, 6.3087e-01, 6.6279e-02], [8.8915e-01, 2.5603e-02, 1.8793e-03, 1.5186e-03], [6.2060e-01, 1.8354e-01, 4.0813e-02, 2.1553e-02], [9.5856e-01, 9.5805e-01, 9.2657e-01, 1.6030e-01], [9.9292e-01, 6.5836e-01, 1.8671e-01, 6.0837e-02], [1.0555e-01, 4.6840e-03, 1.1164e-03, 1.7749e-04], [9.6029e-01, 4.0485e-01, 3.0195e-02, 2.0155e-03], [9.8264e-01, 9.1183e-01, 4.3322e-01, 2.3925e-03], [8.9595e-01, 3.6590e-01, 3.0114e-02, 1.9936e-03]], device='cuda:0')","title":"5 -- Rank probabilities from logits"},{"location":"tutorials/pure_pytorch/CORN_mnist/","text":"CORN CNN for predicting handwritten digits (MNIST) This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cuda:0 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with a CORN layer In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 0.6896 Epoch: 001/010 | Batch 200/468 | Cost: 0.1449 Epoch: 001/010 | Batch 400/468 | Cost: 0.0761 Epoch: 002/010 | Batch 000/468 | Cost: 0.0927 Epoch: 002/010 | Batch 200/468 | Cost: 0.0679 Epoch: 002/010 | Batch 400/468 | Cost: 0.0714 Epoch: 003/010 | Batch 000/468 | Cost: 0.0593 Epoch: 003/010 | Batch 200/468 | Cost: 0.0516 Epoch: 003/010 | Batch 400/468 | Cost: 0.0470 Epoch: 004/010 | Batch 000/468 | Cost: 0.0301 Epoch: 004/010 | Batch 200/468 | Cost: 0.0417 Epoch: 004/010 | Batch 400/468 | Cost: 0.0366 Epoch: 005/010 | Batch 000/468 | Cost: 0.0449 Epoch: 005/010 | Batch 200/468 | Cost: 0.0380 Epoch: 005/010 | Batch 400/468 | Cost: 0.0141 Epoch: 006/010 | Batch 000/468 | Cost: 0.0272 Epoch: 006/010 | Batch 200/468 | Cost: 0.0267 Epoch: 006/010 | Batch 400/468 | Cost: 0.0405 Epoch: 007/010 | Batch 000/468 | Cost: 0.0649 Epoch: 007/010 | Batch 200/468 | Cost: 0.0253 Epoch: 007/010 | Batch 400/468 | Cost: 0.0215 Epoch: 008/010 | Batch 000/468 | Cost: 0.0389 Epoch: 008/010 | Batch 200/468 | Cost: 0.0297 Epoch: 008/010 | Batch 400/468 | Cost: 0.0343 Epoch: 009/010 | Batch 000/468 | Cost: 0.0249 Epoch: 009/010 | Batch 200/468 | Cost: 0.0498 Epoch: 009/010 | Batch 400/468 | Cost: 0.0300 Epoch: 010/010 | Batch 000/468 | Cost: 0.0201 Epoch: 010/010 | Batch 200/468 | Cost: 0.0290 Epoch: 010/010 | Batch 400/468 | Cost: 0.0303 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.15 | 0.15 Mean squared error (train/test): 0.69 | 0.74 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9986e-01, 9.9941e-01, 2.5950e-08], [1.0000e+00, 1.0000e+00, 9.9315e-01, ..., 9.8477e-01, 9.8476e-01, 9.7987e-08], [9.1224e-01, 9.1223e-01, 9.1223e-01, ..., 8.5374e-01, 8.5216e-01, 1.6753e-03], ..., [9.9812e-01, 9.9811e-01, 9.9811e-01, ..., 9.8991e-01, 9.8968e-01, 4.1033e-03], [9.9979e-01, 9.9979e-01, 9.9979e-01, ..., 1.5020e-02, 1.5015e-02, 2.7997e-04], [7.7070e-07, 7.7070e-07, 7.5224e-07, ..., 7.6964e-08, 7.6941e-08, 6.1278e-13]], device='cuda:0')","title":"CORN CNN model for image data (MNIST dataset)"},{"location":"tutorials/pure_pytorch/CORN_mnist/#corn-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORN CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/pure_pytorch/CORN_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cuda:0 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORN_mnist/#2-equipping-cnn-with-a-corn-layer","text":"In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with a CORN layer"},{"location":"tutorials/pure_pytorch/CORN_mnist/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 0.6896 Epoch: 001/010 | Batch 200/468 | Cost: 0.1449 Epoch: 001/010 | Batch 400/468 | Cost: 0.0761 Epoch: 002/010 | Batch 000/468 | Cost: 0.0927 Epoch: 002/010 | Batch 200/468 | Cost: 0.0679 Epoch: 002/010 | Batch 400/468 | Cost: 0.0714 Epoch: 003/010 | Batch 000/468 | Cost: 0.0593 Epoch: 003/010 | Batch 200/468 | Cost: 0.0516 Epoch: 003/010 | Batch 400/468 | Cost: 0.0470 Epoch: 004/010 | Batch 000/468 | Cost: 0.0301 Epoch: 004/010 | Batch 200/468 | Cost: 0.0417 Epoch: 004/010 | Batch 400/468 | Cost: 0.0366 Epoch: 005/010 | Batch 000/468 | Cost: 0.0449 Epoch: 005/010 | Batch 200/468 | Cost: 0.0380 Epoch: 005/010 | Batch 400/468 | Cost: 0.0141 Epoch: 006/010 | Batch 000/468 | Cost: 0.0272 Epoch: 006/010 | Batch 200/468 | Cost: 0.0267 Epoch: 006/010 | Batch 400/468 | Cost: 0.0405 Epoch: 007/010 | Batch 000/468 | Cost: 0.0649 Epoch: 007/010 | Batch 200/468 | Cost: 0.0253 Epoch: 007/010 | Batch 400/468 | Cost: 0.0215 Epoch: 008/010 | Batch 000/468 | Cost: 0.0389 Epoch: 008/010 | Batch 200/468 | Cost: 0.0297 Epoch: 008/010 | Batch 400/468 | Cost: 0.0343 Epoch: 009/010 | Batch 000/468 | Cost: 0.0249 Epoch: 009/010 | Batch 200/468 | Cost: 0.0498 Epoch: 009/010 | Batch 400/468 | Cost: 0.0300 Epoch: 010/010 | Batch 000/468 | Cost: 0.0201 Epoch: 010/010 | Batch 200/468 | Cost: 0.0290 Epoch: 010/010 | Batch 400/468 | Cost: 0.0303","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/pure_pytorch/CORN_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.15 | 0.15 Mean squared error (train/test): 0.69 | 0.74 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_mnist/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9986e-01, 9.9941e-01, 2.5950e-08], [1.0000e+00, 1.0000e+00, 9.9315e-01, ..., 9.8477e-01, 9.8476e-01, 9.7987e-08], [9.1224e-01, 9.1223e-01, 9.1223e-01, ..., 8.5374e-01, 8.5216e-01, 1.6753e-03], ..., [9.9812e-01, 9.9811e-01, 9.9811e-01, ..., 9.8991e-01, 9.8968e-01, 4.1033e-03], [9.9979e-01, 9.9979e-01, 9.9979e-01, ..., 1.5020e-02, 1.5015e-02, 2.7997e-04], [7.7070e-07, 7.7070e-07, 7.5224e-07, ..., 7.6964e-08, 7.6941e-08, 6.1278e-13]], device='cuda:0')","title":"5 -- Rank probabilities from logits"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/","text":"A Multilayer Perceptron for Ordinal Regression using CORAL -- Cement Dataset In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 32 NUM_EPOCHS = 200 LEARNING_RATE = 0.01 NUM_WORKERS = 0 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the CORAL loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(hidden_units[-1], num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels . type_as ( logits )) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset: Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model. Creating a Dataset class Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ] Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 24 , 16 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-coral-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 636 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 636 Trainable params 0 Non-trainable params 636 Total params 0.003 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.94 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 110. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.25} -------------------------------------------------------------------------------- [{'test_mae': 0.25}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 4, 0, 3, 1])","title":"CORAL multilayer perceptron for tabular data (Cement dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#a-multilayer-perceptron-for-ordinal-regression-using-coral-cement-dataset","text":"In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331","title":"A Multilayer Perceptron for Ordinal Regression using CORAL -- Cement Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 32 NUM_EPOCHS = 200 LEARNING_RATE = 0.01 NUM_WORKERS = 0 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the CORAL loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(hidden_units[-1], num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels . type_as ( logits )) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset:","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#creating-a-dataset-class","text":"Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ]","title":"Creating a Dataset class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 24 , 16 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-coral-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 636 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 636 Trainable params 0 Non-trainable params 636 Total params 0.003 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.94 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 110. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.25} -------------------------------------------------------------------------------- [{'test_mae': 0.25}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 4, 0, 3, 1])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/","text":"A Convolutional Neural Net for Ordinal Regression using CORAL -- MNIST Dataset In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing a ConvNet using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(294, num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = 294 , num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( logits ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [4], in <cell line: 5>() 2 from coral_pytorch.dataset import levels_from_labelbatch 3 from coral_pytorch.dataset import proba_to_label ----> 5 import pytorch_lightning as pl 6 import torchmetrics 9 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy' Setting up the dataset In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. Inspecting the dataset import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-coral-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) It's hard to tell what the best model (based on the lowest validation set MAE) is in this case, but no worries, the trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"CORAL convolutional neural net for image data (MNIST dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#a-convolutional-neural-net-for-ordinal-regression-using-coral-mnist-dataset","text":"In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"A Convolutional Neural Net for Ordinal Regression using CORAL -- MNIST Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(294, num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = 294 , num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( logits ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [4], in <cell line: 5>() 2 from coral_pytorch.dataset import levels_from_labelbatch 3 from coral_pytorch.dataset import proba_to_label ----> 5 import pytorch_lightning as pl 6 import torchmetrics 9 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy'","title":"Implementing a ConvNet using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#inspecting-the-dataset","text":"import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-coral-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" )","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) It's hard to tell what the best model (based on the lowest validation set MAE) is in this case, but no worries, the trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/","text":"A Recurrent Neural Net for Ordinal Regression using CORAL -- TripAdvisor Dataset In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.0005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing an RNN using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # self.output_layer = torch.nn.Linear(hidden_dim, num_classes) # We replace it by the CORAL layer: self . output_layer = CoralLayer ( size_in = hidden_dim , num_classes = num_classes ) # ---------------------------------------------------------------- def forward ( self , text , text_length ): # text dim: [sentence length, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence length, batch size, embedding dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence length, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( features ) # ------------------------------------------------------- logits = self ( features , text_length ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ----------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TEXT_COLUMN_NAME LABEL_COLUMN_NAME 6995 beautiful hotel, stay punta cana majestic colo... 5 6996 stay, n't stay, stayed week april, weather ama... 5 6997 stay hotel fantastic, great location, looked n... 5 6998 birthday meal havnt stayed hotel staying barce... 5 6999 great hotel great location stayed royal magda ... 5 import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) 39:1: E122 continuation line missing indentation or outdented # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Training label distribution: tensor([964, 963, 954, 953, 926], device='cuda:0') Test labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Test label distribution: tensor([275, 267, 300, 274, 284], device='cuda:0') Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.18 In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): Training the model using the PyTorch Lightning Trainer class Next, we initialize our RNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-coral-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Text length: 469 Batch size (from text): 16 Batch size (from labels): 16 Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | PyTorchRNN | 3.0 M 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 3.0 M Trainable params 0 Non-trainable params 3.0 M Total params 11.823 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 3.39 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 5. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 1.0885714292526245} -------------------------------------------------------------------------------- [{'test_mae': 1.0885714292526245}] Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([2, 0, 3, 1, 1], device='cuda:0')","title":"CORAL recurrent neural net for text data (TripAdvisor dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#a-recurrent-neural-net-for-ordinal-regression-using-coral-tripadvisor-dataset","text":"In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript.","title":"A Recurrent Neural Net for Ordinal Regression using CORAL -- TripAdvisor Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.0005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#implementing-an-rnn-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # self.output_layer = torch.nn.Linear(hidden_dim, num_classes) # We replace it by the CORAL layer: self . output_layer = CoralLayer ( size_in = hidden_dim , num_classes = num_classes ) # ---------------------------------------------------------------- def forward ( self , text , text_length ): # text dim: [sentence length, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence length, batch size, embedding dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence length, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( features ) # ------------------------------------------------------- logits = self ( features , text_length ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ----------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing an RNN using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TEXT_COLUMN_NAME LABEL_COLUMN_NAME 6995 beautiful hotel, stay punta cana majestic colo... 5 6996 stay, n't stay, stayed week april, weather ama... 5 6997 stay hotel fantastic, great location, looked n... 5 6998 birthday meal havnt stayed hotel staying barce... 5 6999 great hotel great location stayed royal magda ... 5 import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) 39:1: E122 continuation line missing indentation or outdented # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Training label distribution: tensor([964, 963, 954, 953, 926], device='cuda:0') Test labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Test label distribution: tensor([275, 267, 300, 274, 284], device='cuda:0') Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.18 In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our RNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-coral-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Text length: 469 Batch size (from text): 16 Batch size (from labels): 16 Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | PyTorchRNN | 3.0 M 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 3.0 M Trainable params 0 Non-trainable params 3.0 M Total params 11.823 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 3.39 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 5. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 1.0885714292526245} -------------------------------------------------------------------------------- [{'test_mae': 1.0885714292526245}]","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([2, 0, 3, 1, 1], device='cuda:0')","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/","text":"A Multilayer Perceptron for Ordinal Regression using CORN -- Cement Dataset In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 128 NUM_EPOCHS = 20 LEARNING_RATE = 0.1 NUM_WORKERS = 0 DATA_BASEPATH = \"./\" Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORN output layer ------------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) # ------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset: Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model. Creating a Dataset class Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ] Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 40 , 20 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-corn-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 1.3 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 1.3 K Trainable params 0 Non-trainable params 1.3 K Total params 0.005 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.08 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 175. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.30000001192092896} -------------------------------------------------------------------------------- [{'test_mae': 0.30000001192092896}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 3, 1, 2, 0])","title":"CORN multilayer perceptron for tabular data (Cement dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#a-multilayer-perceptron-for-ordinal-regression-using-corn-cement-dataset","text":"In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"A Multilayer Perceptron for Ordinal Regression using CORN -- Cement Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 128 NUM_EPOCHS = 20 LEARNING_RATE = 0.1 NUM_WORKERS = 0 DATA_BASEPATH = \"./\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORN output layer ------------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) # ------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset:","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#creating-a-dataset-class","text":"Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ]","title":"Creating a Dataset class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 40 , 20 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-corn-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 1.3 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 1.3 K Trainable params 0 Non-trainable params 1.3 K Total params 0.005 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.08 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 175. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.30000001192092896} -------------------------------------------------------------------------------- [{'test_mae': 0.30000001192092896}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 3, 1, 2, 0])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/","text":"A Convolutional Neural Net for Ordinal Regression using CORN -- MNIST Dataset In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./\" Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing a ConvNet using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORN output layer -------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( 294 , num_classes - 1 ) # --------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a CNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. Inspecting the dataset import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Training label distribution: tensor([5911, 6730, 5949, 6125, 5832, 5410, 5911, 6254, 5841, 5941]) Test labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Test label distribution: tensor([ 980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 2.52 In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-corn-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs Missing logger folder: logs/cnn-corn-mnist LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | ConvNet | 2.9 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 2.9 K Trainable params 0 Non-trainable params 2.9 K Total params 0.011 Total estimated model params size (MB) Sanity Checking: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Training took 1.38 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 16. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt Testing: 0it [00:00, ?it/s] \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Test metric \u2503 DataLoader 0 \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 test_mae \u2502 0.11959999799728394 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [{'test_mae': 0.11959999799728394}] The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([7, 2, 1, 0, 4])","title":"CORN convolutional neural net for image data (MNIST dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#a-convolutional-neural-net-for-ordinal-regression-using-corn-mnist-dataset","text":"In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"A Convolutional Neural Net for Ordinal Regression using CORN -- MNIST Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORN output layer -------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( 294 , num_classes - 1 ) # --------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a CNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a ConvNet using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#inspecting-the-dataset","text":"import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Training label distribution: tensor([5911, 6730, 5949, 6125, 5832, 5410, 5911, 6254, 5841, 5941]) Test labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Test label distribution: tensor([ 980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 2.52 In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-corn-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs Missing logger folder: logs/cnn-corn-mnist LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | ConvNet | 2.9 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 2.9 K Trainable params 0 Non-trainable params 2.9 K Total params 0.011 Total estimated model params size (MB) Sanity Checking: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Training took 1.38 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 16. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt Testing: 0it [00:00, ?it/s] \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Test metric \u2503 DataLoader 0 \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 test_mae \u2502 0.11959999799728394 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [{'test_mae': 0.11959999799728394}] The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([7, 2, 1, 0, 4])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/","text":"A Recurrent Neural Net for Ordinal Regression using CORN -- TripAdvisor Dataset In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" This tutorial also requires the spacy English vocabulary, which can be downloaded as shown below: python - m spacy download en_core_web_sm Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing an RNN using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORN output layer ------------------------------------------ # Regular classifier would use num_classes instead of # num_classes-1 below self . output_layer = torch . nn . Linear ( hidden_dim , num_classes - 1 ) # ------------------------------------------------------------ self . num_classes = num_classes def forward ( self , text , text_length ): # text dim: [sentence len, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence len, batch size, embed dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence len, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given an RNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME logits = self ( features , text_length ) # Use CORN loss --------------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ----------------------------------------------------------------- # CORN logits to labels ------------------------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ----------------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [5], in <cell line: 4>() 1 from coral_pytorch.losses import corn_loss 2 from coral_pytorch.dataset import corn_label_from_logits ----> 4 import pytorch_lightning as pl 5 import torchmetrics 8 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy' Setting up the dataset In this section, we are going to set up our dataset. Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Training the model using the PyTorch Lightning Trainer class Next, we initialize our PyTorchRNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-corn-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 8. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"CORN recurrent neural net for text data (TripAdvisor dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#a-recurrent-neural-net-for-ordinal-regression-using-corn-tripadvisor-dataset","text":"In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript.","title":"A Recurrent Neural Net for Ordinal Regression using CORN -- TripAdvisor Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" This tutorial also requires the spacy English vocabulary, which can be downloaded as shown below: python - m spacy download en_core_web_sm","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#implementing-an-rnn-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORN output layer ------------------------------------------ # Regular classifier would use num_classes instead of # num_classes-1 below self . output_layer = torch . nn . Linear ( hidden_dim , num_classes - 1 ) # ------------------------------------------------------------ self . num_classes = num_classes def forward ( self , text , text_length ): # text dim: [sentence len, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence len, batch size, embed dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence len, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given an RNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME logits = self ( features , text_length ) # Use CORN loss --------------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ----------------------------------------------------------------- # CORN logits to labels ------------------------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ----------------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [5], in <cell line: 4>() 1 from coral_pytorch.losses import corn_loss 2 from coral_pytorch.dataset import corn_label_from_logits ----> 4 import pytorch_lightning as pl 5 import torchmetrics 8 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy'","title":"Implementing an RNN using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly.","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our PyTorchRNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-corn-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" )","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 8. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' )","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"Predicting labels of new data"}]}