{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CORAL implementation for ordinal regression with deep neural networks. About CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet References CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"Home"},{"location":"#about","text":"CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet","title":"About"},{"location":"#references","text":"CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"References"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md. 1.2.0dev (11-17-2021) Downloads Source code (zip) Source code (tar.gz) New Features Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \" Changes Bug Fixes - 1.1.0 (04/08/2021) Downloads Source code (zip) Source code (tar.gz) New Features - Changes By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 ) Bug Fixes - 1.0.0 (11/15/2020) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Release Notes"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md.","title":"Release Notes"},{"location":"CHANGELOG/#120dev-11-17-2021","text":"","title":"1.2.0dev (11-17-2021)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \"","title":"New Features"},{"location":"CHANGELOG/#changes","text":"","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#110-04082021","text":"","title":"1.1.0 (04/08/2021)"},{"location":"CHANGELOG/#downloads_1","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_1","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes_1","text":"By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#100-11152020","text":"","title":"1.0.0 (11/15/2020)"},{"location":"CHANGELOG/#downloads_2","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_2","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes_2","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_2","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use CORAL or CORN as part of your workflow in a scientific publication, please consider citing the corresponding paper: CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title={Rank consistent ordinal regression for neural networks with application to age estimation}, journal={Pattern Recognition Letters}, volume={140}, pages={325-331}, year={2020}, issn={0167-8655}, doi={https://doi.org/10.1016/j.patrec.2020.11.008}, url={http://www.sciencedirect.com/science/article/pii/S016786552030413X}, author={Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka} } CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 @misc{shi2021deep, title={Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities}, author={Xintong Shi and Wenzhi Cao and Sebastian Raschka}, year={2021}, eprint={2111.08851}, archivePrefix={arXiv}, primaryClass={cs.LG} }","title":"Citing"},{"location":"installation/","text":"Installing coral_pytorch Requirements Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0 PyPI You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Installation"},{"location":"installation/#installing-coral_pytorch","text":"","title":"Installing coral_pytorch"},{"location":"installation/#requirements","text":"Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Latest GitHub Source Code"},{"location":"license/","text":"MIT License Copyright (c) 2020 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2020 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/","text":"corn_label_from_logits corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"Corn label from logits"},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/#corn_label_from_logits","text":"corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"corn_label_from_logits"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/","text":"label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"Label to levels"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/","text":"levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"Levels from labelbatch"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/","text":"proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"Proba to label"},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/","text":"CoralLayer CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/#corallayer","text":"CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.losses/coral_loss/","text":"coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"Coral loss"},{"location":"api_modules/coral_pytorch.losses/coral_loss/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"api_modules/coral_pytorch.losses/corn_loss/","text":"corn_loss corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(3.4210, grad_fn=<DivBackward0>)","title":"Corn loss"},{"location":"api_modules/coral_pytorch.losses/corn_loss/#corn_loss","text":"corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(3.4210, grad_fn=<DivBackward0>)","title":"corn_loss"},{"location":"api_subpackages/coral_pytorch.dataset/","text":"coral_pytorch version: 1.2.0dev0 label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.]) proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5]) corn_label_from_logits corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3]) levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"coral_pytorch.dataset"},{"location":"api_subpackages/coral_pytorch.dataset/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_subpackages/coral_pytorch.dataset/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_subpackages/coral_pytorch.dataset/#corn_label_from_logits","text":"corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"corn_label_from_logits"},{"location":"api_subpackages/coral_pytorch.dataset/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_subpackages/coral_pytorch.layers/","text":"coral_pytorch version: 1.2.0dev0 CoralLayer CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"coral_pytorch.layers"},{"location":"api_subpackages/coral_pytorch.layers/#corallayer","text":"CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_subpackages/coral_pytorch.losses/","text":"coral_pytorch version: 1.2.0dev0 corn_loss corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(3.4210, grad_fn=<DivBackward0>) coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_pytorch.losses"},{"location":"api_subpackages/coral_pytorch.losses/#corn_loss","text":"corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(3.4210, grad_fn=<DivBackward0>)","title":"corn_loss"},{"location":"api_subpackages/coral_pytorch.losses/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"tutorials/CORAL_cement/","text":"CORAL MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"CORAL Cement (tabular, MLP)"},{"location":"tutorials/CORAL_cement/#coral-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression.","title":"CORAL MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/CORAL_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/CORAL_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/CORAL_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/CORAL_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/CORAL_cement/#2-equipping-mlp-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with CORAL layer"},{"location":"tutorials/CORAL_cement/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/CORAL_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"4 -- Evaluate model"},{"location":"tutorials/CORAL_mnist/","text":"CORAL CNN for predicting handwritten digits (MNIST) This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = 'data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = 'data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"CORAL MNIST (image, CNN)"},{"location":"tutorials/CORAL_mnist/#coral-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORAL CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/CORAL_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = 'data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = 'data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/CORAL_mnist/#2-equipping-cnn-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with CORAL layer"},{"location":"tutorials/CORAL_mnist/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/CORAL_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/CORAL_poker/","text":"CORAL MLP for predicting poker hands This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CORAL. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\" , header = None ) train_features = train_df . loc [:, 0 : 10 ] train_labels = train_df . loc [:, 10 ] print ( 'Number of features:' , train_features . shape [ 1 ]) print ( 'Number of training examples:' , train_features . shape [ 0 ]) Number of features: 11 Number of training examples: 25010 test_df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\" , header = None ) test_df . head () test_features = test_df . loc [:, 0 : 10 ] test_labels = test_df . loc [:, 10 ] print ( 'Number of test examples:' , test_features . shape [ 0 ]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler () train_features_sc = sc . fit_transform ( train_features ) test_features_sc = sc . transform ( test_features ) Save training and test set as CSV files locally pd . DataFrame ( train_features_sc ) . to_csv ( 'train_features.csv' , index = False ) train_labels . to_csv ( 'train_labels.csv' , index = False ) pd . DataFrame ( test_features_sc ) . to_csv ( 'test_features.csv' , index = False ) test_labels . to_csv ( 'test_labels.csv' , index = False ) # don't need those anymore del test_features del train_features del train_labels del test_labels 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset ( Dataset ): def __init__ ( self , csv_path_features , csv_path_labels , dtype = np . float32 ): self . features = pd . read_csv ( csv_path_features ) . values . astype ( np . float32 ) self . labels = pd . read_csv ( csv_path_labels ) . values . flatten () def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( 'train_features.csv' , 'train_labels.csv' ) test_dataset = MyDataset ( 'test_features.csv' , 'test_labels.csv' ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class CoralMLP ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( CoralMLP , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Linear ( 11 , 5 ), torch . nn . Linear ( 5 , 5 )) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 5 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = CoralMLP ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/196 | Loss: 9.0919 Epoch: 002/020 | Batch 000/196 | Loss: 3.7998 Epoch: 003/020 | Batch 000/196 | Loss: 1.4505 Epoch: 004/020 | Batch 000/196 | Loss: 1.5629 Epoch: 005/020 | Batch 000/196 | Loss: 1.4695 Epoch: 006/020 | Batch 000/196 | Loss: 1.1261 Epoch: 007/020 | Batch 000/196 | Loss: 1.0789 Epoch: 008/020 | Batch 000/196 | Loss: 1.3151 Epoch: 009/020 | Batch 000/196 | Loss: 1.1998 Epoch: 010/020 | Batch 000/196 | Loss: 0.9775 Epoch: 011/020 | Batch 000/196 | Loss: 0.9414 Epoch: 012/020 | Batch 000/196 | Loss: 0.8363 Epoch: 013/020 | Batch 000/196 | Loss: 0.8292 Epoch: 014/020 | Batch 000/196 | Loss: 0.7139 Epoch: 015/020 | Batch 000/196 | Loss: 0.6953 Epoch: 016/020 | Batch 000/196 | Loss: 0.7651 Epoch: 017/020 | Batch 000/196 | Loss: 0.6434 Epoch: 018/020 | Batch 000/196 | Loss: 0.6914 Epoch: 019/020 | Batch 000/196 | Loss: 0.6330 Epoch: 020/020 | Batch 000/196 | Loss: 0.5410 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.10 | 0.10 Mean squared error (train/test): 0.21 | 0.21","title":"CORAL MLP for predicting poker hands"},{"location":"tutorials/CORAL_poker/#coral-mlp-for-predicting-poker-hands","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression.","title":"CORAL MLP for predicting poker hands"},{"location":"tutorials/CORAL_poker/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CORAL. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\" , header = None ) train_features = train_df . loc [:, 0 : 10 ] train_labels = train_df . loc [:, 10 ] print ( 'Number of features:' , train_features . shape [ 1 ]) print ( 'Number of training examples:' , train_features . shape [ 0 ]) Number of features: 11 Number of training examples: 25010 test_df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\" , header = None ) test_df . head () test_features = test_df . loc [:, 0 : 10 ] test_labels = test_df . loc [:, 10 ] print ( 'Number of test examples:' , test_features . shape [ 0 ]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler () train_features_sc = sc . fit_transform ( train_features ) test_features_sc = sc . transform ( test_features ) Save training and test set as CSV files locally pd . DataFrame ( train_features_sc ) . to_csv ( 'train_features.csv' , index = False ) train_labels . to_csv ( 'train_labels.csv' , index = False ) pd . DataFrame ( test_features_sc ) . to_csv ( 'test_features.csv' , index = False ) test_labels . to_csv ( 'test_labels.csv' , index = False ) # don't need those anymore del test_features del train_features del train_labels del test_labels","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/CORAL_poker/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset ( Dataset ): def __init__ ( self , csv_path_features , csv_path_labels , dtype = np . float32 ): self . features = pd . read_csv ( csv_path_features ) . values . astype ( np . float32 ) self . labels = pd . read_csv ( csv_path_labels ) . values . flatten () def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( 'train_features.csv' , 'train_labels.csv' ) test_dataset = MyDataset ( 'test_features.csv' , 'test_labels.csv' ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/CORAL_poker/#2-equipping-mlp-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class CoralMLP ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( CoralMLP , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Linear ( 11 , 5 ), torch . nn . Linear ( 5 , 5 )) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 5 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = CoralMLP ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping MLP with CORAL layer"},{"location":"tutorials/CORAL_poker/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/196 | Loss: 9.0919 Epoch: 002/020 | Batch 000/196 | Loss: 3.7998 Epoch: 003/020 | Batch 000/196 | Loss: 1.4505 Epoch: 004/020 | Batch 000/196 | Loss: 1.5629 Epoch: 005/020 | Batch 000/196 | Loss: 1.4695 Epoch: 006/020 | Batch 000/196 | Loss: 1.1261 Epoch: 007/020 | Batch 000/196 | Loss: 1.0789 Epoch: 008/020 | Batch 000/196 | Loss: 1.3151 Epoch: 009/020 | Batch 000/196 | Loss: 1.1998 Epoch: 010/020 | Batch 000/196 | Loss: 0.9775 Epoch: 011/020 | Batch 000/196 | Loss: 0.9414 Epoch: 012/020 | Batch 000/196 | Loss: 0.8363 Epoch: 013/020 | Batch 000/196 | Loss: 0.8292 Epoch: 014/020 | Batch 000/196 | Loss: 0.7139 Epoch: 015/020 | Batch 000/196 | Loss: 0.6953 Epoch: 016/020 | Batch 000/196 | Loss: 0.7651 Epoch: 017/020 | Batch 000/196 | Loss: 0.6434 Epoch: 018/020 | Batch 000/196 | Loss: 0.6914 Epoch: 019/020 | Batch 000/196 | Loss: 0.6330 Epoch: 020/020 | Batch 000/196 | Loss: 0.5410","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/CORAL_poker/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.10 | 0.10 Mean squared error (train/test): 0.21 | 0.21","title":"4 -- Evaluate model"},{"location":"tutorials/CORN_cement/","text":"CORN MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with a CORN layer In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 42.3084 Epoch: 002/020 | Batch 000/007 | Cost: 33.4198 Epoch: 003/020 | Batch 000/007 | Cost: 34.3413 Epoch: 004/020 | Batch 000/007 | Cost: 27.4008 Epoch: 005/020 | Batch 000/007 | Cost: 29.5580 Epoch: 006/020 | Batch 000/007 | Cost: 28.3949 Epoch: 007/020 | Batch 000/007 | Cost: 26.0713 Epoch: 008/020 | Batch 000/007 | Cost: 24.0429 Epoch: 009/020 | Batch 000/007 | Cost: 22.1783 Epoch: 010/020 | Batch 000/007 | Cost: 25.2757 Epoch: 011/020 | Batch 000/007 | Cost: 22.2279 Epoch: 012/020 | Batch 000/007 | Cost: 23.1534 Epoch: 013/020 | Batch 000/007 | Cost: 18.5136 Epoch: 014/020 | Batch 000/007 | Cost: 23.8390 Epoch: 015/020 | Batch 000/007 | Cost: 18.9016 Epoch: 016/020 | Batch 000/007 | Cost: 18.0890 Epoch: 017/020 | Batch 000/007 | Cost: 13.8526 Epoch: 018/020 | Batch 000/007 | Cost: 17.3017 Epoch: 019/020 | Batch 000/007 | Cost: 15.3039 Epoch: 020/020 | Batch 000/007 | Cost: 16.0646 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.38 Mean squared error (train/test): 0.30 | 0.43 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[9.7585e-01, 9.7536e-01, 9.6610e-01, 2.2675e-01], [9.4921e-01, 1.7135e-01, 9.7073e-03, 3.4552e-04], [2.4214e-02, 4.6617e-04, 3.3125e-05, 2.7729e-05], [9.8303e-01, 5.6925e-01, 3.2523e-01, 5.3245e-02], [9.5153e-01, 5.7437e-01, 1.4063e-01, 1.6096e-02], [1.8260e-02, 7.7530e-06, 5.1410e-06, 5.1152e-06], [9.7835e-01, 9.5596e-01, 3.6877e-01, 1.9158e-02], [9.8692e-01, 9.4746e-01, 3.2161e-01, 4.4179e-02], [3.5250e-01, 6.7358e-03, 2.6506e-03, 2.0687e-03], [9.2053e-01, 3.5487e-01, 1.0667e-01, 5.3238e-02], [1.2923e-01, 2.7282e-03, 3.3910e-04, 2.3258e-04], [9.7515e-01, 8.6582e-01, 2.0421e-01, 4.9532e-02], [9.9591e-01, 4.9053e-01, 3.7695e-02, 1.0938e-02], [1.8391e-01, 5.2724e-03, 5.6778e-04, 3.7333e-04], [8.8100e-01, 7.3442e-01, 5.6249e-01, 1.2397e-01], [6.7335e-01, 5.0340e-02, 8.1046e-03, 5.6927e-03], [9.0837e-01, 2.8030e-01, 3.3884e-02, 7.7040e-03], [8.9811e-01, 2.3693e-01, 2.2545e-02, 3.1750e-03], [9.5224e-01, 7.0664e-01, 1.0633e-01, 5.8928e-02], [7.1353e-01, 1.1418e-02, 4.1087e-04, 2.9408e-04], [4.8610e-04, 3.1541e-05, 2.3382e-05, 2.1088e-05], [9.6261e-01, 7.2110e-01, 7.1777e-01, 1.7590e-01], [9.7763e-01, 2.6866e-01, 4.9495e-02, 2.0072e-02], [9.7093e-01, 9.4531e-01, 9.1623e-02, 4.7309e-02], [8.3720e-01, 7.3658e-02, 1.1421e-02, 7.7481e-03], [9.2363e-01, 9.0403e-01, 5.8055e-01, 7.3952e-03], [9.7614e-01, 9.7225e-01, 6.8393e-01, 2.5626e-02], [9.7249e-01, 4.3916e-01, 2.6588e-01, 1.7958e-01], [9.7547e-01, 9.5796e-01, 8.4703e-01, 7.7577e-01], [9.5975e-01, 9.5148e-01, 4.9699e-01, 6.1305e-02]])","title":"CORN Cement (tabular, MLP)"},{"location":"tutorials/CORN_cement/#corn-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression.","title":"CORN MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/CORN_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/CORN_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/CORN_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/CORN_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/CORN_cement/#2-equipping-mlp-with-a-corn-layer","text":"In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with a CORN layer"},{"location":"tutorials/CORN_cement/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 42.3084 Epoch: 002/020 | Batch 000/007 | Cost: 33.4198 Epoch: 003/020 | Batch 000/007 | Cost: 34.3413 Epoch: 004/020 | Batch 000/007 | Cost: 27.4008 Epoch: 005/020 | Batch 000/007 | Cost: 29.5580 Epoch: 006/020 | Batch 000/007 | Cost: 28.3949 Epoch: 007/020 | Batch 000/007 | Cost: 26.0713 Epoch: 008/020 | Batch 000/007 | Cost: 24.0429 Epoch: 009/020 | Batch 000/007 | Cost: 22.1783 Epoch: 010/020 | Batch 000/007 | Cost: 25.2757 Epoch: 011/020 | Batch 000/007 | Cost: 22.2279 Epoch: 012/020 | Batch 000/007 | Cost: 23.1534 Epoch: 013/020 | Batch 000/007 | Cost: 18.5136 Epoch: 014/020 | Batch 000/007 | Cost: 23.8390 Epoch: 015/020 | Batch 000/007 | Cost: 18.9016 Epoch: 016/020 | Batch 000/007 | Cost: 18.0890 Epoch: 017/020 | Batch 000/007 | Cost: 13.8526 Epoch: 018/020 | Batch 000/007 | Cost: 17.3017 Epoch: 019/020 | Batch 000/007 | Cost: 15.3039 Epoch: 020/020 | Batch 000/007 | Cost: 16.0646","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/CORN_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.38 Mean squared error (train/test): 0.30 | 0.43 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/CORN_cement/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[9.7585e-01, 9.7536e-01, 9.6610e-01, 2.2675e-01], [9.4921e-01, 1.7135e-01, 9.7073e-03, 3.4552e-04], [2.4214e-02, 4.6617e-04, 3.3125e-05, 2.7729e-05], [9.8303e-01, 5.6925e-01, 3.2523e-01, 5.3245e-02], [9.5153e-01, 5.7437e-01, 1.4063e-01, 1.6096e-02], [1.8260e-02, 7.7530e-06, 5.1410e-06, 5.1152e-06], [9.7835e-01, 9.5596e-01, 3.6877e-01, 1.9158e-02], [9.8692e-01, 9.4746e-01, 3.2161e-01, 4.4179e-02], [3.5250e-01, 6.7358e-03, 2.6506e-03, 2.0687e-03], [9.2053e-01, 3.5487e-01, 1.0667e-01, 5.3238e-02], [1.2923e-01, 2.7282e-03, 3.3910e-04, 2.3258e-04], [9.7515e-01, 8.6582e-01, 2.0421e-01, 4.9532e-02], [9.9591e-01, 4.9053e-01, 3.7695e-02, 1.0938e-02], [1.8391e-01, 5.2724e-03, 5.6778e-04, 3.7333e-04], [8.8100e-01, 7.3442e-01, 5.6249e-01, 1.2397e-01], [6.7335e-01, 5.0340e-02, 8.1046e-03, 5.6927e-03], [9.0837e-01, 2.8030e-01, 3.3884e-02, 7.7040e-03], [8.9811e-01, 2.3693e-01, 2.2545e-02, 3.1750e-03], [9.5224e-01, 7.0664e-01, 1.0633e-01, 5.8928e-02], [7.1353e-01, 1.1418e-02, 4.1087e-04, 2.9408e-04], [4.8610e-04, 3.1541e-05, 2.3382e-05, 2.1088e-05], [9.6261e-01, 7.2110e-01, 7.1777e-01, 1.7590e-01], [9.7763e-01, 2.6866e-01, 4.9495e-02, 2.0072e-02], [9.7093e-01, 9.4531e-01, 9.1623e-02, 4.7309e-02], [8.3720e-01, 7.3658e-02, 1.1421e-02, 7.7481e-03], [9.2363e-01, 9.0403e-01, 5.8055e-01, 7.3952e-03], [9.7614e-01, 9.7225e-01, 6.8393e-01, 2.5626e-02], [9.7249e-01, 4.3916e-01, 2.6588e-01, 1.7958e-01], [9.7547e-01, 9.5796e-01, 8.4703e-01, 7.7577e-01], [9.5975e-01, 9.5148e-01, 4.9699e-01, 6.1305e-02]])","title":"5 -- Rank probabilities from logits"},{"location":"tutorials/CORN_mnist/","text":"CORN CNN for predicting handwritten digits (MNIST) This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = 'data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = 'data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with a CORN layer In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 50.5479 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Cost: 9.7984 Epoch: 001/010 | Batch 400/468 | Cost: 5.6653 Epoch: 002/010 | Batch 000/468 | Cost: 6.6017 Epoch: 002/010 | Batch 200/468 | Cost: 4.7958 Epoch: 002/010 | Batch 400/468 | Cost: 4.7984 Epoch: 003/010 | Batch 000/468 | Cost: 3.9449 Epoch: 003/010 | Batch 200/468 | Cost: 3.6385 Epoch: 003/010 | Batch 400/468 | Cost: 2.8829 Epoch: 004/010 | Batch 000/468 | Cost: 2.0917 Epoch: 004/010 | Batch 200/468 | Cost: 2.8083 Epoch: 004/010 | Batch 400/468 | Cost: 2.6029 Epoch: 005/010 | Batch 000/468 | Cost: 3.0181 Epoch: 005/010 | Batch 200/468 | Cost: 2.5722 Epoch: 005/010 | Batch 400/468 | Cost: 1.0547 Epoch: 006/010 | Batch 000/468 | Cost: 1.8847 Epoch: 006/010 | Batch 200/468 | Cost: 1.8378 Epoch: 006/010 | Batch 400/468 | Cost: 2.7391 Epoch: 007/010 | Batch 000/468 | Cost: 4.4030 Epoch: 007/010 | Batch 200/468 | Cost: 1.7034 Epoch: 007/010 | Batch 400/468 | Cost: 1.4372 Epoch: 008/010 | Batch 000/468 | Cost: 2.5416 Epoch: 008/010 | Batch 200/468 | Cost: 2.0749 Epoch: 008/010 | Batch 400/468 | Cost: 2.3005 Epoch: 009/010 | Batch 000/468 | Cost: 1.7815 Epoch: 009/010 | Batch 200/468 | Cost: 3.4259 Epoch: 009/010 | Batch 400/468 | Cost: 1.8984 Epoch: 010/010 | Batch 000/468 | Cost: 1.4577 Epoch: 010/010 | Batch 200/468 | Cost: 2.1422 Epoch: 010/010 | Batch 400/468 | Cost: 2.0863 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.37 | 3.35 Mean squared error (train/test): 17.28 | 16.98 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9987e-01, 9.9947e-01, 2.6341e-08], [1.0000e+00, 1.0000e+00, 9.9275e-01, ..., 9.8443e-01, 9.8443e-01, 9.2676e-08], [9.1224e-01, 9.1224e-01, 9.1224e-01, ..., 8.5583e-01, 8.5442e-01, 1.7306e-03], ..., [9.9801e-01, 9.9800e-01, 9.9800e-01, ..., 9.8942e-01, 9.8922e-01, 4.1247e-03], [9.9977e-01, 9.9977e-01, 9.9977e-01, ..., 1.5548e-02, 1.5543e-02, 2.8278e-04], [7.4167e-07, 7.4167e-07, 7.2308e-07, ..., 7.4769e-08, 7.4750e-08, 5.6809e-13]])","title":"CORN MNIST (image, CNN)"},{"location":"tutorials/CORN_mnist/#corn-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORN CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/CORN_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = 'data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = 'data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/CORN_mnist/#2-equipping-cnn-with-a-corn-layer","text":"In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with a CORN layer"},{"location":"tutorials/CORN_mnist/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 50.5479 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Cost: 9.7984 Epoch: 001/010 | Batch 400/468 | Cost: 5.6653 Epoch: 002/010 | Batch 000/468 | Cost: 6.6017 Epoch: 002/010 | Batch 200/468 | Cost: 4.7958 Epoch: 002/010 | Batch 400/468 | Cost: 4.7984 Epoch: 003/010 | Batch 000/468 | Cost: 3.9449 Epoch: 003/010 | Batch 200/468 | Cost: 3.6385 Epoch: 003/010 | Batch 400/468 | Cost: 2.8829 Epoch: 004/010 | Batch 000/468 | Cost: 2.0917 Epoch: 004/010 | Batch 200/468 | Cost: 2.8083 Epoch: 004/010 | Batch 400/468 | Cost: 2.6029 Epoch: 005/010 | Batch 000/468 | Cost: 3.0181 Epoch: 005/010 | Batch 200/468 | Cost: 2.5722 Epoch: 005/010 | Batch 400/468 | Cost: 1.0547 Epoch: 006/010 | Batch 000/468 | Cost: 1.8847 Epoch: 006/010 | Batch 200/468 | Cost: 1.8378 Epoch: 006/010 | Batch 400/468 | Cost: 2.7391 Epoch: 007/010 | Batch 000/468 | Cost: 4.4030 Epoch: 007/010 | Batch 200/468 | Cost: 1.7034 Epoch: 007/010 | Batch 400/468 | Cost: 1.4372 Epoch: 008/010 | Batch 000/468 | Cost: 2.5416 Epoch: 008/010 | Batch 200/468 | Cost: 2.0749 Epoch: 008/010 | Batch 400/468 | Cost: 2.3005 Epoch: 009/010 | Batch 000/468 | Cost: 1.7815 Epoch: 009/010 | Batch 200/468 | Cost: 3.4259 Epoch: 009/010 | Batch 400/468 | Cost: 1.8984 Epoch: 010/010 | Batch 000/468 | Cost: 1.4577 Epoch: 010/010 | Batch 200/468 | Cost: 2.1422 Epoch: 010/010 | Batch 400/468 | Cost: 2.0863","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/CORN_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.37 | 3.35 Mean squared error (train/test): 17.28 | 16.98 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/CORN_mnist/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9987e-01, 9.9947e-01, 2.6341e-08], [1.0000e+00, 1.0000e+00, 9.9275e-01, ..., 9.8443e-01, 9.8443e-01, 9.2676e-08], [9.1224e-01, 9.1224e-01, 9.1224e-01, ..., 8.5583e-01, 8.5442e-01, 1.7306e-03], ..., [9.9801e-01, 9.9800e-01, 9.9800e-01, ..., 9.8942e-01, 9.8922e-01, 4.1247e-03], [9.9977e-01, 9.9977e-01, 9.9977e-01, ..., 1.5548e-02, 1.5543e-02, 2.8278e-04], [7.4167e-07, 7.4167e-07, 7.2308e-07, ..., 7.4769e-08, 7.4750e-08, 5.6809e-13]])","title":"5 -- Rank probabilities from logits"}]}