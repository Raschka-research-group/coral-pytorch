{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CORAL & CORN implementations for ordinal regression with deep neural networks. About CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet References CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"Home"},{"location":"#about","text":"CORAL (COnsistent RAnk Logits) and CORN (Conditional Ordinal Regression for Neural networks) are methods for ordinal regression with deep neural networks, which address the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL and CORN functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" that can be found on the documentation website at https://Raschka-research-group.github.io/coral_pytorch . If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: CORAL: https://github.com/Raschka-research-group/coral-cnn . CORN: https://github.com/Raschka-research-group/corn-ordinal-neuralnet","title":"About"},{"location":"#references","text":"CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"References"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md. 1.4.0 (07-17-2022) Downloads Source code (zip) Source code (tar.gz) New Features Adds object-oriented versions of the losses: coral_pytorch.losses.CoralLoss and coral_pytorch.losses.CornLoss . Changes Bug Fixes - 1.3.0 (07-16-2022) Downloads Source code (zip) Source code (tar.gz) New Features - Changes - Bug Fixes Fixes a bug where the normalization of the corn_loss different from the one proposed in the original paper. ( #22 ) 1.2.0 (11-17-2021) Downloads Source code (zip) Source code (tar.gz) New Features Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \" Changes Bug Fixes - 1.1.0 (04/08/2021) Downloads Source code (zip) Source code (tar.gz) New Features - Changes By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 ) Bug Fixes - 1.0.0 (11/15/2020) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Changelog"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at [https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/raschka-research-group/coral_pytorch/blob/main/docs/CHANGELOG.md.","title":"Release Notes"},{"location":"CHANGELOG/#140-07-17-2022","text":"","title":"1.4.0 (07-17-2022)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"Adds object-oriented versions of the losses: coral_pytorch.losses.CoralLoss and coral_pytorch.losses.CornLoss .","title":"New Features"},{"location":"CHANGELOG/#changes","text":"","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#130-07-16-2022","text":"","title":"1.3.0 (07-16-2022)"},{"location":"CHANGELOG/#downloads_1","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_1","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes_1","text":"-","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"Fixes a bug where the normalization of the corn_loss different from the one proposed in the original paper. ( #22 )","title":"Bug Fixes"},{"location":"CHANGELOG/#120-11-17-2021","text":"","title":"1.2.0 (11-17-2021)"},{"location":"CHANGELOG/#downloads_2","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_2","text":"Add CORN loss corresponding to the manuscript, \" Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities \"","title":"New Features"},{"location":"CHANGELOG/#changes_2","text":"","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_2","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#110-04082021","text":"","title":"1.1.0 (04/08/2021)"},{"location":"CHANGELOG/#downloads_3","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_3","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes_3","text":"By default, bias units are now preinitialized to descending values in [0, 1] range (instead of all zero values), which results in faster training and better generalization performance. (PR #5 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_3","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#100-11152020","text":"","title":"1.0.0 (11/15/2020)"},{"location":"CHANGELOG/#downloads_4","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_4","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes_4","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_4","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use CORAL or CORN as part of your workflow in a scientific publication, please consider citing the corresponding paper: CORAL Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title={Rank consistent ordinal regression for neural networks with application to age estimation}, journal={Pattern Recognition Letters}, volume={140}, pages={325-331}, year={2020}, issn={0167-8655}, doi={https://doi.org/10.1016/j.patrec.2020.11.008}, url={http://www.sciencedirect.com/science/article/pii/S016786552030413X}, author={Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka} } CORN Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 @misc{shi2021deep, title={Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities}, author={Xintong Shi and Wenzhi Cao and Sebastian Raschka}, year={2021}, eprint={2111.08851}, archivePrefix={arXiv}, primaryClass={cs.LG} }","title":"Citing"},{"location":"installation/","text":"Installing coral_pytorch Requirements Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0 PyPI You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Installation"},{"location":"installation/#installing-coral_pytorch","text":"","title":"Installing coral_pytorch"},{"location":"installation/#requirements","text":"Coral-pytorch requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Latest GitHub Source Code"},{"location":"license/","text":"MIT License Copyright (c) 2020-2022 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2020-2022 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/","text":"corn_label_from_logits corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"Corn label from logits"},{"location":"api_modules/coral_pytorch.dataset/corn_label_from_logits/#corn_label_from_logits","text":"corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"corn_label_from_logits"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/","text":"label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"Label to levels"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/","text":"levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"Levels from labelbatch"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/","text":"proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"Proba to label"},{"location":"api_modules/coral_pytorch.dataset/proba_to_label/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/","text":"CoralLayer CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/#corallayer","text":"CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/","text":"CoralLoss CoralLoss(reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Examples >>> import torch >>> from coral_pytorch.losses import CoralLoss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> loss = CoralLoss() >>> loss(logits, levels) tensor(0.6920) Methods add_module(name: str, module: Optional[ForwardRef('Module')]) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: ``` >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) ``` bfloat16(self: ~T) -> ~T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Module: self buffers(recurse: bool = True) -> Iterator[torch.Tensor] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: ``` >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` children() -> Iterator[ForwardRef('Module')] Returns an iterator over immediate children modules. Yields: Module: a child module cpu(self: ~T) -> ~T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self double(self: ~T) -> ~T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Module: self eval(self: ~T) -> ~T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self extra_repr() -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float(self: ~T) -> ~T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Module: self forward(logits, levels, importance_weights=None) Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) get_buffer(target: str) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer get_extra_state() -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict get_parameter(target: str) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` get_submodule(target: str) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` half(self: ~T) -> ~T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Module: self load_state_dict(state_dict: 'OrderedDict[str, Tensor]', strict: bool = True) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . modules() -> Iterator[ForwardRef('Module')] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) ``` named_buffers(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: ``` >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) ``` named_children() -> Iterator[Tuple[str, ForwardRef('Module')]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: ``` >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) ``` named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) ``` named_parameters(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: ``` >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) ``` parameters(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: ``` >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: ``` >>> self.register_buffer('running_mean', torch.zeros(num_features)) ``` register_forward_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_forward_pre_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If None , the parameter is not included in the module's :attr: state_dict . requires_grad_(self: ~T, requires_grad: bool = True) -> ~T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self set_extra_state(state: Any) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Args: state (dict): Extra state from the `state_dict` share_memory(self: ~T) -> ~T See :meth: torch.Tensor.share_memory_ state_dict(destination=None, prefix='', keep_vars=False) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: ``` >>> module.state_dict().keys() ['bias', 'weight'] ``` to( args, * kwargs) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: ``` >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) ``` to_empty(self: ~T, , device: Union[str, torch.device]) -> ~T* Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self train(self: ~T, mode: bool = True) -> ~T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self zero_grad(set_to_none: bool = False) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. Properties","title":"CoralLoss"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#coralloss","text":"CoralLoss(reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Examples >>> import torch >>> from coral_pytorch.losses import CoralLoss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> loss = CoralLoss() >>> loss(logits, levels) tensor(0.6920)","title":"CoralLoss"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#methods","text":"add_module(name: str, module: Optional[ForwardRef('Module')]) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: ``` >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) ``` bfloat16(self: ~T) -> ~T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Module: self buffers(recurse: bool = True) -> Iterator[torch.Tensor] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: ``` >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` children() -> Iterator[ForwardRef('Module')] Returns an iterator over immediate children modules. Yields: Module: a child module cpu(self: ~T) -> ~T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self double(self: ~T) -> ~T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Module: self eval(self: ~T) -> ~T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self extra_repr() -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float(self: ~T) -> ~T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Module: self forward(logits, levels, importance_weights=None) Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) get_buffer(target: str) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer get_extra_state() -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict get_parameter(target: str) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` get_submodule(target: str) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` half(self: ~T) -> ~T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Module: self load_state_dict(state_dict: 'OrderedDict[str, Tensor]', strict: bool = True) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . modules() -> Iterator[ForwardRef('Module')] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) ``` named_buffers(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: ``` >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) ``` named_children() -> Iterator[Tuple[str, ForwardRef('Module')]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: ``` >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) ``` named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) ``` named_parameters(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: ``` >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) ``` parameters(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: ``` >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: ``` >>> self.register_buffer('running_mean', torch.zeros(num_features)) ``` register_forward_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_forward_pre_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If None , the parameter is not included in the module's :attr: state_dict . requires_grad_(self: ~T, requires_grad: bool = True) -> ~T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self set_extra_state(state: Any) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Args: state (dict): Extra state from the `state_dict` share_memory(self: ~T) -> ~T See :meth: torch.Tensor.share_memory_ state_dict(destination=None, prefix='', keep_vars=False) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: ``` >>> module.state_dict().keys() ['bias', 'weight'] ``` to( args, * kwargs) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: ``` >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) ``` to_empty(self: ~T, , device: Union[str, torch.device]) -> ~T* Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self train(self: ~T, mode: bool = True) -> ~T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self zero_grad(set_to_none: bool = False) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details.","title":"Methods"},{"location":"api_modules/coral_pytorch.losses/CoralLoss/#properties","text":"","title":"Properties"},{"location":"api_modules/coral_pytorch.losses/CornLoss/","text":"CornLoss CornLoss(num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters num_classes : int Number of unique class labels (class labels should start at 0). Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>) Methods add_module(name: str, module: Optional[ForwardRef('Module')]) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: ``` >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) ``` bfloat16(self: ~T) -> ~T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Module: self buffers(recurse: bool = True) -> Iterator[torch.Tensor] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: ``` >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` children() -> Iterator[ForwardRef('Module')] Returns an iterator over immediate children modules. Yields: Module: a child module cpu(self: ~T) -> ~T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self double(self: ~T) -> ~T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Module: self eval(self: ~T) -> ~T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self extra_repr() -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float(self: ~T) -> ~T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Module: self forward(logits, y_train) Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> loss = CornLoss(num_classes=NUM_CLASSES) >>> loss(logits, y_train) tensor(0.7127, grad_fn=<DivBackward0>) get_buffer(target: str) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer get_extra_state() -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict get_parameter(target: str) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` get_submodule(target: str) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` half(self: ~T) -> ~T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Module: self load_state_dict(state_dict: 'OrderedDict[str, Tensor]', strict: bool = True) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . modules() -> Iterator[ForwardRef('Module')] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) ``` named_buffers(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: ``` >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) ``` named_children() -> Iterator[Tuple[str, ForwardRef('Module')]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: ``` >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) ``` named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) ``` named_parameters(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: ``` >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) ``` parameters(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: ``` >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: ``` >>> self.register_buffer('running_mean', torch.zeros(num_features)) ``` register_forward_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_forward_pre_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If None , the parameter is not included in the module's :attr: state_dict . requires_grad_(self: ~T, requires_grad: bool = True) -> ~T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self set_extra_state(state: Any) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Args: state (dict): Extra state from the `state_dict` share_memory(self: ~T) -> ~T See :meth: torch.Tensor.share_memory_ state_dict(destination=None, prefix='', keep_vars=False) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: ``` >>> module.state_dict().keys() ['bias', 'weight'] ``` to( args, * kwargs) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: ``` >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) ``` to_empty(self: ~T, , device: Union[str, torch.device]) -> ~T* Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self train(self: ~T, mode: bool = True) -> ~T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self zero_grad(set_to_none: bool = False) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details. Properties","title":"CornLoss"},{"location":"api_modules/coral_pytorch.losses/CornLoss/#cornloss","text":"CornLoss(num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters num_classes : int Number of unique class labels (class labels should start at 0). Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>)","title":"CornLoss"},{"location":"api_modules/coral_pytorch.losses/CornLoss/#methods","text":"add_module(name: str, module: Optional[ForwardRef('Module')]) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: ``` >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) ``` bfloat16(self: ~T) -> ~T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Module: self buffers(recurse: bool = True) -> Iterator[torch.Tensor] Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: ``` >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` children() -> Iterator[ForwardRef('Module')] Returns an iterator over immediate children modules. Yields: Module: a child module cpu(self: ~T) -> ~T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Module: self cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self double(self: ~T) -> ~T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Module: self eval(self: ~T) -> ~T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`. See :ref:`locally-disable-grad-doc` for a comparison between `.eval()` and several similar mechanisms that may be confused with it. Returns: Module: self extra_repr() -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float(self: ~T) -> ~T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Module: self forward(logits, y_train) Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> loss = CornLoss(num_classes=NUM_CLASSES) >>> loss(logits, y_train) tensor(0.7127, grad_fn=<DivBackward0>) get_buffer(target: str) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer get_extra_state() -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict get_parameter(target: str) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the Parameter to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.nn.Parameter: The Parameter referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Parameter`` get_submodule(target: str) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an ``nn.Module`` ``A`` that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested submodule ``net_b``, which itself has two submodules ``net_c`` and ``linear``. ``net_c`` then has a submodule ``conv``.) To check whether or not we have the ``linear`` submodule, we would call ``get_submodule(\"net_b.linear\")``. To check whether we have the ``conv`` submodule, we would call ``get_submodule(\"net_b.net_c.conv\")``. The runtime of ``get_submodule`` is bounded by the degree of module nesting in ``target``. A query against ``named_modules`` achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, ``get_submodule`` should always be used. Args: target: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) Returns: torch.nn.Module: The submodule referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not an ``nn.Module`` half(self: ~T) -> ~T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Module: self load_state_dict(state_dict: 'OrderedDict[str, Tensor]', strict: bool = True) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr:`state_dict` match the keys returned by this module's :meth:`~torch.nn.Module.state_dict` function. Default: ``True`` Returns: ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields: missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . modules() -> Iterator[ForwardRef('Module')] Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) ``` named_buffers(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: ``` >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) ``` named_children() -> Iterator[Tuple[str, ForwardRef('Module')]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: ``` >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) ``` named_modules(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, ``l`` will be returned only once. Example:: ``` >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) ``` named_parameters(prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: ``` >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) ``` parameters(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter] Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: ``` >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) ``` register_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: ``` >>> self.register_buffer('running_mean', torch.zeros(num_features)) ``` register_forward_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func:`forward` has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:`forward` is called. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_forward_pre_hook(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func:`forward` is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the ``forward``. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_full_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()`` register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If None , the parameter is not included in the module's :attr: state_dict . requires_grad_(self: ~T, requires_grad: bool = True) -> ~T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr:`requires_grad` attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref:`locally-disable-grad-doc` for a comparison between `.requires_grad_()` and several similar mechanisms that may be confused with it. Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: ``True``. Returns: Module: self set_extra_state(state: Any) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding :func: get_extra_state for your module if you need to store extra state within its state_dict . Args: state (dict): Extra state from the `state_dict` share_memory(self: ~T) -> ~T See :meth: torch.Tensor.share_memory_ state_dict(destination=None, prefix='', keep_vars=False) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. Returns: dict: a dictionary containing a whole state of the module Example:: ``` >>> module.state_dict().keys() ['bias', 'weight'] ``` to( args, * kwargs) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: ``` >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) ``` to_empty(self: ~T, , device: Union[str, torch.device]) -> ~T* Moves the parameters and buffers to the specified device without copying storage. Args: device (:class:`torch.device`): The desired device of the parameters and buffers in this module. Returns: Module: self train(self: ~T, mode: bool = True) -> ~T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`, etc. Args: mode (bool): whether to set training mode (``True``) or evaluation mode (``False``). Default: ``True``. Returns: Module: self type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Args: dst_type (type or string): the desired type Returns: Module: self xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self zero_grad(set_to_none: bool = False) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details.","title":"Methods"},{"location":"api_modules/coral_pytorch.losses/CornLoss/#properties","text":"","title":"Properties"},{"location":"api_modules/coral_pytorch.losses/coral_loss/","text":"coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> from coral_pytorch.losses import coral_loss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"Coral loss"},{"location":"api_modules/coral_pytorch.losses/coral_loss/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> from coral_pytorch.losses import coral_loss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"api_modules/coral_pytorch.losses/corn_loss/","text":"corn_loss corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>)","title":"Corn loss"},{"location":"api_modules/coral_pytorch.losses/corn_loss/#corn_loss","text":"corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>)","title":"corn_loss"},{"location":"api_subpackages/coral_pytorch.dataset/","text":"coral_pytorch version: 1.4.0 label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.]) proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5]) corn_label_from_logits corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3]) levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"coral_pytorch.dataset"},{"location":"api_subpackages/coral_pytorch.dataset/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_subpackages/coral_pytorch.dataset/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_subpackages/coral_pytorch.dataset/#corn_label_from_logits","text":"corn_label_from_logits(logits) Returns the predicted rank label from logits for a network trained via the CORN loss. Parameters logits : torch.tensor, shape=(n_examples, n_classes) Torch tensor consisting of logits returned by the neural net. Returns labels : torch.tensor, shape=(n_examples) Integer tensor containing the predicted rank (class) labels Examples >>> # 2 training examples, 5 classes >>> logits = torch.tensor([[14.152, -6.1942, 0.47710, 0.96850], ... [65.667, 0.303, 11.500, -4.524]]) >>> corn_label_from_logits(logits) tensor([1, 3])","title":"corn_label_from_logits"},{"location":"api_subpackages/coral_pytorch.dataset/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_subpackages/coral_pytorch.layers/","text":"coral_pytorch version: 1.4.0 CoralLayer CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"coral_pytorch.layers"},{"location":"api_subpackages/coral_pytorch.layers/#corallayer","text":"CoralLayer(size_in, num_classes, preinit_bias=True) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset. preinit_bias : bool (default=True) If true, it will pre-initialize the biases to descending values in [0, 1] range instead of initializing it to all zeros. This pre- initialization scheme results in faster learning and better generalization performance in practice.","title":"CoralLayer"},{"location":"api_subpackages/coral_pytorch.losses/","text":"coral_pytorch version: 1.4.0 CornLoss CornLoss(num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters num_classes : int Number of unique class labels (class labels should start at 0). Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>) Methods forward(logits, y_train) Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> loss = CornLoss(num_classes=NUM_CLASSES) >>> loss(logits, y_train) tensor(0.7127, grad_fn=<DivBackward0>) CoralLoss CoralLoss(reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Examples >>> import torch >>> from coral_pytorch.losses import CoralLoss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> loss = CoralLoss() >>> loss(logits, levels) tensor(0.6920) Methods forward(logits, levels, importance_weights=None) Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. corn_loss corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>) coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> from coral_pytorch.losses import coral_loss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_pytorch.losses"},{"location":"api_subpackages/coral_pytorch.losses/#cornloss","text":"CornLoss(num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters num_classes : int Number of unique class labels (class labels should start at 0). Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>)","title":"CornLoss"},{"location":"api_subpackages/coral_pytorch.losses/#methods","text":"forward(logits, y_train) Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> loss = CornLoss(num_classes=NUM_CLASSES) >>> loss(logits, y_train) tensor(0.7127, grad_fn=<DivBackward0>)","title":"Methods"},{"location":"api_subpackages/coral_pytorch.losses/#coralloss","text":"CoralLoss(reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Examples >>> import torch >>> from coral_pytorch.losses import CoralLoss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> loss = CoralLoss() >>> loss(logits, levels) tensor(0.6920)","title":"CoralLoss"},{"location":"api_subpackages/coral_pytorch.losses/#methods_1","text":"forward(logits, levels, importance_weights=None) Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None.","title":"Methods"},{"location":"api_subpackages/coral_pytorch.losses/#corn_loss","text":"corn_loss(logits, y_train, num_classes) Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal Regression based on Conditional Probabilities' manuscript. Parameters logits : torch.tensor, shape=(num_examples, num_classes-1) Outputs of the CORN layer. y_train : torch.tensor, shape=(num_examples) Torch tensor containing the class labels. num_classes : int Number of unique class labels (class labels should start at 0). Returns loss : torch.tensor A torch.tensor containing a single loss value. Examples >>> import torch >>> from coral_pytorch.losses import corn_loss >>> # Consider 8 training examples >>> _ = torch.manual_seed(123) >>> X_train = torch.rand(8, 99) >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4]) >>> NUM_CLASSES = 5 >>> # >>> # >>> # def __init__(self): >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1) >>> # >>> # >>> # def forward(self, X_train): >>> logits = corn_net(X_train) >>> logits.shape torch.Size([8, 4]) >>> corn_loss(logits, y_train, NUM_CLASSES) tensor(0.7127, grad_fn=<DivBackward0>)","title":"corn_loss"},{"location":"api_subpackages/coral_pytorch.losses/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> from coral_pytorch.losses import coral_loss >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"tutorials/pure_pytorch/CORAL_cement/","text":"CORAL MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"CORAL MLP model for tabular data (Cement dataset)"},{"location":"tutorials/pure_pytorch/CORAL_cement/#coral-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORAL layer and loss function for ordinal regression.","title":"CORAL MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/pure_pytorch/CORAL_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/pure_pytorch/CORAL_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/pure_pytorch/CORAL_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/pure_pytorch/CORAL_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cpu from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORAL_cement/#2-equipping-mlp-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ) ### Specify CORAL layer self . fc = CoralLayer ( size_in = num_hidden_2 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . my_network ( x ) ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with CORAL layer"},{"location":"tutorials/pure_pytorch/CORAL_cement/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Loss: 1.0222 Epoch: 002/020 | Batch 000/007 | Loss: 1.1131 Epoch: 003/020 | Batch 000/007 | Loss: 0.9594 Epoch: 004/020 | Batch 000/007 | Loss: 0.9661 Epoch: 005/020 | Batch 000/007 | Loss: 0.9792 Epoch: 006/020 | Batch 000/007 | Loss: 1.0311 Epoch: 007/020 | Batch 000/007 | Loss: 0.9157 Epoch: 008/020 | Batch 000/007 | Loss: 0.8542 Epoch: 009/020 | Batch 000/007 | Loss: 0.9652 Epoch: 010/020 | Batch 000/007 | Loss: 0.9483 Epoch: 011/020 | Batch 000/007 | Loss: 0.8316 Epoch: 012/020 | Batch 000/007 | Loss: 0.9067 Epoch: 013/020 | Batch 000/007 | Loss: 1.0139 Epoch: 014/020 | Batch 000/007 | Loss: 0.8505 Epoch: 015/020 | Batch 000/007 | Loss: 0.8289 Epoch: 016/020 | Batch 000/007 | Loss: 0.8277 Epoch: 017/020 | Batch 000/007 | Loss: 0.7669 Epoch: 018/020 | Batch 000/007 | Loss: 0.8366 Epoch: 019/020 | Batch 000/007 | Loss: 0.7514 Epoch: 020/020 | Batch 000/007 | Loss: 0.8221 from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/pure_pytorch/CORAL_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.27 | 0.34 Mean squared error (train/test): 0.28 | 0.34","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORAL_mnist/","text":"CORAL CNN for predicting handwritten digits (MNIST) This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"CORAL CNN model for image data (MNIST dataset)"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#coral-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORAL CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#2-equipping-cnn-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) This is because the convolutional and pooling layers torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self . fc ( x ) probas = torch . sigmoid ( logits ) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORAL layer self . fc = CoralLayer ( size_in = 294 , num_classes = num_classes ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORAL layer ##### logits = self . fc ( x ) probas = torch . sigmoid ( logits ) ###--------------------------------------------------------------------### return logits , probas torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with CORAL layer"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) 2) Apply the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): ##### Convert class labels for CORAL levels = levels_from_labelbatch ( class_labels , num_classes = NUM_CLASSES ) ###--------------------------------------------------------------------### features = features . to ( DEVICE ) levels = levels . to ( DEVICE ) logits , probas = model ( features ) #### CORAL loss loss = coral_loss ( logits , levels ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Loss: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Loss: 5.9835 /Users/sebastian/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-gqmopi53/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 200/468 | Loss: 4.2022 Epoch: 001/010 | Batch 400/468 | Loss: 3.6785 Epoch: 002/010 | Batch 000/468 | Loss: 3.5811 Epoch: 002/010 | Batch 200/468 | Loss: 3.0574 Epoch: 002/010 | Batch 400/468 | Loss: 3.3966 Epoch: 003/010 | Batch 000/468 | Loss: 2.9386 Epoch: 003/010 | Batch 200/468 | Loss: 2.9354 Epoch: 003/010 | Batch 400/468 | Loss: 3.0238 Epoch: 004/010 | Batch 000/468 | Loss: 2.7420 Epoch: 004/010 | Batch 200/468 | Loss: 2.5817 Epoch: 004/010 | Batch 400/468 | Loss: 2.5847 Epoch: 005/010 | Batch 000/468 | Loss: 2.6086 Epoch: 005/010 | Batch 200/468 | Loss: 2.4370 Epoch: 005/010 | Batch 400/468 | Loss: 2.4903 Epoch: 006/010 | Batch 000/468 | Loss: 2.3428 Epoch: 006/010 | Batch 200/468 | Loss: 2.4846 Epoch: 006/010 | Batch 400/468 | Loss: 2.3392 Epoch: 007/010 | Batch 000/468 | Loss: 2.4983 Epoch: 007/010 | Batch 200/468 | Loss: 2.4828 Epoch: 007/010 | Batch 400/468 | Loss: 2.2048 Epoch: 008/010 | Batch 000/468 | Loss: 2.3902 Epoch: 008/010 | Batch 200/468 | Loss: 2.2189 Epoch: 008/010 | Batch 400/468 | Loss: 2.1895 Epoch: 009/010 | Batch 000/468 | Loss: 2.2189 Epoch: 009/010 | Batch 200/468 | Loss: 2.1120 Epoch: 009/010 | Batch 400/468 | Loss: 2.1923 Epoch: 010/010 | Batch 000/468 | Loss: 2.1188 Epoch: 010/010 | Batch 200/468 | Loss: 2.0416 Epoch: 010/010 | Batch 400/468 | Loss: 1.9729","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/pure_pytorch/CORAL_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits , probas = model ( features ) predicted_labels = proba_to_label ( probas ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 3.45 | 3.34 Mean squared error (train/test): 18.00 | 16.91 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_cement/","text":"CORN MLP for predicting cement strength (cement_strength) This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression. 0 -- Obtaining and preparing the cement_strength dataset We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Split into training and test data from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values ) Standardize features from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test ) 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cuda:0 from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with a CORN layer In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate ) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 0.7095 Epoch: 002/020 | Batch 000/007 | Cost: 0.5793 Epoch: 003/020 | Batch 000/007 | Cost: 0.5107 Epoch: 004/020 | Batch 000/007 | Cost: 0.4893 Epoch: 005/020 | Batch 000/007 | Cost: 0.4294 Epoch: 006/020 | Batch 000/007 | Cost: 0.3942 Epoch: 007/020 | Batch 000/007 | Cost: 0.3905 Epoch: 008/020 | Batch 000/007 | Cost: 0.3877 Epoch: 009/020 | Batch 000/007 | Cost: 0.3327 Epoch: 010/020 | Batch 000/007 | Cost: 0.3442 Epoch: 011/020 | Batch 000/007 | Cost: 0.3513 Epoch: 012/020 | Batch 000/007 | Cost: 0.3395 Epoch: 013/020 | Batch 000/007 | Cost: 0.3272 Epoch: 014/020 | Batch 000/007 | Cost: 0.3372 Epoch: 015/020 | Batch 000/007 | Cost: 0.2994 Epoch: 016/020 | Batch 000/007 | Cost: 0.3409 Epoch: 017/020 | Batch 000/007 | Cost: 0.3158 Epoch: 018/020 | Batch 000/007 | Cost: 0.2988 Epoch: 019/020 | Batch 000/007 | Cost: 0.2793 Epoch: 020/020 | Batch 000/007 | Cost: 0.2516 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.29 | 0.36 Mean squared error (train/test): 0.34 | 0.39 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[8.4400e-01, 1.1552e-01, 2.4885e-02, 2.1235e-02], [9.6955e-01, 9.6440e-01, 7.9017e-01, 4.0131e-01], [9.6926e-01, 9.6164e-01, 2.8837e-01, 1.1151e-01], [2.7557e-01, 1.7854e-03, 1.3533e-04, 6.4534e-05], [4.4200e-04, 2.9050e-05, 1.8071e-05, 8.5216e-06], [4.1626e-02, 6.8911e-06, 1.1300e-06, 1.1232e-06], [9.5031e-01, 3.2661e-01, 7.6083e-03, 3.6258e-03], [9.8467e-01, 9.0953e-01, 4.3580e-01, 3.9399e-01], [8.0870e-01, 1.9610e-01, 1.9341e-02, 1.6238e-03], [9.6289e-01, 7.2809e-01, 2.1034e-01, 1.4426e-01], [9.8087e-01, 3.4986e-01, 7.5893e-03, 2.1336e-04], [8.3218e-02, 2.9795e-04, 8.8117e-05, 7.7257e-05], [6.4886e-01, 3.3336e-01, 1.7751e-01, 1.1291e-01], [8.0380e-01, 5.5894e-03, 3.1419e-04, 2.4602e-04], [9.3716e-01, 9.3670e-01, 9.3338e-01, 8.3394e-01], [9.0723e-01, 9.0255e-01, 8.7473e-01, 4.9182e-01], [9.8959e-01, 3.3517e-01, 5.4329e-02, 1.7331e-03], [9.6824e-01, 8.0327e-01, 2.5958e-01, 8.4942e-03], [9.6470e-01, 9.1665e-01, 6.9238e-01, 3.8931e-01], [9.6623e-01, 9.6491e-01, 9.4429e-01, 4.3117e-01], [8.0910e-02, 1.5353e-04, 2.7122e-05, 2.1541e-05], [9.9247e-01, 8.6671e-01, 6.3087e-01, 6.6279e-02], [8.8915e-01, 2.5603e-02, 1.8793e-03, 1.5186e-03], [6.2060e-01, 1.8354e-01, 4.0813e-02, 2.1553e-02], [9.5856e-01, 9.5805e-01, 9.2657e-01, 1.6030e-01], [9.9292e-01, 6.5836e-01, 1.8671e-01, 6.0837e-02], [1.0555e-01, 4.6840e-03, 1.1164e-03, 1.7749e-04], [9.6029e-01, 4.0485e-01, 3.0195e-02, 2.0155e-03], [9.8264e-01, 9.1183e-01, 4.3322e-01, 2.3925e-03], [8.9595e-01, 3.6590e-01, 3.0114e-02, 1.9936e-03]], device='cuda:0')","title":"CORN MLP model for tabular data (Cement dataset)"},{"location":"tutorials/pure_pytorch/CORN_cement/#corn-mlp-for-predicting-cement-strength-cement_strength","text":"This tutorial explains how to train a deep neural network (here: multilayer perceptron) with the CORN loss function for ordinal regression.","title":"CORN MLP for predicting cement strength (cement_strength)"},{"location":"tutorials/pure_pytorch/CORN_cement/#0-obtaining-and-preparing-the-cement_strength-dataset","text":"We will be using the cement_strength dataset from https://github.com/gagolews/ordinal_regression_data/blob/master/cement_strength.csv . First, we are going to download and prepare the and save it as CSV files locally. This is a general procedure that is not specific to CORN. This dataset has 5 ordinal labels (1, 2, 3, 4, and 5). Note that CORN requires labels to be starting at 0, which is why we subtract \"1\" from the label column. import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4]","title":"0 -- Obtaining and preparing the cement_strength dataset"},{"location":"tutorials/pure_pytorch/CORN_cement/#split-into-training-and-test-data","text":"from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data_features . values , data_labels . values , test_size = 0.2 , random_state = 1 , stratify = data_labels . values )","title":"Split into training and test data"},{"location":"tutorials/pure_pytorch/CORN_cement/#standardize-features","text":"from sklearn.preprocessing import StandardScaler sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_test_std = sc . transform ( X_test )","title":"Standardize features"},{"location":"tutorials/pure_pytorch/CORN_cement/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 5 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) Training on cuda:0 from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( np . float32 ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . labels . shape [ 0 ] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset ( X_train_std , y_train ) test_dataset = MyDataset ( X_test_std , y_test ) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , shuffle = True , # want to shuffle the dataset num_workers = 0 ) # number processes/CPUs to use test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = 0 ) # Checking the dataset for inputs , labels in train_loader : print ( 'Input batch dimensions:' , inputs . shape ) print ( 'Input label dimensions:' , labels . shape ) break Input batch dimensions: torch.Size([128, 8]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORN_cement/#2-equipping-mlp-with-a-corn-layer","text":"In this section, we are implementing a simple MLP for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class MLP ( torch . nn . Module ): def __init__ ( self , in_features , num_classes , num_hidden_1 = 300 , num_hidden_2 = 300 ): super () . __init__ () self . my_network = torch . nn . Sequential ( # 1st hidden layer torch . nn . Linear ( in_features , num_hidden_1 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_1 ), # 2nd hidden layer torch . nn . Linear ( num_hidden_1 , num_hidden_2 , bias = False ), torch . nn . LeakyReLU (), torch . nn . Dropout ( 0.2 ), torch . nn . BatchNorm1d ( num_hidden_2 ), ### Specify CORN layer torch . nn . Linear ( num_hidden_2 , ( num_classes - 1 )) ###--------------------------------------------------------------------### ) def forward ( self , x ): logits = self . my_network ( x ) return logits torch . manual_seed ( random_seed ) model = MLP ( in_features = 8 , num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters (), lr = learning_rate )","title":"2 - Equipping MLP with a CORN layer"},{"location":"tutorials/pure_pytorch/CORN_cement/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/020 | Batch 000/007 | Cost: 0.7095 Epoch: 002/020 | Batch 000/007 | Cost: 0.5793 Epoch: 003/020 | Batch 000/007 | Cost: 0.5107 Epoch: 004/020 | Batch 000/007 | Cost: 0.4893 Epoch: 005/020 | Batch 000/007 | Cost: 0.4294 Epoch: 006/020 | Batch 000/007 | Cost: 0.3942 Epoch: 007/020 | Batch 000/007 | Cost: 0.3905 Epoch: 008/020 | Batch 000/007 | Cost: 0.3877 Epoch: 009/020 | Batch 000/007 | Cost: 0.3327 Epoch: 010/020 | Batch 000/007 | Cost: 0.3442 Epoch: 011/020 | Batch 000/007 | Cost: 0.3513 Epoch: 012/020 | Batch 000/007 | Cost: 0.3395 Epoch: 013/020 | Batch 000/007 | Cost: 0.3272 Epoch: 014/020 | Batch 000/007 | Cost: 0.3372 Epoch: 015/020 | Batch 000/007 | Cost: 0.2994 Epoch: 016/020 | Batch 000/007 | Cost: 0.3409 Epoch: 017/020 | Batch 000/007 | Cost: 0.3158 Epoch: 018/020 | Batch 000/007 | Cost: 0.2988 Epoch: 019/020 | Batch 000/007 | Cost: 0.2793 Epoch: 020/020 | Batch 000/007 | Cost: 0.2516","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/pure_pytorch/CORN_cement/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.29 | 0.36 Mean squared error (train/test): 0.34 | 0.39 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_cement/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[8.4400e-01, 1.1552e-01, 2.4885e-02, 2.1235e-02], [9.6955e-01, 9.6440e-01, 7.9017e-01, 4.0131e-01], [9.6926e-01, 9.6164e-01, 2.8837e-01, 1.1151e-01], [2.7557e-01, 1.7854e-03, 1.3533e-04, 6.4534e-05], [4.4200e-04, 2.9050e-05, 1.8071e-05, 8.5216e-06], [4.1626e-02, 6.8911e-06, 1.1300e-06, 1.1232e-06], [9.5031e-01, 3.2661e-01, 7.6083e-03, 3.6258e-03], [9.8467e-01, 9.0953e-01, 4.3580e-01, 3.9399e-01], [8.0870e-01, 1.9610e-01, 1.9341e-02, 1.6238e-03], [9.6289e-01, 7.2809e-01, 2.1034e-01, 1.4426e-01], [9.8087e-01, 3.4986e-01, 7.5893e-03, 2.1336e-04], [8.3218e-02, 2.9795e-04, 8.8117e-05, 7.7257e-05], [6.4886e-01, 3.3336e-01, 1.7751e-01, 1.1291e-01], [8.0380e-01, 5.5894e-03, 3.1419e-04, 2.4602e-04], [9.3716e-01, 9.3670e-01, 9.3338e-01, 8.3394e-01], [9.0723e-01, 9.0255e-01, 8.7473e-01, 4.9182e-01], [9.8959e-01, 3.3517e-01, 5.4329e-02, 1.7331e-03], [9.6824e-01, 8.0327e-01, 2.5958e-01, 8.4942e-03], [9.6470e-01, 9.1665e-01, 6.9238e-01, 3.8931e-01], [9.6623e-01, 9.6491e-01, 9.4429e-01, 4.3117e-01], [8.0910e-02, 1.5353e-04, 2.7122e-05, 2.1541e-05], [9.9247e-01, 8.6671e-01, 6.3087e-01, 6.6279e-02], [8.8915e-01, 2.5603e-02, 1.8793e-03, 1.5186e-03], [6.2060e-01, 1.8354e-01, 4.0813e-02, 2.1553e-02], [9.5856e-01, 9.5805e-01, 9.2657e-01, 1.6030e-01], [9.9292e-01, 6.5836e-01, 1.8671e-01, 6.0837e-02], [1.0555e-01, 4.6840e-03, 1.1164e-03, 1.7749e-04], [9.6029e-01, 4.0485e-01, 3.0195e-02, 2.0155e-03], [9.8264e-01, 9.1183e-01, 4.3322e-01, 2.3925e-03], [8.9595e-01, 3.6590e-01, 3.0114e-02, 1.9936e-03]], device='cuda:0')","title":"5 -- Rank probabilities from logits"},{"location":"tutorials/pure_pytorch/CORN_mnist/","text":"CORN CNN for predicting handwritten digits (MNIST) This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cuda:0 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with a CORN layer In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ()) 3 - Using the CORN loss for model training During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 0.6896 Epoch: 001/010 | Batch 200/468 | Cost: 0.1449 Epoch: 001/010 | Batch 400/468 | Cost: 0.0761 Epoch: 002/010 | Batch 000/468 | Cost: 0.0927 Epoch: 002/010 | Batch 200/468 | Cost: 0.0679 Epoch: 002/010 | Batch 400/468 | Cost: 0.0714 Epoch: 003/010 | Batch 000/468 | Cost: 0.0593 Epoch: 003/010 | Batch 200/468 | Cost: 0.0516 Epoch: 003/010 | Batch 400/468 | Cost: 0.0470 Epoch: 004/010 | Batch 000/468 | Cost: 0.0301 Epoch: 004/010 | Batch 200/468 | Cost: 0.0417 Epoch: 004/010 | Batch 400/468 | Cost: 0.0366 Epoch: 005/010 | Batch 000/468 | Cost: 0.0449 Epoch: 005/010 | Batch 200/468 | Cost: 0.0380 Epoch: 005/010 | Batch 400/468 | Cost: 0.0141 Epoch: 006/010 | Batch 000/468 | Cost: 0.0272 Epoch: 006/010 | Batch 200/468 | Cost: 0.0267 Epoch: 006/010 | Batch 400/468 | Cost: 0.0405 Epoch: 007/010 | Batch 000/468 | Cost: 0.0649 Epoch: 007/010 | Batch 200/468 | Cost: 0.0253 Epoch: 007/010 | Batch 400/468 | Cost: 0.0215 Epoch: 008/010 | Batch 000/468 | Cost: 0.0389 Epoch: 008/010 | Batch 200/468 | Cost: 0.0297 Epoch: 008/010 | Batch 400/468 | Cost: 0.0343 Epoch: 009/010 | Batch 000/468 | Cost: 0.0249 Epoch: 009/010 | Batch 200/468 | Cost: 0.0498 Epoch: 009/010 | Batch 400/468 | Cost: 0.0300 Epoch: 010/010 | Batch 000/468 | Cost: 0.0201 Epoch: 010/010 | Batch 200/468 | Cost: 0.0290 Epoch: 010/010 | Batch 400/468 | Cost: 0.0303 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.15 | 0.15 Mean squared error (train/test): 0.69 | 0.74 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes. 5 -- Rank probabilities from logits To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9986e-01, 9.9941e-01, 2.5950e-08], [1.0000e+00, 1.0000e+00, 9.9315e-01, ..., 9.8477e-01, 9.8476e-01, 9.7987e-08], [9.1224e-01, 9.1223e-01, 9.1223e-01, ..., 8.5374e-01, 8.5216e-01, 1.6753e-03], ..., [9.9812e-01, 9.9811e-01, 9.9811e-01, ..., 9.8991e-01, 9.8968e-01, 4.1033e-03], [9.9979e-01, 9.9979e-01, 9.9979e-01, ..., 1.5020e-02, 1.5015e-02, 2.7997e-04], [7.7070e-07, 7.7070e-07, 7.5224e-07, ..., 7.6964e-08, 7.6941e-08, 6.1278e-13]], device='cuda:0')","title":"CORN CNN model for image data (MNIST dataset)"},{"location":"tutorials/pure_pytorch/CORN_mnist/#corn-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to train a deep neural network with the CORN loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORN CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/pure_pytorch/CORN_mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORN. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( 'Training on' , DEVICE ) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets . MNIST ( root = '../data' , train = True , transform = transforms . ToTensor (), download = True ) test_dataset = datasets . MNIST ( root = '../data' , train = False , transform = transforms . ToTensor ()) train_loader = DataLoader ( dataset = train_dataset , batch_size = batch_size , drop_last = True , shuffle = True ) test_loader = DataLoader ( dataset = test_dataset , batch_size = batch_size , drop_last = True , shuffle = False ) # Checking the dataset for images , labels in train_loader : print ( 'Image batch dimensions:' , images . shape ) print ( 'Image label dimensions:' , labels . shape ) break Training on cuda:0 Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/pure_pytorch/CORN_mnist/#2-equipping-cnn-with-a-corn-layer","text":"In this section, we are implementing a simple CNN for ordinal regression with CORN. Note that the only specific modification required is setting the number of output of the last layer (a fully connected layer) to the number of classes - 1 (these correspond to the binary tasks used in the extended binary classification as described in the paper). class ConvNet ( torch . nn . Module ): def __init__ ( self , num_classes ): super ( ConvNet , self ) . __init__ () self . features = torch . nn . Sequential ( torch . nn . Conv2d ( 1 , 3 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 )), torch . nn . Conv2d ( 3 , 6 , ( 3 , 3 ), ( 1 , 1 ), 1 ), torch . nn . MaxPool2d (( 2 , 2 ), ( 2 , 2 ))) ### Specify CORN layer self . output_layer = torch . nn . Linear ( in_features = 294 , out_features = num_classes - 1 ) ###--------------------------------------------------------------------### def forward ( self , x ): x = self . features ( x ) x = x . view ( x . size ( 0 ), - 1 ) # flatten ##### Use CORN layer ##### logits = self . output_layer ( x ) ###--------------------------------------------------------------------### return logits torch . manual_seed ( random_seed ) model = ConvNet ( num_classes = NUM_CLASSES ) model . to ( DEVICE ) optimizer = torch . optim . Adam ( model . parameters ())","title":"2 - Equipping CNN with a CORN layer"},{"location":"tutorials/pure_pytorch/CORN_mnist/#3-using-the-corn-loss-for-model-training","text":"During training, all you need to do is to use the corn_loss provided via coral_pytorch . The loss function will take care of the conditional training set processing and modeling the conditional probabilities used in the chain rule (aka general product rule). from coral_pytorch.losses import corn_loss for epoch in range ( num_epochs ): model = model . train () for batch_idx , ( features , class_labels ) in enumerate ( train_loader ): class_labels = class_labels . to ( DEVICE ) features = features . to ( DEVICE ) logits = model ( features ) #### CORN loss loss = corn_loss ( logits , class_labels , NUM_CLASSES ) ###--------------------------------------------------------------------### optimizer . zero_grad () loss . backward () optimizer . step () ### LOGGING if not batch_idx % 200 : print ( 'Epoch: %03d / %03d | Batch %03d / %03d | Cost: %.4f ' % ( epoch + 1 , num_epochs , batch_idx , len ( train_loader ), loss )) Epoch: 001/010 | Batch 000/468 | Cost: 0.6896 Epoch: 001/010 | Batch 200/468 | Cost: 0.1449 Epoch: 001/010 | Batch 400/468 | Cost: 0.0761 Epoch: 002/010 | Batch 000/468 | Cost: 0.0927 Epoch: 002/010 | Batch 200/468 | Cost: 0.0679 Epoch: 002/010 | Batch 400/468 | Cost: 0.0714 Epoch: 003/010 | Batch 000/468 | Cost: 0.0593 Epoch: 003/010 | Batch 200/468 | Cost: 0.0516 Epoch: 003/010 | Batch 400/468 | Cost: 0.0470 Epoch: 004/010 | Batch 000/468 | Cost: 0.0301 Epoch: 004/010 | Batch 200/468 | Cost: 0.0417 Epoch: 004/010 | Batch 400/468 | Cost: 0.0366 Epoch: 005/010 | Batch 000/468 | Cost: 0.0449 Epoch: 005/010 | Batch 200/468 | Cost: 0.0380 Epoch: 005/010 | Batch 400/468 | Cost: 0.0141 Epoch: 006/010 | Batch 000/468 | Cost: 0.0272 Epoch: 006/010 | Batch 200/468 | Cost: 0.0267 Epoch: 006/010 | Batch 400/468 | Cost: 0.0405 Epoch: 007/010 | Batch 000/468 | Cost: 0.0649 Epoch: 007/010 | Batch 200/468 | Cost: 0.0253 Epoch: 007/010 | Batch 400/468 | Cost: 0.0215 Epoch: 008/010 | Batch 000/468 | Cost: 0.0389 Epoch: 008/010 | Batch 200/468 | Cost: 0.0297 Epoch: 008/010 | Batch 400/468 | Cost: 0.0343 Epoch: 009/010 | Batch 000/468 | Cost: 0.0249 Epoch: 009/010 | Batch 200/468 | Cost: 0.0498 Epoch: 009/010 | Batch 400/468 | Cost: 0.0300 Epoch: 010/010 | Batch 000/468 | Cost: 0.0201 Epoch: 010/010 | Batch 200/468 | Cost: 0.0290 Epoch: 010/010 | Batch 400/468 | Cost: 0.0303","title":"3 - Using the CORN loss for model training"},{"location":"tutorials/pure_pytorch/CORN_mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the corn_label_from_logits utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import corn_label_from_logits def compute_mae_and_mse ( model , data_loader , device ): with torch . no_grad (): mae , mse , acc , num_examples = 0. , 0. , 0. , 0 for i , ( features , targets ) in enumerate ( data_loader ): features = features . to ( device ) targets = targets . float () . to ( device ) logits = model ( features ) predicted_labels = corn_label_from_logits ( logits ) . float () num_examples += targets . size ( 0 ) mae += torch . sum ( torch . abs ( predicted_labels - targets )) mse += torch . sum (( predicted_labels - targets ) ** 2 ) mae = mae / num_examples mse = mse / num_examples return mae , mse train_mae , train_mse = compute_mae_and_mse ( model , train_loader , DEVICE ) test_mae , test_mse = compute_mae_and_mse ( model , test_loader , DEVICE ) print ( f 'Mean absolute error (train/test): { train_mae : .2f } | { test_mae : .2f } ' ) print ( f 'Mean squared error (train/test): { train_mse : .2f } | { test_mse : .2f } ' ) Mean absolute error (train/test): 0.15 | 0.15 Mean squared error (train/test): 0.69 | 0.74 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/pure_pytorch/CORN_mnist/#5-rank-probabilities-from-logits","text":"To obtain the rank probabilities from the logits, you can use the sigmoid function to get the conditional probabilities for each task and then compute the task probabilities via the chain rule for probabilities. Note that this is also done internally by the corn_label_from_logits we used above. logits = model ( features ) with torch . no_grad (): probas = torch . sigmoid ( logits ) probas = torch . cumprod ( probas , dim = 1 ) print ( probas ) tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, ..., 9.9986e-01, 9.9941e-01, 2.5950e-08], [1.0000e+00, 1.0000e+00, 9.9315e-01, ..., 9.8477e-01, 9.8476e-01, 9.7987e-08], [9.1224e-01, 9.1223e-01, 9.1223e-01, ..., 8.5374e-01, 8.5216e-01, 1.6753e-03], ..., [9.9812e-01, 9.9811e-01, 9.9811e-01, ..., 9.8991e-01, 9.8968e-01, 4.1033e-03], [9.9979e-01, 9.9979e-01, 9.9979e-01, ..., 1.5020e-02, 1.5015e-02, 2.7997e-04], [7.7070e-07, 7.7070e-07, 7.5224e-07, ..., 7.6964e-08, 7.6941e-08, 6.1278e-13]], device='cuda:0')","title":"5 -- Rank probabilities from logits"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/","text":"A Multilayer Perceptron for Ordinal Regression using CORAL -- Cement Dataset In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 32 NUM_EPOCHS = 200 LEARNING_RATE = 0.01 NUM_WORKERS = 0 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the CORAL loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(hidden_units[-1], num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels . type_as ( logits )) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset: Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model. Creating a Dataset class Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ] Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 24 , 16 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-coral-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 636 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 636 Trainable params 0 Non-trainable params 636 Total params 0.003 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.94 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 110. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.25} -------------------------------------------------------------------------------- [{'test_mae': 0.25}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 4, 0, 3, 1])","title":"CORAL multilayer perceptron for tabular data (Cement dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#a-multilayer-perceptron-for-ordinal-regression-using-coral-cement-dataset","text":"In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331","title":"A Multilayer Perceptron for Ordinal Regression using CORAL -- Cement Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 32 NUM_EPOCHS = 200 LEARNING_RATE = 0.01 NUM_WORKERS = 0 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the CORAL loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(hidden_units[-1], num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels . type_as ( logits )) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset:","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#creating-a-dataset-class","text":"Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ]","title":"Creating a Dataset class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 24 , 16 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-coral-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 636 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 636 Trainable params 0 Non-trainable params 636 Total params 0.003 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.94 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 110. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.25} -------------------------------------------------------------------------------- [{'test_mae': 0.25}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_cement/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-coral-cement/version_3/checkpoints/epoch=114-step=2529.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 4, 0, 3, 1])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/","text":"A Convolutional Neural Net for Ordinal Regression using CORAL -- MNIST Dataset In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing a ConvNet using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(294, num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = 294 , num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( logits ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [4], in <cell line: 5>() 2 from coral_pytorch.dataset import levels_from_labelbatch 3 from coral_pytorch.dataset import proba_to_label ----> 5 import pytorch_lightning as pl 6 import torchmetrics 9 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy' Setting up the dataset In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. Inspecting the dataset import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-coral-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) It's hard to tell what the best model (based on the lowest validation set MAE) is in this case, but no worries, the trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"CORAL convolutional neural net for image data (MNIST dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#a-convolutional-neural-net-for-ordinal-regression-using-coral-mnist-dataset","text":"In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"A Convolutional Neural Net for Ordinal Regression using CORAL -- MNIST Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORAL as explained in the previous section. In the code example below, we use \"1) the CoralLayer \". import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # output_layer = torch.nn.Linear(294, num_classes) # We replace it by the CORAL layer: output_layer = CoralLayer ( size_in = 294 , num_classes = num_classes ) # ---------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( logits ) # ------------------------------------------------------- logits = self ( features ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [4], in <cell line: 5>() 2 from coral_pytorch.dataset import levels_from_labelbatch 3 from coral_pytorch.dataset import proba_to_label ----> 5 import pytorch_lightning as pl 6 import torchmetrics 9 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy'","title":"Implementing a ConvNet using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#inspecting-the-dataset","text":"import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-coral-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" )","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) It's hard to tell what the best model (based on the lowest validation set MAE) is in this case, but no worries, the trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_mnist/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/","text":"A Recurrent Neural Net for Ordinal Regression using CORAL -- TripAdvisor Dataset In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.0005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" Converting a regular classifier into a CORAL ordinal regression model Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas ) Implementing an RNN using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # self.output_layer = torch.nn.Linear(hidden_dim, num_classes) # We replace it by the CORAL layer: self . output_layer = CoralLayer ( size_in = hidden_dim , num_classes = num_classes ) # ---------------------------------------------------------------- def forward ( self , text , text_length ): # text dim: [sentence length, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence length, batch size, embedding dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence length, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( features ) # ------------------------------------------------------- logits = self ( features , text_length ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ----------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TEXT_COLUMN_NAME LABEL_COLUMN_NAME 6995 beautiful hotel, stay punta cana majestic colo... 5 6996 stay, n't stay, stayed week april, weather ama... 5 6997 stay hotel fantastic, great location, looked n... 5 6998 birthday meal havnt stayed hotel staying barce... 5 6999 great hotel great location stayed royal magda ... 5 import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) 39:1: E122 continuation line missing indentation or outdented # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Training label distribution: tensor([964, 963, 954, 953, 926], device='cuda:0') Test labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Test label distribution: tensor([275, 267, 300, 274, 284], device='cuda:0') Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.18 In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): Training the model using the PyTorch Lightning Trainer class Next, we initialize our RNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-coral-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Text length: 469 Batch size (from text): 16 Batch size (from labels): 16 Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | PyTorchRNN | 3.0 M 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 3.0 M Trainable params 0 Non-trainable params 3.0 M Total params 11.823 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 3.39 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 5. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 1.0885714292526245} -------------------------------------------------------------------------------- [{'test_mae': 1.0885714292526245}] Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([2, 0, 3, 1, 1], device='cuda:0')","title":"CORAL recurrent neural net for text data (TripAdvisor dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#a-recurrent-neural-net-for-ordinal-regression-using-coral-tripadvisor-dataset","text":"In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORAL method. To learn more about CORAL, please have a look at our paper: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020): Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters. 140, 325-331 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript.","title":"A Recurrent Neural Net for Ordinal Regression using CORAL -- TripAdvisor Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.0005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#converting-a-regular-classifier-into-a-coral-ordinal-regression-model","text":"Changing a classifier to a CORAL model for ordinal regression is actually really simple and only requires a few changes: 1) We replace the output layer output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) by a CORAL layer (available through coral_pytorch ): output_layer = CoralLayer ( size_in = hidden_units [ - 1 ], num_classes = num_classes ) ` 2) Convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch ( class_labels , num_classes = num_classes ) 3) Swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORAL loss (also provided via coral_pytorch ): loss = coral_loss ( logits , levels ) 4) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) Replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = proba_to_label ( probas )","title":"Converting a regular classifier into a CORAL ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#implementing-an-rnn-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch from coral_pytorch.layers import CoralLayer # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORAL: output layer ------------------------------------------- # Regular classifier would use the following output layer: # self.output_layer = torch.nn.Linear(hidden_dim, num_classes) # We replace it by the CORAL layer: self . output_layer = CoralLayer ( size_in = hidden_dim , num_classes = num_classes ) # ---------------------------------------------------------------- def forward ( self , text , text_length ): # text dim: [sentence length, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence length, batch size, embedding dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence length, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Note that we make changes 2) ( levels_from_labelbatch ), 3) ( coral_loss ), and 4) ( proba_to_label ) to implement a CORAL model instead of a regular classifier: from coral_pytorch.losses import coral_loss from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.dataset import proba_to_label import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME # Convert class labels for CORAL ------------------------ levels = levels_from_labelbatch ( true_labels , num_classes = self . model . num_classes ) . type_as ( features ) # ------------------------------------------------------- logits = self ( features , text_length ) # CORAL Loss -------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = coral_loss ( logits , levels ) # ------------------------------------------------------- # CORAL Prediction to label ----------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) # ----------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing an RNN using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TEXT_COLUMN_NAME LABEL_COLUMN_NAME 6995 beautiful hotel, stay punta cana majestic colo... 5 6996 stay, n't stay, stayed week april, weather ama... 5 6997 stay hotel fantastic, great location, looked n... 5 6998 birthday meal havnt stayed hotel staying barce... 5 6999 great hotel great location stayed royal magda ... 5 import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) 39:1: E122 continuation line missing indentation or outdented # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Training label distribution: tensor([964, 963, 954, 953, 926], device='cuda:0') Test labels: tensor([0, 1, 2, 3, 4], device='cuda:0') Test label distribution: tensor([275, 267, 300, 274, 284], device='cuda:0') Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.18 In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code):","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our RNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-coral-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Text length: 469 Batch size (from text): 16 Batch size (from labels): 16 Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | PyTorchRNN | 3.0 M 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 3.0 M Trainable params 0 Non-trainable params 3.0 M Total params 11.823 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 3.39 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 5. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 1.0885714292526245} -------------------------------------------------------------------------------- [{'test_mae': 1.0885714292526245}]","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-coral_tripadvisor/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/rnn-coral-mnist/version_16/checkpoints/epoch=38-step=11621.ckpt lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) probas = torch . sigmoid ( logits ) predicted_labels = proba_to_label ( probas ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([2, 0, 3, 1, 1], device='cuda:0')","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/","text":"A Multilayer Perceptron for Ordinal Regression using CORN -- Cement Dataset In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 128 NUM_EPOCHS = 20 LEARNING_RATE = 0.1 NUM_WORKERS = 0 DATA_BASEPATH = \"./\" Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORN output layer ------------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) # ------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset: Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model. Creating a Dataset class Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ] Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 40 , 20 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-corn-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 1.3 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 1.3 K Trainable params 0 Non-trainable params 1.3 K Total params 0.005 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.08 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 175. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.30000001192092896} -------------------------------------------------------------------------------- [{'test_mae': 0.30000001192092896}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 3, 1, 2, 0])","title":"CORN multilayer perceptron for tabular data (Cement dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#a-multilayer-perceptron-for-ordinal-regression-using-corn-cement-dataset","text":"In this tutorial, we implement a multilayer perceptron for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851","title":"A Multilayer Perceptron for Ordinal Regression using CORN -- Cement Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch BATCH_SIZE = 128 NUM_EPOCHS = 20 LEARNING_RATE = 0.1 NUM_WORKERS = 0 DATA_BASEPATH = \"./\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#implementing-a-multilayerperceptron-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our MultiLayerPerceptron model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class MultiLayerPerceptron ( torch . nn . Module ): def __init__ ( self , input_size , hidden_units , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize MLP layers all_layers = [] for hidden_unit in hidden_units : layer = torch . nn . Linear ( input_size , hidden_unit ) all_layers . append ( layer ) all_layers . append ( torch . nn . ReLU ()) input_size = hidden_unit # CORN output layer ------------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) # ------------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a multilayer perceptron classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningMLP ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a MultiLayerPerceptron using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. We start by downloading and taking a look at the Cement dataset:","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/gagolews/\" \"ordinal_regression_data/master/cement_strength.csv\" ) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 data_labels = data_df [ \"response\" ] data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] data_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } response V1 V2 V3 V4 V5 V6 V7 V8 0 4 540.0 0.0 0.0 162.0 2.5 1040.0 676.0 28 1 4 540.0 0.0 0.0 162.0 2.5 1055.0 676.0 28 2 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 270 3 2 332.5 142.5 0.0 228.0 0.0 932.0 594.0 365 4 2 198.6 132.4 0.0 192.0 0.0 978.4 825.5 360 print ( 'Number of features:' , data_features . shape [ 1 ]) print ( 'Number of examples:' , data_features . shape [ 0 ]) print ( 'Labels:' , np . unique ( data_labels . values )) print ( 'Label distribution:' , np . bincount ( data_labels )) Number of features: 8 Number of examples: 998 Labels: [0 1 2 3 4] Label distribution: [196 310 244 152 96] Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: avg_prediction = np . median ( data_labels . values ) # median minimizes MAE baseline_mae = np . mean ( np . abs ( data_labels . values - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 1.03 In other words, a model that would always predict the dataset median would achieve a MAE of 1.03. A model that has an MAE of > 1 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#creating-a-dataset-class","text":"Next, let us set up a data loading mechanism for our model. Note that the Cement dataset is a relatively small dataset that fits into memory quite comfortably so this may seem like overkill. However, the following steps are useful as a template since you can use those for arbitrarily-sized datatsets. First, we define a PyTorch Dataset class that returns the features (inputs) and labels: from torch.utils.data import Dataset class MyDataset ( Dataset ): def __init__ ( self , feature_array , label_array , dtype = np . float32 ): self . features = feature_array . astype ( dtype ) self . labels = label_array def __getitem__ ( self , index ): inputs = self . features [ index ] label = self . labels [ index ] return inputs , label def __len__ ( self ): return self . features . shape [ 0 ]","title":"Creating a Dataset class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): data_df = pd . read_csv ( 'https://raw.githubusercontent.com/gagolews/' 'ordinal_regression_data/master/cement_strength.csv' ) data_df . to_csv ( os . path . join ( self . data_path , 'cement_strength.csv' ), index = None ) return def setup ( self , stage = None ): data_df = pd . read_csv ( os . path . join ( self . data_path , 'cement_strength.csv' )) data_df [ \"response\" ] = data_df [ \"response\" ] - 1 # labels should start at 0 self . data_labels = data_df [ \"response\" ] self . data_features = data_df . loc [:, [ \"V1\" , \"V2\" , \"V3\" , \"V4\" , \"V5\" , \"V6\" , \"V7\" , \"V8\" ]] # Split into # 70% train, 10% validation, 20% testing X_temp , X_test , y_temp , y_test = train_test_split ( self . data_features . values , self . data_labels . values , test_size = 0.2 , random_state = 1 , stratify = self . data_labels . values ) X_train , X_valid , y_train , y_valid = train_test_split ( X_temp , y_temp , test_size = 0.1 , random_state = 1 , stratify = y_temp ) # Standardize features sc = StandardScaler () X_train_std = sc . fit_transform ( X_train ) X_valid_std = sc . transform ( X_valid ) X_test_std = sc . transform ( X_test ) self . train = MyDataset ( X_train_std , y_train ) self . valid = MyDataset ( X_valid_std , y_valid ) self . test = MyDataset ( X_test_std , y_test ) def train_dataloader ( self ): return DataLoader ( self . train , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True ) def val_dataloader ( self ): return DataLoader ( self . valid , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) def test_dataloader ( self ): return DataLoader ( self . test , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS ) Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our multilayer perceptron model (here, a 2-layer MLP with 24 units in the first hidden layer, and 16 units in the second hidden layer). We wrap the model in our LightningMLP so that we can use PyTorch Lightning's powerful Trainer API. Also, we define a callback so that we can obtain the model with the best validation set performance after training. Note PyTorch Lightning offers many advanced logging services like Weights & Biases. However, here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = MultiLayerPerceptron ( input_size = data_features . shape [ 1 ], hidden_units = ( 40 , 20 ), num_classes = np . bincount ( data_labels ) . shape [ 0 ]) lightning_model = LightningMLP ( model = pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = \"min\" , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"mlp-corn-cement\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params --------------------------------------------------- 0 | model | MultiLayerPerceptron | 1.3 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 --------------------------------------------------- 1.3 K Trainable params 0 Non-trainable params 1.3 K Total params 0.005 Total estimated model params size (MB) Validation sanity check: 0it [00:00, ?it/s] /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch. rank_zero_warn( Training: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Validating: 0it [00:00, ?it/s] Training took 0.08 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 175. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance. rank_zero_warn( Testing: 0it [00:00, ?it/s] -------------------------------------------------------------------------------- DATALOADER:0 TEST RESULTS {'test_mae': 0.30000001192092896} -------------------------------------------------------------------------------- [{'test_mae': 0.30000001192092896}] The MAE of our model is quite good, especially compared to the 1.03 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_cement/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/mlp-corn-cement/version_6/checkpoints/epoch=17-step=89.ckpt lightning_model = LightningMLP . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our MultilayerPerceptron , which is passed to LightningMLP requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningMLP 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([0, 3, 1, 2, 0])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/","text":"A Convolutional Neural Net for Ordinal Regression using CORN -- MNIST Dataset In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./\" Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing a ConvNet using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORN output layer -------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( 294 , num_classes - 1 ) # --------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a CNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer Setting up the dataset In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. Inspecting the dataset import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Training label distribution: tensor([5911, 6730, 5949, 6125, 5832, 5410, 5911, 6254, 5841, 5941]) Test labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Test label distribution: tensor([ 980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 2.52 In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH ) Training the model using the PyTorch Lightning Trainer class Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-corn-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs Missing logger folder: logs/cnn-corn-mnist LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | ConvNet | 2.9 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 2.9 K Trainable params 0 Non-trainable params 2.9 K Total params 0.011 Total estimated model params size (MB) Sanity Checking: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Training took 1.38 min in total. Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 16. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt Testing: 0it [00:00, ?it/s] \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Test metric \u2503 DataLoader 0 \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 test_mae \u2502 0.11959999799728394 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [{'test_mae': 0.11959999799728394}] The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier. Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([7, 2, 1, 0, 4])","title":"CORN convolutional neural net for image data (MNIST dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#a-convolutional-neural-net-for-ordinal-regression-using-corn-mnist-dataset","text":"In this tutorial, we implement a convolutional neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"A Convolutional Neural Net for Ordinal Regression using CORN -- MNIST Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 256 NUM_EPOCHS = 20 LEARNING_RATE = 0.005 NUM_WORKERS = 4 DATA_BASEPATH = \"./\"","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#implementing-a-convnet-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our convolutional neural network ConvNet model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class ConvNet ( torch . nn . Module ): def __init__ ( self , in_channels , num_classes ): super () . __init__ () # num_classes is used by the corn loss function self . num_classes = num_classes # Initialize CNN layers all_layers = [ torch . nn . Conv2d ( in_channels = in_channels , out_channels = 3 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Conv2d ( in_channels = 3 , out_channels = 6 , kernel_size = ( 3 , 3 ), stride = ( 1 , 1 ), padding = 1 ), torch . nn . MaxPool2d ( kernel_size = ( 2 , 2 ), stride = ( 2 , 2 )), torch . nn . Flatten () ] # CORN output layer -------------------------------------- # Regular classifier would use num_classes instead of # num_classes-1 below output_layer = torch . nn . Linear ( 294 , num_classes - 1 ) # --------------------------------------------------------- all_layers . append ( output_layer ) self . model = torch . nn . Sequential ( * all_layers ) def forward ( self , x ): x = self . model ( x ) return x In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given a CNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningCNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , x ): return self . model ( x ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): features , true_labels = batch logits = self ( features ) # Use CORN loss -------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, y) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ---------------------------------------------------- # CORN logits to labels ------------------------------ # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ---------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss ) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False ) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss ) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True ) def test_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False ) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer","title":"Implementing a ConvNet using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#inspecting-the-dataset","text":"import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader train_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = True , transform = transforms . ToTensor (), download = True ) train_loader = DataLoader ( dataset = train_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = True , shuffle = True ) test_dataset = datasets . MNIST ( root = DATA_BASEPATH , train = False , transform = transforms . ToTensor ()) test_loader = DataLoader ( dataset = test_dataset , batch_size = BATCH_SIZE , num_workers = NUM_WORKERS , drop_last = False , shuffle = False ) # Checking the dataset all_train_labels = [] all_test_labels = [] for images , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for images , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz 0%| | 0/9912422 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz 0%| | 0/28881 [00:00<?, ?it/s] Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz 0%| | 0/1648877 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz 0%| | 0/4542 [00:00<?, ?it/s] Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Training labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Training label distribution: tensor([5911, 6730, 5949, 6125, 5832, 5410, 5911, 6254, 5841, 5941]) Test labels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Test label distribution: tensor([ 980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite imbalanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) Baseline MAE: 2.52 In other words, a model that would always predict the dataset median would achieve a MAE of 2.52. A model that has an MAE of > 2.52 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule. Here, we are going to use approach 3, which is the most organized approach. The LightningDataModule consists of several self-explanatory methods as we can see below: import os from torch.utils.data.dataset import random_split from torch.utils.data import DataLoader class DataModule ( pl . LightningDataModule ): def __init__ ( self , data_path = './' ): super () . __init__ () self . data_path = data_path def prepare_data ( self ): datasets . MNIST ( root = self . data_path , download = True ) return def setup ( self , stage = None ): # Note transforms.ToTensor() scales input images # to 0-1 range train = datasets . MNIST ( root = self . data_path , train = True , transform = transforms . ToTensor (), download = False ) self . test = datasets . MNIST ( root = self . data_path , train = False , transform = transforms . ToTensor (), download = False ) self . train , self . valid = random_split ( train , lengths = [ 55000 , 5000 ]) def train_dataloader ( self ): train_loader = DataLoader ( dataset = self . train , batch_size = BATCH_SIZE , drop_last = True , shuffle = True , num_workers = NUM_WORKERS ) return train_loader def val_dataloader ( self ): valid_loader = DataLoader ( dataset = self . valid , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return valid_loader def test_dataloader ( self ): test_loader = DataLoader ( dataset = self . test , batch_size = BATCH_SIZE , drop_last = False , shuffle = False , num_workers = NUM_WORKERS ) return test_loader Note that the prepare_data method is usually used for steps that only need to be executed once, for example, downloading the dataset; the setup method defines the the dataset loading -- if you run your code in a distributed setting, this will be called on each node / GPU. Next, lets initialize the DataModule ; we use a random seed for reproducibility (so that the data set is shuffled the same way when we re-execute this code): torch . manual_seed ( 1 ) data_module = DataModule ( data_path = DATA_BASEPATH )","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our CNN ( ConvNet ) model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = ConvNet ( in_channels = 1 , num_classes = torch . unique ( all_test_labels ) . shape [ 0 ]) lightning_model = LightningCNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"cnn-corn-mnist\" ) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , datamodule = data_module ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) /home/jovyan/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer. rank_zero_deprecation( GPU available: True, used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs Missing logger folder: logs/cnn-corn-mnist LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------------------ 0 | model | ConvNet | 2.9 K 1 | train_mae | MeanAbsoluteError | 0 2 | valid_mae | MeanAbsoluteError | 0 3 | test_mae | MeanAbsoluteError | 0 ------------------------------------------------ 2.9 K Trainable params 0 Non-trainable params 2.9 K Total params 0.011 Total estimated model params size (MB) Sanity Checking: 0it [00:00, ?it/s] Training: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Validation: 0it [00:00, ?it/s] Training took 1.38 min in total.","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) <AxesSubplot:xlabel='Epoch', ylabel='MAE'> As we can see from the loss plot above, the model starts overfitting pretty quickly; however the validation set MAE keeps improving. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 16. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , datamodule = data_module , ckpt_path = 'best' ) Restoring states from the checkpoint path at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] Loaded model weights from checkpoint at logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt Testing: 0it [00:00, ?it/s] \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Test metric \u2503 DataLoader 0 \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 test_mae \u2502 0.11959999799728394 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 [{'test_mae': 0.11959999799728394}] The MAE of our model is quite good, especially compared to the 2.52 MAE baseline earlier.","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_mnist/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) logs/cnn-corn-mnist/version_0/checkpoints/epoch=17-step=3852.ckpt lightning_model = LightningCNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . eval (); Note that our ConvNet , which is passed to LightningCNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningCNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. test_dataloader = data_module . test_dataloader () all_predicted_labels = [] for batch in test_dataloader : features , _ = batch logits = lightning_model ( features ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ] tensor([7, 2, 1, 0, 4])","title":"Predicting labels of new data"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/","text":"A Recurrent Neural Net for Ordinal Regression using CORN -- TripAdvisor Dataset In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript. General settings and hyperparameters Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" This tutorial also requires the spacy English vocabulary, which can be downloaded as shown below: python - m spacy download en_core_web_sm Converting a regular classifier into a CORN ordinal regression model Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits ) Implementing an RNN using PyTorch Lightning's LightningModule In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORN output layer ------------------------------------------ # Regular classifier would use num_classes instead of # num_classes-1 below self . output_layer = torch . nn . Linear ( hidden_dim , num_classes - 1 ) # ------------------------------------------------------------ self . num_classes = num_classes def forward ( self , text , text_length ): # text dim: [sentence len, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence len, batch size, embed dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence len, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given an RNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME logits = self ( features , text_length ) # Use CORN loss --------------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ----------------------------------------------------------------- # CORN logits to labels ------------------------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ----------------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [5], in <cell line: 4>() 1 from coral_pytorch.losses import corn_loss 2 from coral_pytorch.dataset import corn_label_from_logits ----> 4 import pytorch_lightning as pl 5 import torchmetrics 8 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy' Setting up the dataset In this section, we are going to set up our dataset. Inspecting the dataset import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced. Performance baseline Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model. Setting up a DataModule There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly. Training the model using the PyTorch Lightning Trainer class Next, we initialize our PyTorchRNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-corn-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" ) Evaluating the model After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 8. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' ) Predicting labels of new data You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"CORN recurrent neural net for text data (TripAdvisor dataset)"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#a-recurrent-neural-net-for-ordinal-regression-using-corn-tripadvisor-dataset","text":"In this tutorial, we implement a recurrent neural network for ordinal regression based on the CORN method. To learn more about CORN, please have a look at our preprint: Xintong Shi, Wenzhi Cao, and Sebastian Raschka (2021). Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. Arxiv preprint; https://arxiv.org/abs/2111.08851 We will be using a balanced version of the TripAdvisor Hotel Review dataset that we used in the CORN manuscript.","title":"A Recurrent Neural Net for Ordinal Regression using CORN -- TripAdvisor Dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#general-settings-and-hyperparameters","text":"Here, we specify some general hyperparameter values and general settings Note that for small datatsets, it is not necessary and better not to use multiple workers as it can sometimes cause issues with too many open files in PyTorch. So, if you have problems with the data loader later, try setting NUM_WORKERS = 0 instead. BATCH_SIZE = 16 NUM_EPOCHS = 40 LEARNING_RATE = 0.005 NUM_WORKERS = 4 RANDOM_SEED = 123 # Architecture: EMBEDDING_DIM = 128 HIDDEN_DIM = 256 # Dataset specific: NUM_CLASSES = 5 VOCABULARY_SIZE = 20000 DATA_BASEPATH = \"./data\" This tutorial also requires the spacy English vocabulary, which can be downloaded as shown below: python - m spacy download en_core_web_sm","title":"General settings and hyperparameters"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#converting-a-regular-classifier-into-a-corn-ordinal-regression-model","text":"Changing a classifier to a CORN model for ordinal regression is actually really simple and only requires a few changes: 1) Consider the following output layer used by a neural network classifier: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes ) In CORN we reduce the number of classes by 1: output_layer = torch . nn . Linear ( hidden_units [ - 1 ], num_classes - 1 ) 2) We swap the cross entropy loss from PyTorch, torch . nn . functional . cross_entropy ( logits , true_labels ) with the CORN loss (also provided via coral_pytorch ): loss = corn_loss ( logits , true_labels , num_classes = num_classes ) Note that we pass num_classes instead of num_classes-1 to the corn_loss as it takes care of the rest internally. 3) In a regular classifier, we usually obtain the predicted class labels as follows: predicted_labels = torch . argmax ( logits , dim = 1 ) In CORN, w replace this with the following code to convert the predicted probabilities into the predicted labels: predicted_labels = corn_label_from_logits ( logits )","title":"Converting a regular classifier into a CORN ordinal regression model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#implementing-an-rnn-using-pytorch-lightnings-lightningmodule","text":"In this section, we set up the main model architecture using the LightningModule from PyTorch Lightning. We start with defining our recurrent neural network ( RNN ) model in pure PyTorch, and then we use it in the LightningModule to get all the extra benefits that PyTorch Lightning provides. import torch # Regular PyTorch Module class PyTorchRNN ( torch . nn . Module ): def __init__ ( self , input_dim , embedding_dim , hidden_dim , num_classes ): super () . __init__ () self . input_dim = input_dim self . embedding_dim = embedding_dim self . hidden_dim = hidden_dim self . num_classes = num_classes self . embedding = torch . nn . Embedding ( input_dim , embedding_dim ) # self.rnn = torch.nn.RNN(embedding_dim, # hidden_dim, # nonlinearity='relu') self . rnn = torch . nn . LSTM ( embedding_dim , hidden_dim ) # CORN output layer ------------------------------------------ # Regular classifier would use num_classes instead of # num_classes-1 below self . output_layer = torch . nn . Linear ( hidden_dim , num_classes - 1 ) # ------------------------------------------------------------ self . num_classes = num_classes def forward ( self , text , text_length ): # text dim: [sentence len, batch size] embedded = self . embedding ( text ) # embedded dim: [sentence len, batch size, embed dim] packed = torch . nn . utils . rnn . pack_padded_sequence ( embedded , text_length . to ( 'cpu' )) packed_output , ( hidden , cell ) = self . rnn ( packed ) # output dim: [sentence len, batch size, hidden dim] # hidden dim: [1, batch size, hidden dim] hidden . squeeze_ ( 0 ) # hidden dim: [batch size, hidden dim] output = self . output_layer ( hidden ) logits = output . view ( - 1 , ( self . num_classes - 1 )) return logits In our LightningModule we use loggers to track mean absolute errors for both the training and validation set during training; this allows us to select the best model based on validation set performance later. Given an RNN classifier with cross-entropy loss, it is very easy to change this classifier into a ordinal regression model using CORN. In essence, it only requires three changes: Instead of using num_classes in the output layer, use num_classes-1 as shown above Change the loss from loss = torch.nn.functional.cross_entropy(logits, y) to loss = corn_loss(logits, y, num_classes=self.num_classes) To obtain the class/rank labels from the logits, change predicted_labels = torch.argmax(logits, dim=1) to predicted_labels = corn_label_from_logits(logits) from coral_pytorch.losses import corn_loss from coral_pytorch.dataset import corn_label_from_logits import pytorch_lightning as pl import torchmetrics # LightningModule that receives a PyTorch model as input class LightningRNN ( pl . LightningModule ): def __init__ ( self , model , learning_rate ): super () . __init__ () self . input_dim = model . input_dim self . embedding_dim = model . embedding_dim self . hidden_dim = model . hidden_dim self . num_classes = model . num_classes self . learning_rate = learning_rate # The inherited PyTorch module self . model = model # Save settings and hyperparameters to the log directory # but skip the model parameters self . save_hyperparameters ( ignore = [ 'model' ]) # Set up attributes for computing the MAE self . train_mae = torchmetrics . MeanAbsoluteError () self . valid_mae = torchmetrics . MeanAbsoluteError () self . test_mae = torchmetrics . MeanAbsoluteError () # (Re)Defining the forward method is only necessary # if you want to use a Trainer's .predict() method (optional) def forward ( self , text , text_length ): return self . model ( text , text_length ) # A common forward step to compute the loss and labels # this is used for training, validation, and testing below def _shared_step ( self , batch ): # These next 3 steps are unique and look a bit tricky due to # how Torchtext's BucketIterator prepares the batches # and how we use an LSTM with packed & padded text # Also, .TEXT_COLUMN_NAME and .LABEL_COLUMN_NAME # depend on the CSV file columns of the data file we load later. features , text_length = batch . TEXT_COLUMN_NAME true_labels = batch . LABEL_COLUMN_NAME logits = self ( features , text_length ) # Use CORN loss --------------------------------------------------- # A regular classifier uses: # loss = torch.nn.functional.cross_entropy(logits, true_labels) loss = corn_loss ( logits , true_labels , num_classes = self . model . num_classes ) # ----------------------------------------------------------------- # CORN logits to labels ------------------------------------------- # A regular classifier uses: # predicted_labels = torch.argmax(logits, dim=1) predicted_labels = corn_label_from_logits ( logits ) # ----------------------------------------------------------------- return loss , true_labels , predicted_labels def training_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"train_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . train_mae ( predicted_labels , true_labels ) self . log ( \"train_mae\" , self . train_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) return loss # this is passed to the optimzer for training def validation_step ( self , batch , batch_idx ): loss , true_labels , predicted_labels = self . _shared_step ( batch ) self . log ( \"valid_loss\" , loss , batch_size = true_labels . shape [ 0 ]) self . valid_mae ( predicted_labels , true_labels ) self . log ( \"valid_mae\" , self . valid_mae , on_epoch = True , on_step = False , prog_bar = True , batch_size = true_labels . shape [ 0 ]) def test_step ( self , batch , batch_idx ): _ , true_labels , predicted_labels = self . _shared_step ( batch ) self . test_mae ( predicted_labels , true_labels ) self . log ( \"test_mae\" , self . test_mae , on_epoch = True , on_step = False , batch_size = true_labels . shape [ 0 ]) def configure_optimizers ( self ): optimizer = torch . optim . Adam ( self . parameters (), lr = self . learning_rate ) return optimizer --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Input In [5], in <cell line: 4>() 1 from coral_pytorch.losses import corn_loss 2 from coral_pytorch.dataset import corn_label_from_logits ----> 4 import pytorch_lightning as pl 5 import torchmetrics 8 # LightningModule that receives a PyTorch model as input File ~/conda/lib/python3.8/site-packages/pytorch_lightning/__init__.py:20, in <module> 17 _PACKAGE_ROOT = os.path.dirname(__file__) 18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT) ---> 20 from pytorch_lightning.callbacks import Callback # noqa: E402 21 from pytorch_lightning.core import LightningDataModule, LightningModule # noqa: E402 22 from pytorch_lightning.trainer import Trainer # noqa: E402 File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py:14, in <module> 1 # Copyright The PyTorch Lightning team. 2 # 3 # Licensed under the Apache License, Version 2.0 (the \"License\"); (...) 12 # See the License for the specific language governing permissions and 13 # limitations under the License. ---> 14 from pytorch_lightning.callbacks.base import Callback 15 from pytorch_lightning.callbacks.device_stats_monitor import DeviceStatsMonitor 16 from pytorch_lightning.callbacks.early_stopping import EarlyStopping File ~/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:26, in <module> 23 from torch.optim import Optimizer 25 import pytorch_lightning as pl ---> 26 from pytorch_lightning.utilities.types import STEP_OUTPUT 29 class Callback(abc.ABC): 30 r\"\"\" 31 Abstract base class used to build new callbacks. 32 33 Subclass this class and override any of the relevant hooks 34 \"\"\" File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py:18, in <module> 14 \"\"\"General utilities.\"\"\" 16 import numpy ---> 18 from pytorch_lightning.utilities.apply_func import move_data_to_device # noqa: F401 19 from pytorch_lightning.utilities.distributed import AllGatherGrad, rank_zero_info, rank_zero_only # noqa: F401 20 from pytorch_lightning.utilities.enums import ( # noqa: F401 21 AMPType, 22 DeviceType, (...) 26 ModelSummaryMode, 27 ) File ~/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:30, in <module> 28 if _TORCHTEXT_AVAILABLE: 29 if _compare_version(\"torchtext\", operator.ge, \"0.9.0\"): ---> 30 from torchtext.legacy.data import Batch 31 else: 32 from torchtext.data import Batch ModuleNotFoundError: No module named 'torchtext.legacy'","title":"Implementing an RNN using PyTorch Lightning's LightningModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#setting-up-the-dataset","text":"In this section, we are going to set up our dataset.","title":"Setting up the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#inspecting-the-dataset","text":"import pandas as pd import numpy as np data_df = pd . read_csv ( \"https://raw.githubusercontent.com/Raschka-research-group/\" \"corn-ordinal-neuralnet/main/datasets/\" \"tripadvisor/tripadvisor_balanced.csv\" ) data_df . tail () import os CSV_PATH = os . path . join ( DATA_BASEPATH , 'tripadvisor_balanced.csv' ) data_df . to_csv ( CSV_PATH , index = None ) import torchtext import random TEXT = torchtext . legacy . data . Field ( tokenize = 'spacy' , # default splits on whitespace tokenizer_language = 'en_core_web_sm' , include_lengths = True ) LABEL = torchtext . legacy . data . LabelField ( dtype = torch . long ) fields = [( 'TEXT_COLUMN_NAME' , TEXT ), ( 'LABEL_COLUMN_NAME' , LABEL )] dataset = torchtext . legacy . data . TabularDataset ( path = CSV_PATH , format = 'csv' , skip_header = True , fields = fields ) train_data , test_data = dataset . split ( split_ratio = [ 0.8 , 0.2 ], random_state = random . seed ( RANDOM_SEED )) train_data , valid_data = train_data . split ( split_ratio = [ 0.85 , 0.15 ], random_state = random . seed ( RANDOM_SEED )) TEXT . build_vocab ( train_data , max_size = VOCABULARY_SIZE ) LABEL . build_vocab ( train_data ) train_loader , valid_loader , test_loader = \\ torchtext . legacy . data . BucketIterator . splits ( ( train_data , valid_data , test_data ), device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ), batch_size = BATCH_SIZE , sort_within_batch = True , # necessary for packed_padded_sequence sort_key = lambda x : len ( x . TEXT_COLUMN_NAME ), ) # Checking the dataset all_train_labels = [] all_test_labels = [] for features , labels in train_loader : all_train_labels . append ( labels ) all_train_labels = torch . cat ( all_train_labels ) for features , labels in test_loader : all_test_labels . append ( labels ) all_test_labels = torch . cat ( all_test_labels ) print ( 'Training labels:' , torch . unique ( all_train_labels )) print ( 'Training label distribution:' , torch . bincount ( all_train_labels )) print ( ' \\n Test labels:' , torch . unique ( all_test_labels )) print ( 'Test label distribution:' , torch . bincount ( all_test_labels )) Above, we can see that the dataset consists of 8 features, and there are 998 examples in total. The labels are in range from 1 (weakest) to 5 (strongest), and we normalize them to start at zero (hence, the normalized labels are in the range 0 to 4). Notice also that the dataset is quite balanced.","title":"Inspecting the dataset"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#performance-baseline","text":"Especially for imbalanced datasets, it's quite useful to compute a performance baseline. In classification contexts, a useful baseline is to compute the accuracy for a scenario where the model always predicts the majority class -- you want your model to be better than that! Note that if you are intersted in a single number that minimized the dataset mean squared error (MSE), that's the mean; similary, the median is a number that minimzes the mean absolute error (MAE). So, if we use the mean absolute error, \\mathrm{MAE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| , to evaluate the model, it is useful to compute the MAE pretending the predicted label is always the median: all_test_labels = all_test_labels . float () avg_prediction = torch . median ( all_test_labels ) # median minimizes MAE baseline_mae = torch . mean ( torch . abs ( all_test_labels - avg_prediction )) print ( f 'Baseline MAE: { baseline_mae : .2f } ' ) In other words, a model that would always predict the dataset median would achieve a MAE of 1.18. A model that has an MAE of > 1.18 is certainly a bad model.","title":"Performance baseline"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#setting-up-a-datamodule","text":"There are three main ways we can prepare the dataset for Lightning. We can make the dataset part of the model; set up the data loaders as usual and feed them to the fit method of a Lightning Trainer -- the Trainer is introduced in the next subsection; create a LightningDataModule . Usually, approach 3 is the most organized approach. However, since we already defined our data loaders above, we can just work with those directly.","title":"Setting up a DataModule"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#training-the-model-using-the-pytorch-lightning-trainer-class","text":"Next, we initialize our PyTorchRNN model. Also, we define a call back so that we can obtain the model with the best validation set performance after training. PyTorch Lightning offers many advanced logging services like Weights & Biases. Here, we will keep things simple and use the CSVLogger : from pytorch_lightning.callbacks import ModelCheckpoint from pytorch_lightning.loggers import CSVLogger pytorch_model = PyTorchRNN ( input_dim = len ( TEXT . vocab ), embedding_dim = EMBEDDING_DIM , hidden_dim = HIDDEN_DIM , num_classes = NUM_CLASSES ) lightning_model = LightningRNN ( pytorch_model , learning_rate = LEARNING_RATE ) callbacks = [ ModelCheckpoint ( save_top_k = 1 , mode = 'min' , monitor = \"valid_mae\" )] # save top 1 model logger = CSVLogger ( save_dir = \"logs/\" , name = \"rnn-corn-mnist\" ) Note that we disable warning as the .log() method of the LightningModule currently warns us that the batch size is inconsistent. This should not happen as we define the batch_size manually in the self.log calls. However, this will be resolved in a future version (https://github.com/PyTorchLightning/pytorch-lightning/pull/10408). Also note that the batch size is not inconsistent, its just that the BucketIterator in torchtext has creates batches where the text length plus padding is the first dimension in a tensor. And the batch size is the second dimension: for features , labels in train_loader : break print ( 'Text length:' , features [ 0 ] . shape [ 0 ]) print ( 'Batch size (from text):' , features [ 0 ] . shape [ 1 ]) print ( 'Batch size (from labels):' , labels . shape [ 0 ]) Now it's time to train our model: import time trainer = pl . Trainer ( max_epochs = NUM_EPOCHS , callbacks = callbacks , progress_bar_refresh_rate = 50 , # recommended for notebooks accelerator = \"auto\" , # Uses GPUs or TPUs if available devices = \"auto\" , # Uses all available GPUs/TPUs if applicable logger = logger , deterministic = True , log_every_n_steps = 10 ) start_time = time . time () trainer . fit ( model = lightning_model , train_dataloaders = train_loader , val_dataloaders = valid_loader ) runtime = ( time . time () - start_time ) / 60 print ( f \"Training took { runtime : .2f } min in total.\" )","title":"Training the model using the PyTorch Lightning Trainer class"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#evaluating-the-model","text":"After training, let's plot our training MAE and validation MAE using pandas, which, in turn, uses matplotlib for plotting (you may want to consider a more advanced logger that does that for you): import pandas as pd metrics = pd . read_csv ( f \" { trainer . logger . log_dir } /metrics.csv\" ) aggreg_metrics = [] agg_col = \"epoch\" for i , dfg in metrics . groupby ( agg_col ): agg = dict ( dfg . mean ()) agg [ agg_col ] = i aggreg_metrics . append ( agg ) df_metrics = pd . DataFrame ( aggreg_metrics ) df_metrics [[ \"train_loss\" , \"valid_loss\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'Loss' ) df_metrics [[ \"train_mae\" , \"valid_mae\" ]] . plot ( grid = True , legend = True , xlabel = 'Epoch' , ylabel = 'MAE' ) As we can see from the loss plot above, the model starts overfitting pretty quickly. Based on the MAE plot, we can see that the best model, based on the validation set MAE, may be around epoch 8. The trainer saved this model automatically for us, we which we can load from the checkpoint via the ckpt_path='best' argument; below we use the trainer instance to evaluate the best model on the test set: trainer . test ( model = lightning_model , dataloaders = test_loader , ckpt_path = 'best' )","title":"Evaluating the model"},{"location":"tutorials/pytorch_lightning/ordinal-corn_tripadvisor/#predicting-labels-of-new-data","text":"You can use the trainer.predict method on a new DataLoader or DataModule to apply the model to new data. Alternatively, you can also manually load the best model from a checkpoint as shown below: path = trainer . checkpoint_callback . best_model_path print ( path ) lightning_model = LightningRNN . load_from_checkpoint ( path , model = pytorch_model ) lightning_model . to ( torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )) lightning_model . eval (); Note that our PyTorchRNN , which is passed to LightningRNN requires input arguments. However, this is automatically being taken care of since we used self.save_hyperparameters() in LightningRNN 's __init__ method. Now, below is an example applying the model manually. Here, pretend that the test_dataloader is a new data loader. all_predicted_labels = [] for batch in test_loader : features , text_length = batch . TEXT_COLUMN_NAME logits = lightning_model ( features , text_length ) predicted_labels = corn_label_from_logits ( logits ) all_predicted_labels . append ( predicted_labels ) all_predicted_labels = torch . cat ( all_predicted_labels ) all_predicted_labels [: 5 ]","title":"Predicting labels of new data"}]}